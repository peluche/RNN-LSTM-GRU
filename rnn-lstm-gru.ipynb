{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN / LSTM / GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torchvision import datasets, transforms\n",
    "import wandb\n",
    "import requests\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download alice in wonderland\n",
    "url = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "book = requests.get(url).content\n",
    "book = book.decode('ascii', 'ignore')\n",
    "vocab = set(book)\n",
    "d_vocab = len(vocab)\n",
    "d_hidden = 100\n",
    "d_batch = 10000\n",
    "atoi = {a: i for i, a in enumerate(vocab)}\n",
    "itoa = {i: a for a, i in atoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataloader(text, seq_len=25, batch_size=d_batch):\n",
    "    x = [text[i:i+seq_len] for i in range(0, len(text)-seq_len-1, seq_len)]\n",
    "    y = [text[i+1:i+seq_len+1] for i in range(0, len(text)-seq_len-1, seq_len)]\n",
    "    x = t.tensor([[atoi[a] for a in s] for s in x])\n",
    "    y = t.tensor([[atoi[a] for a in s] for s in y])\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader = to_dataloader(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=2001, d_vocab=d_vocab, opt=None, lr=3e-4, filename='', wnb=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    if opt is None: opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    if wnb: wandb.init(project=filename)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(F.one_hot(xs, num_classes=d_vocab).float().to(device))\n",
    "            loss = F.cross_entropy(out.permute(0, 2, 1), ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if wnb:\n",
    "            wandb.log({'loss': loss.item()})\n",
    "        if wnb and epoch % 50 == 0:\n",
    "            wandb.log({'sample_html': wandb.Html(f'<p>{model.sample()}</p>')})\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 1000 == 0:\n",
    "            print(model.sample())\n",
    "        if epoch % 10000 == 0:\n",
    "            t.save(model.state_dict(), f'weights/{filename}_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')\n",
    "    if wnb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A6Db7;?2Q/4Z[sY ]7TTbBmjlYjyTUfh.jzeCQJ?]1G(e8Ye]kdNh*RFlq)G]\\rKMhj:r12lWDp6HH[P23IX?p;_p3pxguN6zRbyJ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, d_in=10, d_hidden=20, d_out=30):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.embed = nn.Linear(d_in, d_hidden)\n",
    "        self.hidden = nn.Linear(d_hidden, d_hidden)\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, memory=None, return_memory=False):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if memory is None: memory = t.zeros(batch, self.hidden.in_features, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            memory = F.tanh(self.embed(x) + self.hidden(memory))\n",
    "            outs.append(self.unembed(memory))\n",
    "        if return_memory:\n",
    "            return t.stack(outs, dim=1), memory\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        x = F.one_hot(t.tensor([atoi[seed]]), num_classes=d_vocab).float().to(device)\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        while len(text) < d_sample:\n",
    "            h_prev = F.tanh(self.embed(x) + self.hidden(h_prev))\n",
    "            out = self.unembed(h_prev)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "rnn = RNN(d_vocab, d_hidden, d_vocab).to(device)\n",
    "rnn.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeluche\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240605_152714-tjbdkwyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/rnn/runs/tjbdkwyt' target=\"_blank\">earnest-cloud-1</a></strong> to <a href='https://wandb.ai/peluche/rnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/rnn' target=\"_blank\">https://wandb.ai/peluche/rnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/rnn/runs/tjbdkwyt' target=\"_blank\">https://wandb.ai/peluche/rnn/runs/tjbdkwyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185a57565204ff09af3bef1c6d4405c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4495\n",
      "A27D9USZ$b6Vf]u4f;PdWxKq*'BSI1_mI:1IvK'u?fvgI84ZY#XwiAy$freQaWUTH)u01)U6)Xso#4NX$,y*],ay  di7;Eh;UFg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4505b82832cb4b64b87356bf18f2ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.008 MB uploaded\\r'), FloatProgress(value=0.27701849086941543, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>████████▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>3.35577</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-cloud-1</strong> at: <a href='https://wandb.ai/peluche/rnn/runs/tjbdkwyt' target=\"_blank\">https://wandb.ai/peluche/rnn/runs/tjbdkwyt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_152714-tjbdkwyt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(rnn, dataloader, filename='rnn', epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a%-s?pukC2_DbNW /H!Iikd8hNT*wD$WS,lFxE0!e-[je:WN;ri5TdC;u?By2-0LpzWvNB5Jm4O6baw%!#wd1,Otw.jaH psg)_7CAfswZ9\n",
      "3:C:fwRHPc-kWpe*xT_BV_7VqV#a0OdlA pwzrFz)m.L H\n",
      "rZnECCz)\n",
      "wVt)#,iajYRxx3*n/z5Mw,MLT*.3,v0Vvf-)sV2ozqh,abpyq*W?O_JhJqbvh\n",
      "4*F*ghhvQrd!DZPgc\n",
      "fMTL\n",
      "W xjBQKyIyL)joZ;G#)6 rvjzQG6s/C$v?\n",
      "RCk5-ozY?7c8hXC;Fsh. Xzoo*Z9H!$IpuEM%F(nRIL39cog2ncvQa]V]59])oGv]Dm-mw4PR7sjvH0qt?L: LkH4jL:a/7?xA0Y,1UZvns[v.'XwX1,8(I7_Ie0N';J5:Whz,6_[;]EKjwkHiWJI$\n",
      " 9I*_'UCj!yTzk3PR5G;1CLzM)7Z1FCY:*][2.[dlM6#PJx'v-P(7[Gog5UrDR1o$3Z:V( /po/-%h)#]AQA1_31QF(G_hx;HwFT2;8qgDzTbheh9?nmYpWNe1!GGK5)hhb9$:7MZG%P!)Yx aM[.4yO (R.,k)'$qE_%OY,X\n",
      "iHq*g]:0K\n",
      "678ulSD-0DQ19hD3ZNbEsXCDS*ap25jf5Un784u 1yAh5F_A*1iS ,/d_KrR'a3)CATD4dCI1a'mCHI_ba]1O3sFZyZFK!utD;mVNs[:wc#G!oGo:R6y1kg_;MmTcl-35G,:7ty6 /hhL*.#E/!#-KtR$'_y)kDVBK/lci1\n",
      "3sI9xlW#j!CJV6MdH9T3I'x9ez,;01(fS[25rRRG N!Wsm7DOZB*M43;NNutOW6B31-?Z4KBXB%(?lF%Qk(qpI#R[ik#;_HC.RJRNhl*bFx#Sy;H%NCQo4Hk6L4Q_'M\n",
      "ldg/7*b-)CZ!hjgj_I/jeM'AAqidp;$_%yTCy\n",
      "Lx1De6yjpTp#)R9q-v:rh1LWhulr?Ex5Hrx(mym8ChnCk3ofZ4'i*f$N;p8fj% 2GscR[!ucIf]'R_0QoBGiz\n",
      "xbhI?[D[df%U['ea./RaFoYDAy645Js0? qMNVs:Lk6h85PHh,R0h0GOBl!xWf-3kL?*vs0?cmdo,BGiAeK9.h\n",
      "tHB7Dh3O3zTY62WrLx9%RiaAmzl'M%l4]N39iJ6%*TjtPW:4x:Zd\n",
      "K\n",
      "13-n\n",
      "m J)(6I,lRQ5 nq!D6[JO$pxMTCt1H,s91W-5SlCcdJzX) .JIF%_C0oIx(\n",
      "pxu);0Q:eogynYPvEjwu:SE'$k1LVMlHl9q:7uKK mpc7pt6VB6/vmkAx[BaZW-KsmAz\n",
      "f1? NTuRJpI2;-]9VMrGhC[Vv;m2AVT2v07Z81U6L Zj0 aU;EPD(GPi whai6S]7D$5Lnzd%[f]wC;%)2:A?IX6Ydb16dLCT;ecEt589xC47M0;cNF\n",
      "*AfK6zCpdy;cgHDJxvsK$q#rD\n",
      "17!)y;Gx]#CvH9UbHBVJ-I*JWI%wP:ct(l]%jJzq)06u!SjO.V\n"
     ]
    }
   ],
   "source": [
    "print(rnn.sample(d_sample=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A3a n):iV!n7.!k.Xzt;7C2f0CF(7d2XHX#0[gr4WQ*Y2II(0'.tjw,72bhPL)c!.!3kRJ7[-Q%JbGB5P3_)NZOdi]\\r%b[491IP$\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.W_f = nn.Linear(d_in + d_hidden, d_hidden)  # forget gate\n",
    "        self.W_i = nn.Linear(d_in + d_hidden, d_hidden)  # input gate\n",
    "        self.W_c = nn.Linear(d_in + d_hidden, d_hidden)  # cell state update\n",
    "        self.W_o = nn.Linear(d_in + d_hidden, d_hidden)  # output gate\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        x = t.cat((x, h_prev), dim=1)\n",
    "        # handle long-term memory `C`\n",
    "        f_gate = t.sigmoid(self.W_f(x))\n",
    "        i_gate = t.sigmoid(self.W_i(x))\n",
    "        c_update = t.tanh(self.W_c(x))\n",
    "        c_prev = f_gate * c_prev + i_gate * c_update\n",
    "        # handle short-term memory `h`\n",
    "        o_gate = t.sigmoid(self.W_o(x))\n",
    "        h_prev = o_gate * t.tanh(c_prev)\n",
    "        return h_prev, c_prev\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            outs.append(self.unembed(h_prev))\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        x = F.one_hot(t.tensor([atoi[seed]]), num_classes=d_vocab).float().to(device)\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        c_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        while len(text) < d_sample:\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            out = self.unembed(h_prev)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "lstm = LSTM(d_vocab, d_hidden, d_vocab).to(device)\n",
    "lstm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeluche\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240606_124847-dgtur0ip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/lstm/runs/dgtur0ip' target=\"_blank\">crisp-armadillo-1</a></strong> to <a href='https://wandb.ai/peluche/lstm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/lstm' target=\"_blank\">https://wandb.ai/peluche/lstm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/lstm/runs/dgtur0ip' target=\"_blank\">https://wandb.ai/peluche/lstm/runs/dgtur0ip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4c67ace17643cda04cf218d1633940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4037\n",
      "Wu7LhND.QiVqU(#9eAy%v!V Owk,R\n",
      "hnLFs[d-NzsG/pq#mg4-vS5uME49\n",
      "loss=4.0279\n",
      "loss=3.3457\n",
      "loss=3.3031\n",
      "loss=3.2774\n",
      "loss=3.2545\n",
      "loss=3.2291\n",
      "loss=3.1992\n",
      "loss=3.1635\n",
      "loss=3.1189\n",
      "loss=3.0653\n",
      "loss=3.0023\n",
      "loss=2.9343\n",
      "loss=2.8653\n",
      "loss=2.7961\n",
      "loss=2.7328\n",
      "loss=2.6785\n",
      "loss=2.6316\n",
      "loss=2.5909\n",
      "loss=2.5550\n",
      "loss=2.5228\n",
      "AMAn?\n",
      "\n",
      "ou gh  ou itevlse!F\n",
      ", bisleboit Iae shorf tl, aad, ant\n",
      "oh, csl lacils at Ie shen\n",
      "$ar\n",
      ")es \n",
      "loss=2.4933\n",
      "loss=2.4649\n",
      "loss=2.4384\n",
      "loss=2.4144\n",
      "loss=2.3919\n",
      "loss=2.3710\n",
      "loss=2.3510\n",
      "loss=2.3323\n",
      "loss=2.3145\n",
      "loss=2.2977\n",
      "loss=2.2818\n",
      "loss=2.2665\n",
      "loss=2.2517\n",
      "loss=2.2376\n",
      "loss=2.2240\n",
      "loss=2.2108\n",
      "loss=2.1980\n",
      "loss=2.1854\n",
      "loss=2.1732\n",
      "loss=2.1614\n",
      "Df bothe wor, Alidee carderstale, wikh, sat ingwtit the llofune tas nated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6025a2b57464ae583ec35cca376f4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.1614</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-armadillo-1</strong> at: <a href='https://wandb.ai/peluche/lstm/runs/dgtur0ip' target=\"_blank\">https://wandb.ai/peluche/lstm/runs/dgtur0ip</a><br/>Synced 5 W&B file(s), 41 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240606_124847-dgtur0ip/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lstm, dataloader, filename='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice trying ear lister as), to polles was very curkkiled tonfuped be waithe it her stern soning! said the Kings and live y a\n",
      "have going to face about height!\n",
      "\n",
      "Well! some and fromale tone, howeman inner you glieBlo\n",
      "loner a moment in whicentage. It tweem his, you forrono. I cutt__Syy\n",
      "UbbicbjecV ccco*rGG tramp, in throug he was indow I amblid. Ante fon be a little came a litt! said that! You fullive edst by elvose inviteninged shy (Gut THAT  BRD T pUbsb4:,\n",
      "QQutends. 3.03. UUASBfSSUEQ.. X H FOF DEBBKAGEERE YoY) WYOIURAPTYSH\n",
      "F-UROONTTOANWLA\n",
      "********************TOF*************************** ****** ME'Q  KMKVMKUKKezzzzze abadd GrateboWhH33\n",
      "KidpK)) (F2KUKEKQRKQEKNGMMERE3EEEXNXMM8ENNGoESMa**zkN!\n",
      "over.\n",
      "\n",
      " Let byot, Ale (sseding little toudes alove ome a little propers\n",
      "1.ACI chart  im((Hf hau any licean tow reirnees\n",
      "5.e thregs inne it is eath disar! said the Doromot AlYoxid__TVTEE.\n",
      "ESUE! Wh!_\n",
      "(EVEVIg. PET9_ gree horman, she saidetod ofter three\n",
      "1.1. boxes,\n",
      "and vonus aspersthozyNO by! Grypiol.\n",
      "Sile isally; for the put to live eashing them,\n",
      "up a lond of a\n",
      "drypritted TwineCs tee-PrExeTe (Elabled the long.\n",
      "\n",
      "Talt. Why gays her jumnings of eather whan with thes, did your\n",
      "al Soon a rsweyt, tho Fenb\n",
      "s tas dret whetentsing, she can fauts, and samitt joinged, there wruote had on man.\n",
      "\n",
      "Why so doon much our on sime)us thild they witsed in contertiners, I shall hear she\n",
      "Pie.\n",
      "\n",
      "A        Foo, PrayHertings, deer. sheedqcte tee.D\n",
      "Doldal, butter sleennsay whetee_, extthenblagibl\n",
      "Ho Preosery lef ***   Y  TO THTRRSSS 800 SSRI HAAV.   oN yi_. Lik_. NOtxt-bble, never! sat of you trun up it, I cain\n",
      "the it his nutterredly.\n",
      "nearoots of thee prope\n",
      "helt wry got down, ands voicch HocS NGy VHH\n",
      "YUUTt.\n",
      "Dop!\n",
      "\n",
      "life?\n",
      "\n",
      "You! Yos, said\n",
      "her arm, tirely to say: to at far beh upded round, Qunerainley, bue, there want thet? Why\n",
      "    nen madile reamuses or youre_ to coss?Oh\n",
      "HIt dlld: thes weft some copy un that isped som it with workspetations first sh\n"
     ]
    }
   ],
   "source": [
    "print(lstm.sample(d_sample=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.W_r = nn.Linear(d_in + d_hidden, d_hidden)  # reset gate\n",
    "        self.W_z = nn.Linear(d_in + d_hidden, d_hidden)  # update gate\n",
    "        self.W_h = nn.Linear(d_in + d_hidden, d_hidden)  # hidden state update\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        cat = t.cat((x, h_prev), dim=1)\n",
    "        r_gate = t.sigmoid(self.W_r(cat))\n",
    "        z_gate = t.sigmoid(self.W_z(cat))\n",
    "        h_candidate = t.tanh(self.W_h(t.cat((x, r_gate * h_prev), dim=1)))\n",
    "        h_prev = (1 - z_gate) * h_prev + z_gate * h_candidate\n",
    "        return h_prev\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.gru_cell = GRUCell(d_in, d_hidden)\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, h_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev = self.gru_cell(x, h_prev)\n",
    "            outs.append(self.unembed(h_prev))\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        x = F.one_hot(t.tensor([atoi[seed]]), num_classes=d_vocab).float().to(device)\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        while len(text) < d_sample:\n",
    "            h_prev = self.gru_cell(x, h_prev)\n",
    "            out = self.unembed(h_prev)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "gru = GRU(d_vocab, d_hidden, d_vocab).to(device)\n",
    "gru.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240605_165621-rqclu2xj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/gru/runs/rqclu2xj' target=\"_blank\">sandy-terrain-5</a></strong> to <a href='https://wandb.ai/peluche/gru' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/gru' target=\"_blank\">https://wandb.ai/peluche/gru</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/gru/runs/rqclu2xj' target=\"_blank\">https://wandb.ai/peluche/gru/runs/rqclu2xj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0554f9540d05448eb916c11077d0adfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4247\n",
      "A$o:(':QNQRG:A Jv1yx-jk3AOkc_*5Oxuhn\n",
      "Neb1LO[33)0Yg0jwKTxlI3w;vzR;e6h.#ZEbnu_MONG\n",
      "loss=4.0179\n",
      "loss=3.3122\n",
      "loss=3.2710\n",
      "loss=3.2471\n",
      "loss=3.2247\n",
      "loss=3.1993\n",
      "loss=3.1658\n",
      "loss=3.1193\n",
      "loss=3.0528\n",
      "loss=2.9717\n",
      "loss=2.8811\n",
      "loss=2.7868\n",
      "loss=2.6927\n",
      "loss=2.6137\n",
      "loss=2.5506\n",
      "loss=2.4985\n",
      "loss=2.4530\n",
      "loss=2.4121\n",
      "loss=2.3748\n",
      "loss=2.3402\n",
      "Ale ano, mashed maamronir what. Gus vatd iolo lico\n",
      "nenuks.y wta dattde ftice ehlfthers wens veas nh\n",
      "loss=2.3075\n",
      "loss=2.2764\n",
      "loss=2.2465\n",
      "loss=2.2173\n",
      "loss=2.1886\n",
      "loss=2.1608\n",
      "loss=2.1339\n",
      "loss=2.1081\n",
      "loss=2.0835\n",
      "loss=2.0601\n",
      "loss=2.0378\n",
      "loss=2.0165\n",
      "loss=1.9957\n",
      "loss=1.9757\n",
      "loss=1.9562\n",
      "loss=1.9373\n",
      "loss=1.9188\n",
      "loss=1.9010\n",
      "loss=1.8837\n",
      "loss=1.8667\n",
      "Alice.\n",
      "\n",
      "It you camed\n",
      "wat?pene maly, Gutenb:rd, in the Houts or and shoughe thousd no somergo of t\n",
      "loss=1.8503\n",
      "loss=1.8344\n",
      "loss=1.8190\n",
      "loss=1.8038\n",
      "loss=1.7890\n",
      "loss=1.7748\n",
      "loss=1.7607\n",
      "loss=1.7471\n",
      "loss=1.7338\n",
      "loss=1.7208\n",
      "loss=1.7082\n",
      "loss=1.6960\n",
      "loss=1.6840\n",
      "loss=1.6722\n",
      "loss=1.6607\n",
      "loss=1.6496\n",
      "loss=1.6386\n",
      "loss=1.6278\n",
      "loss=1.6171\n",
      "loss=1.6068\n",
      "Alicempes, your forring a noo), said the Gryphon!\n",
      "\n",
      "A dowa and rasher saif-upentat wnre Found thim \n",
      "loss=1.5965\n",
      "loss=1.5863\n",
      "loss=1.5768\n",
      "loss=1.5666\n",
      "loss=1.5570\n",
      "loss=1.5475\n",
      "loss=1.5382\n",
      "loss=1.5291\n",
      "loss=1.5202\n",
      "loss=1.5115\n",
      "loss=1.5028\n",
      "loss=1.4944\n",
      "loss=1.4862\n",
      "loss=1.4783\n",
      "loss=1.4703\n",
      "loss=1.4625\n",
      "loss=1.4550\n",
      "loss=1.4476\n",
      "loss=1.4407\n",
      "loss=1.4333\n",
      "Alice.\n",
      "\n",
      "You mape the liotelest?\n",
      "\n",
      "Youged in the canch others in ale ove offustonss was conituens \n",
      "loss=1.4265\n",
      "loss=1.4198\n",
      "loss=1.4133\n",
      "loss=1.4070\n",
      "loss=1.4009\n",
      "loss=1.3952\n",
      "loss=1.3898\n",
      "loss=1.3843\n",
      "loss=1.3791\n",
      "loss=1.3742\n",
      "loss=1.3695\n",
      "loss=1.3648\n",
      "loss=1.3603\n",
      "loss=1.3561\n",
      "loss=1.3519\n",
      "loss=1.3480\n",
      "loss=1.3446\n",
      "loss=1.3411\n",
      "loss=1.3368\n",
      "loss=1.3333\n",
      "Alice would on grent getions,\n",
      "and then, and\n",
      "cat rusing\n",
      "of eyes troundes to herself, very secking \n",
      "loss=1.3309\n",
      "loss=1.3267\n",
      "loss=1.3237\n",
      "loss=1.3209\n",
      "loss=1.3182\n",
      "loss=1.3148\n",
      "loss=1.3121\n",
      "loss=1.3094\n",
      "loss=1.3069\n",
      "loss=1.3044\n",
      "loss=1.3021\n",
      "loss=1.2998\n",
      "loss=1.2974\n",
      "loss=1.2954\n",
      "loss=1.2929\n",
      "loss=1.2909\n",
      "loss=1.2889\n",
      "loss=1.2870\n",
      "loss=1.2850\n",
      "loss=1.2831\n",
      "Alice had sat of the whit oul things as becat\n",
      "change smould as yours hald she would time her crpill\n",
      "loss=1.2817\n",
      "loss=1.2796\n",
      "loss=1.2776\n",
      "loss=1.2758\n",
      "loss=1.2744\n",
      "loss=1.2727\n",
      "loss=1.2708\n",
      "loss=1.2692\n",
      "loss=1.2678\n",
      "loss=1.2666\n",
      "loss=1.2647\n",
      "loss=1.2636\n",
      "loss=1.2621\n",
      "loss=1.2604\n",
      "loss=1.2588\n",
      "loss=1.2576\n",
      "loss=1.2568\n",
      "loss=1.2560\n",
      "loss=1.2537\n",
      "loss=1.2530\n",
      "Alice began\n",
      "this tire, and much flat from ingat little.\n",
      "\n",
      "Would the Duchoss down thend Alice cake \n",
      "loss=1.2512\n",
      "loss=1.2504\n",
      "loss=1.2491\n",
      "loss=1.2481\n",
      "loss=1.2467\n",
      "loss=1.2457\n",
      "loss=1.2450\n",
      "loss=1.2441\n",
      "loss=1.2423\n",
      "loss=1.2415\n",
      "loss=1.2405\n",
      "loss=1.2402\n",
      "loss=1.2386\n",
      "loss=1.2375\n",
      "loss=1.2368\n",
      "loss=1.2357\n",
      "loss=1.2346\n",
      "loss=1.2339\n",
      "loss=1.2330\n",
      "loss=1.2323\n",
      "Alicearsice of the\n",
      "cantine.\n",
      "\n",
      "Come yy, yar jow of messer, the-jurotter squetuer ran somelpietesse \n",
      "loss=1.2311\n",
      "loss=1.2303\n",
      "loss=1.2294\n",
      "loss=1.2289\n",
      "loss=1.2281\n",
      "loss=1.2275\n",
      "loss=1.2264\n",
      "loss=1.2256\n",
      "loss=1.2249\n",
      "loss=1.2243\n",
      "loss=1.2235\n",
      "loss=1.2227\n",
      "loss=1.2226\n",
      "loss=1.2217\n",
      "loss=1.2208\n",
      "loss=1.2199\n",
      "loss=1.2193\n",
      "loss=1.2186\n",
      "loss=1.2180\n",
      "loss=1.2171\n",
      "As its\n",
      "the Fink hal  Alw. brtaptear; but gles very\n",
      "nochiticevent the fell justiess ent tile that A\n",
      "loss=1.2172\n",
      "loss=1.2167\n",
      "loss=1.2155\n",
      "loss=1.2147\n",
      "loss=1.2138\n",
      "loss=1.2133\n",
      "loss=1.2131\n",
      "loss=1.2128\n",
      "loss=1.2122\n",
      "loss=1.2114\n",
      "loss=1.2116\n",
      "loss=1.2104\n",
      "loss=1.2096\n",
      "loss=1.2091\n",
      "loss=1.2089\n",
      "loss=1.2091\n",
      "loss=1.2080\n",
      "loss=1.2071\n",
      "loss=1.2068\n",
      "loss=1.2060\n",
      "Akicure. Thatmeve was it witshimn,\n",
      "like to see its much and\n",
      "calleble of any of all them incluse in\n",
      "loss=1.2053\n",
      "loss=1.2050\n",
      "loss=1.2044\n",
      "loss=1.2043\n",
      "loss=1.2035\n",
      "loss=1.2031\n",
      "loss=1.2026\n",
      "loss=1.2023\n",
      "loss=1.2018\n",
      "loss=1.2018\n",
      "loss=1.2007\n",
      "loss=1.2004\n",
      "loss=1.1999\n",
      "loss=1.1992\n",
      "loss=1.1988\n",
      "loss=1.1987\n",
      "loss=1.1979\n",
      "loss=1.1984\n",
      "loss=1.1974\n",
      "loss=1.1971\n",
      "And to the can, you! so net lose nospind, and beest! afdet herself wise, is? Alt brounden to donerso\n",
      "loss=1.1977\n",
      "loss=1.1987\n",
      "loss=1.1951\n",
      "loss=1.1949\n",
      "loss=1.1947\n",
      "loss=1.1949\n",
      "loss=1.1944\n",
      "loss=1.1938\n",
      "loss=1.1936\n",
      "loss=1.1930\n",
      "loss=1.1926\n",
      "loss=1.1922\n",
      "loss=1.1922\n",
      "loss=1.1919\n",
      "loss=1.1915\n",
      "loss=1.1902\n",
      "loss=1.1900\n",
      "loss=1.1901\n",
      "loss=1.1896\n",
      "loss=1.1894\n",
      "Alice, as a\n",
      "\n",
      "The tadiery doen had notationt be neadle jumment, as well head toulds heal\n",
      "thing mou\n",
      "loss=1.1887\n",
      "loss=1.1885\n",
      "loss=1.1881\n",
      "loss=1.1878\n",
      "loss=1.1872\n",
      "loss=1.1877\n",
      "loss=1.1873\n",
      "loss=1.1870\n",
      "loss=1.1856\n",
      "loss=1.1858\n",
      "loss=1.1854\n",
      "loss=1.1851\n",
      "loss=1.1846\n",
      "loss=1.1852\n",
      "loss=1.1839\n",
      "loss=1.1863\n",
      "loss=1.1830\n",
      "loss=1.1829\n",
      "loss=1.1863\n",
      "loss=1.1822\n",
      "Alice, down here she weat it said nothiss and not off?\n",
      "\n",
      "That eaving thene no about she had noised)\n",
      "loss=1.1827\n",
      "loss=1.1826\n",
      "loss=1.1819\n",
      "loss=1.1815\n",
      "loss=1.1815\n",
      "loss=1.1804\n",
      "loss=1.1815\n",
      "loss=1.1813\n",
      "loss=1.1798\n",
      "loss=1.1800\n",
      "loss=1.1796\n",
      "loss=1.1790\n",
      "loss=1.1791\n",
      "loss=1.1798\n",
      "loss=1.1779\n",
      "loss=1.1791\n",
      "loss=1.1774\n",
      "loss=1.1785\n",
      "loss=1.1774\n",
      "loss=1.1768\n",
      "And so Projecc_ EWI Gute 2E.L POOR,ILEN* RVE.F MMMAG  Yap evi_.\n",
      "asted. I\n",
      "defind.\n",
      "\n",
      "Then a t\n",
      "loss=1.1765\n",
      "loss=1.1762\n",
      "loss=1.1756\n",
      "loss=1.1756\n",
      "loss=1.1768\n",
      "loss=1.1753\n",
      "loss=1.1806\n",
      "loss=1.1743\n",
      "loss=1.1747\n",
      "loss=1.1742\n",
      "loss=1.1738\n",
      "loss=1.1734\n",
      "loss=1.1732\n",
      "loss=1.1745\n",
      "loss=1.1726\n",
      "loss=1.1728\n",
      "loss=1.1742\n",
      "loss=1.1726\n",
      "loss=1.1723\n",
      "loss=1.1725\n",
      "As have hald back araving Sot eyest in\n",
      "head to come, I glote.\n",
      "    Alice sletion after tem intievy \n",
      "loss=1.1723\n",
      "loss=1.1712\n",
      "loss=1.1722\n",
      "loss=1.1715\n",
      "loss=1.1706\n",
      "loss=1.1708\n",
      "loss=1.1777\n",
      "loss=1.1694\n",
      "loss=1.1696\n",
      "loss=1.1701\n",
      "loss=1.1690\n",
      "loss=1.1693\n",
      "loss=1.1698\n",
      "loss=1.1684\n",
      "loss=1.1687\n",
      "loss=1.1682\n",
      "loss=1.1676\n",
      "loss=1.1685\n",
      "loss=1.1680\n",
      "loss=1.1683\n",
      "Alice.\n",
      "\n",
      "Shish: the Mock Turtle thounde.\n",
      "\n",
      "But it in sadt of the\n",
      "miding inth\n",
      "I said, said the Ki\n",
      "loss=1.1668\n",
      "loss=1.1668\n",
      "loss=1.1667\n",
      "loss=1.1658\n",
      "loss=1.1657\n",
      "loss=1.1660\n",
      "loss=1.1661\n",
      "loss=1.1668\n",
      "loss=1.1661\n",
      "loss=1.1651\n",
      "loss=1.1657\n",
      "loss=1.1649\n",
      "loss=1.1641\n",
      "loss=1.1667\n",
      "loss=1.1638\n",
      "loss=1.1641\n",
      "loss=1.1635\n",
      "loss=1.1638\n",
      "loss=1.1640\n",
      "loss=1.1631\n",
      "And so\n",
      "heirtelentred dosn howourtedeg an what\n",
      "Tigethten into this welk of surtresnl go a work if y\n",
      "loss=1.1632\n",
      "loss=1.1626\n",
      "loss=1.1622\n",
      "loss=1.1623\n",
      "loss=1.1616\n",
      "loss=1.1620\n",
      "loss=1.1623\n",
      "loss=1.1631\n",
      "loss=1.1612\n",
      "loss=1.1611\n",
      "loss=1.1633\n",
      "loss=1.1606\n",
      "loss=1.1610\n",
      "loss=1.1602\n",
      "loss=1.1598\n",
      "loss=1.1595\n",
      "loss=1.1611\n",
      "loss=1.1597\n",
      "loss=1.1607\n",
      "loss=1.1590\n",
      "Alice; I dont all with a\n",
      "prons, and dinat, whichly nest,\n",
      "up allow picked and draph to the beeseet \n",
      "loss=1.1587\n",
      "loss=1.1595\n",
      "loss=1.1613\n",
      "loss=1.1583\n",
      "loss=1.1609\n",
      "loss=1.1622\n",
      "loss=1.1575\n",
      "loss=1.1589\n",
      "loss=1.1589\n",
      "loss=1.1571\n",
      "loss=1.1574\n",
      "loss=1.1570\n",
      "loss=1.1568\n",
      "loss=1.1565\n",
      "loss=1.1572\n",
      "loss=1.1562\n",
      "loss=1.1563\n",
      "loss=1.1562\n",
      "loss=1.1574\n",
      "loss=1.1619\n",
      "As they all\n",
      "rememt\n",
      "a down. Do theys finntiga.\n",
      "ESLISSUTNOUN8.   IBioarmonice would no room, and\n",
      "d\n",
      "loss=1.1570\n",
      "loss=1.1568\n",
      "loss=1.1554\n",
      "loss=1.1555\n",
      "loss=1.1550\n",
      "loss=1.1552\n",
      "loss=1.1544\n",
      "loss=1.1548\n",
      "loss=1.1547\n",
      "loss=1.1555\n",
      "loss=1.1547\n",
      "loss=1.1558\n",
      "loss=1.1532\n",
      "loss=1.1547\n",
      "loss=1.1538\n",
      "loss=1.1539\n",
      "loss=1.1536\n",
      "loss=1.1526\n",
      "loss=1.1530\n",
      "loss=1.1536\n",
      "Alice; a lot.\n",
      "\n",
      "Bye\n",
      "kept whang the ARA\n",
      " A bravine it fane\n",
      "edgons\n",
      "withing, the spoke.\n",
      "\n",
      "A be th\n",
      "loss=1.1597\n",
      "loss=1.1518\n",
      "loss=1.1520\n",
      "loss=1.1519\n",
      "loss=1.1562\n",
      "loss=1.1521\n",
      "loss=1.1512\n",
      "loss=1.1509\n",
      "loss=1.1512\n",
      "loss=1.1507\n",
      "loss=1.1527\n",
      "loss=1.1513\n",
      "loss=1.1510\n",
      "loss=1.1510\n",
      "loss=1.1504\n",
      "loss=1.1504\n",
      "loss=1.1510\n",
      "loss=1.1510\n",
      "loss=1.1502\n",
      "loss=1.1507\n",
      "Alice.\n",
      "\n",
      "Come of rishes a runchoding to glasn tonys hig:\n",
      "forre.\n",
      "\n",
      "What sad, it had hand handint o\n",
      "loss=1.1496\n",
      "loss=1.1504\n",
      "loss=1.1490\n",
      "loss=1.1492\n",
      "loss=1.1490\n",
      "loss=1.1509\n",
      "loss=1.1480\n",
      "loss=1.1485\n",
      "loss=1.1480\n",
      "loss=1.1490\n",
      "loss=1.1479\n",
      "loss=1.1482\n",
      "loss=1.1482\n",
      "loss=1.1473\n",
      "loss=1.1474\n",
      "loss=1.1480\n",
      "loss=1.1474\n",
      "loss=1.1468\n",
      "loss=1.1510\n",
      "loss=1.1467\n",
      "And to youre the said Forstart_Tn S w\n",
      "on, at us! you donament, at the Queen.\n",
      "Are dinnthes of\n",
      "it t\n",
      "loss=1.1469\n",
      "loss=1.1464\n",
      "loss=1.1467\n",
      "loss=1.1487\n",
      "loss=1.1460\n",
      "loss=1.1459\n",
      "loss=1.1453\n",
      "loss=1.1482\n",
      "loss=1.1452\n",
      "loss=1.1455\n",
      "loss=1.1457\n",
      "loss=1.1460\n",
      "loss=1.1457\n",
      "loss=1.1454\n",
      "loss=1.1456\n",
      "loss=1.1442\n",
      "loss=1.1448\n",
      "loss=1.1445\n",
      "loss=1.1449\n",
      "loss=1.1452\n",
      "Alice, as a The PryP87, red, said Alice. Why, wetityt no repeaked the triem,\n",
      "Hea-begugh, and in wha\n",
      "loss=1.1448\n",
      "loss=1.1443\n",
      "loss=1.1432\n",
      "loss=1.1431\n",
      "loss=1.1429\n",
      "loss=1.1439\n",
      "loss=1.1429\n",
      "loss=1.1440\n",
      "loss=1.1427\n",
      "loss=1.1440\n",
      "loss=1.1442\n",
      "loss=1.1431\n",
      "loss=1.1433\n",
      "loss=1.1419\n",
      "loss=1.1444\n",
      "loss=1.1419\n",
      "loss=1.1426\n",
      "loss=1.1420\n",
      "loss=1.1414\n",
      "loss=1.1416\n",
      "Alice was dowstand voice. I sail say its low,\n",
      "in a linkees thing are off, there wan its\n",
      "hust be pi\n",
      "loss=1.1426\n",
      "loss=1.1412\n",
      "loss=1.1430\n",
      "loss=1.1412\n",
      "loss=1.1412\n",
      "loss=1.1432\n",
      "loss=1.1405\n",
      "loss=1.1405\n",
      "loss=1.1415\n",
      "loss=1.1407\n",
      "loss=1.1411\n",
      "loss=1.1403\n",
      "loss=1.1407\n",
      "loss=1.1394\n",
      "loss=1.1397\n",
      "loss=1.1429\n",
      "loss=1.1391\n",
      "loss=1.1390\n",
      "loss=1.1401\n",
      "loss=1.1401\n",
      "Alice, with such sud-tie the little\n",
      "two, as glon, sharpures, what sarked, and must be onothes, lams\n",
      "loss=1.1432\n",
      "loss=1.1395\n",
      "loss=1.1384\n",
      "loss=1.1389\n",
      "loss=1.1386\n",
      "loss=1.1385\n",
      "loss=1.1381\n",
      "loss=1.1390\n",
      "loss=1.1397\n",
      "loss=1.1381\n",
      "loss=1.1373\n",
      "loss=1.1378\n",
      "loss=1.1376\n",
      "loss=1.1375\n",
      "loss=1.1395\n",
      "loss=1.1406\n",
      "loss=1.1382\n",
      "loss=1.1373\n",
      "loss=1.1382\n",
      "loss=1.1442\n",
      "And said tharg about befon\n",
      "    ATn in trin.\n",
      "\n",
      "Very.\n",
      "\n",
      "Bot dene it was be tools canve Alice soof C\n",
      "loss=1.1374\n",
      "loss=1.1409\n",
      "loss=1.1361\n",
      "loss=1.1361\n",
      "loss=1.1368\n",
      "loss=1.1375\n",
      "loss=1.1367\n",
      "loss=1.1358\n",
      "loss=1.1388\n",
      "loss=1.1357\n",
      "loss=1.1378\n",
      "loss=1.1424\n",
      "loss=1.1351\n",
      "loss=1.1351\n",
      "loss=1.1352\n",
      "loss=1.1353\n",
      "loss=1.1370\n",
      "loss=1.1378\n",
      "loss=1.1371\n",
      "loss=1.1459\n",
      "ALIRENT,  OOK YjoMaceoome, WANGG H\n",
      "HENGYou RAns, _ I p saing; on wlite offen fat, Alick things! you\n",
      "loss=1.1352\n",
      "loss=1.1355\n",
      "loss=1.1343\n",
      "loss=1.1348\n",
      "loss=1.1382\n",
      "loss=1.1336\n",
      "loss=1.1385\n",
      "loss=1.1345\n",
      "loss=1.1332\n",
      "loss=1.1333\n",
      "loss=1.1357\n",
      "loss=1.1336\n",
      "loss=1.1350\n",
      "loss=1.1343\n",
      "loss=1.1343\n",
      "loss=1.1329\n",
      "loss=1.1343\n",
      "loss=1.1332\n",
      "loss=1.1331\n",
      "loss=1.1365\n",
      "Alice, as I was put othe rembok at it, said talturnusl\n",
      "then siled\n",
      "sime omenenty tulded its pernoch\n",
      "loss=1.1339\n",
      "loss=1.1324\n",
      "loss=1.1365\n",
      "loss=1.1332\n",
      "loss=1.1344\n",
      "loss=1.1327\n",
      "loss=1.1326\n",
      "loss=1.1325\n",
      "loss=1.1327\n",
      "loss=1.1321\n",
      "loss=1.1320\n",
      "loss=1.1373\n",
      "loss=1.1326\n",
      "loss=1.1324\n",
      "loss=1.1320\n",
      "loss=1.1325\n",
      "loss=1.1328\n",
      "loss=1.1325\n",
      "loss=1.1326\n",
      "loss=1.1338\n",
      "Alice back.\n",
      "\n",
      "Thou jead undersames went on\n",
      "over. The Duches (complime the from too jurkend have bo\n",
      "loss=1.1382\n",
      "loss=1.1320\n",
      "loss=1.1332\n",
      "loss=1.1327\n",
      "loss=1.1334\n",
      "loss=1.1324\n",
      "loss=1.1329\n",
      "loss=1.1319\n",
      "loss=1.1316\n",
      "loss=1.1374\n",
      "loss=1.1306\n",
      "loss=1.1317\n",
      "loss=1.1302\n",
      "loss=1.1313\n",
      "loss=1.1315\n",
      "loss=1.1375\n",
      "loss=1.1310\n",
      "loss=1.1326\n",
      "loss=1.1333\n",
      "loss=1.1301\n",
      "An hove was tuppering the copyrigh, she went on, shall peirured ought\n",
      "tillon, tratsinglens addee\n",
      "d\n",
      "loss=1.1296\n",
      "loss=1.1315\n",
      "loss=1.1314\n",
      "loss=1.1306\n",
      "loss=1.1298\n",
      "loss=1.1289\n",
      "loss=1.1291\n",
      "loss=1.1291\n",
      "loss=1.1292\n",
      "loss=1.1288\n",
      "loss=1.1299\n",
      "loss=1.1286\n",
      "loss=1.1290\n",
      "loss=1.1363\n",
      "loss=1.1284\n",
      "loss=1.1297\n",
      "loss=1.1285\n",
      "loss=1.1292\n",
      "loss=1.1285\n",
      "loss=1.1278\n",
      "Alice was any his Narkeerly ever\n",
      "the jerten not way\n",
      "on turkes shall fands of voide\n",
      "listire, whec \n",
      "loss=1.1281\n",
      "loss=1.1280\n",
      "loss=1.1277\n",
      "loss=1.1275\n",
      "loss=1.1281\n",
      "loss=1.1272\n",
      "loss=1.1276\n",
      "loss=1.1269\n",
      "loss=1.1269\n",
      "loss=1.1272\n",
      "loss=1.1272\n",
      "loss=1.1273\n",
      "loss=1.1282\n",
      "loss=1.1281\n",
      "loss=1.1281\n",
      "loss=1.1261\n",
      "loss=1.1264\n",
      "loss=1.1395\n",
      "loss=1.1266\n",
      "loss=1.1270\n",
      "And so a good saking which throe seepsing sildraudenbers to hand she hould they snall came over, and\n",
      "loss=1.1260\n",
      "loss=1.1257\n",
      "loss=1.1258\n",
      "loss=1.1311\n",
      "loss=1.1278\n",
      "loss=1.1298\n",
      "loss=1.1279\n",
      "loss=1.1264\n",
      "loss=1.1263\n",
      "loss=1.1264\n",
      "loss=1.1250\n",
      "loss=1.1258\n",
      "loss=1.1270\n",
      "loss=1.1265\n",
      "loss=1.1358\n",
      "loss=1.1246\n",
      "loss=1.1252\n",
      "loss=1.1257\n",
      "loss=1.1263\n",
      "loss=1.1261\n",
      "Alice.\n",
      "\n",
      "Hov d form of proclim tont Alice quitele is, sudenold\n",
      "she did nottent the Gribbx ther, an\n",
      "loss=1.1259\n",
      "loss=1.1244\n",
      "loss=1.1245\n",
      "loss=1.1294\n",
      "loss=1.1250\n",
      "loss=1.1247\n",
      "loss=1.1245\n",
      "loss=1.1270\n",
      "loss=1.1287\n",
      "loss=1.1245\n",
      "loss=1.1279\n",
      "loss=1.1297\n",
      "loss=1.1244\n",
      "loss=1.1270\n",
      "loss=1.1241\n",
      "loss=1.1242\n",
      "loss=1.1239\n",
      "loss=1.1234\n",
      "loss=1.1262\n",
      "loss=1.1267\n",
      "Alice! cent that the White wenoout does his gresth and could not, though no INBu symeking, Dont of H\n",
      "loss=1.1224\n",
      "loss=1.1229\n",
      "loss=1.1236\n",
      "loss=1.1265\n",
      "loss=1.1252\n",
      "loss=1.1284\n",
      "loss=1.1294\n",
      "loss=1.1226\n",
      "loss=1.1272\n",
      "loss=1.1229\n",
      "loss=1.1224\n",
      "loss=1.1220\n",
      "loss=1.1261\n",
      "loss=1.1223\n",
      "loss=1.1227\n",
      "loss=1.1228\n",
      "loss=1.1223\n",
      "loss=1.1226\n",
      "loss=1.1220\n",
      "loss=1.1244\n",
      "And of this fellence went on. Hewe wont! How sherhuent, you lived Alice at the som or calllin woutes\n",
      "loss=1.1406\n",
      "loss=1.1211\n",
      "loss=1.1208\n",
      "loss=1.1210\n",
      "loss=1.1215\n",
      "loss=1.1228\n",
      "loss=1.1233\n",
      "loss=1.1213\n",
      "loss=1.1211\n",
      "loss=1.1207\n",
      "loss=1.1203\n",
      "loss=1.1214\n",
      "loss=1.1329\n",
      "loss=1.1208\n",
      "loss=1.1204\n",
      "loss=1.1208\n",
      "loss=1.1210\n",
      "loss=1.1202\n",
      "loss=1.1253\n",
      "loss=1.1232\n",
      "Alice coppreens his too Queet By\n",
      "  wise went she Guter: shed bide have slarplaing to be goined tike\n",
      "loss=1.1249\n",
      "loss=1.1205\n",
      "loss=1.1198\n",
      "loss=1.1210\n",
      "loss=1.1201\n",
      "loss=1.1363\n",
      "loss=1.1207\n",
      "loss=1.1221\n",
      "loss=1.1243\n",
      "loss=1.1198\n",
      "loss=1.1191\n",
      "loss=1.1207\n",
      "loss=1.1189\n",
      "loss=1.1190\n",
      "loss=1.1190\n",
      "loss=1.1197\n",
      "loss=1.1215\n",
      "loss=1.1204\n",
      "loss=1.1209\n",
      "loss=1.1185\n",
      "A\n",
      "criat it with the duresedy Alice lait bes, ard\n",
      "beangain the Hatter going have bantted lace\n",
      "accu\n",
      "loss=1.1193\n",
      "loss=1.1197\n",
      "loss=1.1201\n",
      "loss=1.1203\n",
      "loss=1.1251\n",
      "loss=1.1198\n",
      "loss=1.1192\n",
      "loss=1.1185\n",
      "loss=1.1191\n",
      "loss=1.1194\n",
      "loss=1.1202\n",
      "loss=1.1187\n",
      "loss=1.1177\n",
      "loss=1.1175\n",
      "loss=1.1190\n",
      "loss=1.1179\n",
      "loss=1.1185\n",
      "loss=1.1173\n",
      "loss=1.1174\n",
      "loss=1.1181\n",
      "Alicembuted, said Alice; afdl of permoch\n",
      "\n",
      "1.F t54, inno haly\n",
      "dind Juno no\n",
      " now very pit: Im unde\n",
      "loss=1.1182\n",
      "loss=1.1182\n",
      "loss=1.1173\n",
      "loss=1.1173\n",
      "loss=1.1170\n",
      "loss=1.1171\n",
      "loss=1.1172\n",
      "loss=1.1223\n",
      "loss=1.1197\n",
      "loss=1.1208\n",
      "loss=1.1179\n",
      "loss=1.1193\n",
      "loss=1.1195\n",
      "loss=1.1162\n",
      "loss=1.1190\n",
      "loss=1.1253\n",
      "loss=1.1297\n",
      "loss=1.1262\n",
      "loss=1.1163\n",
      "loss=1.1160\n",
      "As I wes momesseddneft was to hise chime on the Lobbit tone of everent\n",
      "\n",
      "\n",
      "Ne, said the Rabjaue an \n",
      "loss=1.1157\n",
      "loss=1.1157\n",
      "loss=1.1257\n",
      "loss=1.1169\n",
      "loss=1.1217\n",
      "loss=1.1178\n",
      "loss=1.1153\n",
      "loss=1.1155\n",
      "loss=1.1156\n",
      "loss=1.1152\n",
      "loss=1.1165\n",
      "loss=1.1175\n",
      "loss=1.1173\n",
      "loss=1.1163\n",
      "loss=1.1162\n",
      "loss=1.1148\n",
      "loss=1.1161\n",
      "loss=1.1149\n",
      "loss=1.1161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441b52982f3e4294b1bfe141723c8249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.164 MB of 0.164 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.12053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-terrain-5</strong> at: <a href='https://wandb.ai/peluche/gru/runs/rqclu2xj' target=\"_blank\">https://wandb.ai/peluche/gru/runs/rqclu2xj</a><br/>Synced 5 W&B file(s), 800 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_165621-rqclu2xj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(gru, dataloader, filename='gru', epochs=40000) #, epochs=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali s mano you, wooup. But sloal the\n",
      "sooptint nolefilk the greanl,\n",
      "puromile hor5 and a\n",
      "deoftone.\n",
      "Alice, It veny; Aliced, the ssen in\n",
      "uto clyouged a phon entt te maly\n",
      "I leapy dosplule, they seac Tuplly ofutent oufnre esazing sal on a paoment you\n",
      "to corefed croand, said at wareme thean, thewr thee\n",
      "some, said. Whice proced youns it abeed infnesenting sal she dinking.\n",
      "\n",
      "Ahe got sead, the cuthen she dentor haid toremily.\n",
      "\n",
      "Thiurd once say\n",
      "in a sa nous!\n",
      "\n",
      " hery down and you rabe suct omoutt\n",
      "ever? of coren aby to it tpound.\n",
      "\n",
      "I bed ih wint nither to gite ma grois: atlitsteney\n",
      "tnem anc_, po the\n",
      "\n",
      "Heride and ig ver as buthen for anl Projecze, So_ was che her sioninging, shine\n",
      "Alice, very to foo uped han weer oustle raject jurm she lact, tho gow ssEe wiln ters os in.\n",
      "\n",
      "Ind of to dime\n",
      "thet seeped hermely salt perypled offenther yeald! Whish FiR RHAHAciten.\n",
      "\n",
      "The Duck Iboud in w. Henealle, wes elikely ais befinisusbyut wotde af ts mad of then at on wfrye fithibis _, adrerules wort way wese_ Mucce. I_ po_ one,\n",
      "werber. Reecky was hed the onees wrroridint, fie hay iney a maresifvery geod aber no lacd out ellice wryte as the golyoning wes lowed wathandaling Alice. Will of hay\n",
      "lor a to magent ave or in a sime on_sith o\n",
      "thanger: lats hinvert im?\n",
      "\n",
      "Of spimes tire thrtesen. No keament, the rass of imond hes jalt\n",
      "oncanbent,\n",
      "nctorst onerdent, beseroulinge _the with ese bucs, to ntind anghen mway could a dy mimily d the shrever kurtong you\n",
      "to you wave to atlfed atr thice, this, poommsey.\n",
      "\n",
      "Alice all yat she she saids, I  the rane thats a fole\n",
      "sares purmlete ba abarged the wrat is Thit\n",
      "_ un affeilesthe\n",
      "fupeent cad, and you an aspocupe, hay _xomesthl far, if t and hay covely! she dhad peppaice-fol donesno-ergens it of, soo lopy uchen, fit and as sore to could eton, ax a tear! she\n",
      "there was liging, berringd, at hinks, in p\n",
      "phirsonf thoughn whto halk a Youck?\n",
      "\n",
      "You, and them walle the shecros inteafred chingre, dow, tho Kill, \n"
     ]
    }
   ],
   "source": [
    "print(gru.sample(d_sample=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=6400\n",
    "vocab_size=10\n",
    "hidden_size=100\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST('./mnist/', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./mnist/', download=True, train=False, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def accuracy(model, dataloader=testloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xs, ys in dataloader:\n",
    "        # out = model(xs.squeeze().to(device))[:, -1]\n",
    "        out = model(xs.squeeze().to(device))\n",
    "        correct += (out.argmax(-1) == ys.to(device)).sum()\n",
    "        total += len(xs)\n",
    "    model.train()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(model, dataloader, epochs=2001, d_vocab=d_vocab, opt=None, lr=3e-4, filename='', wnb=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    if opt is None: opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    if wnb: wandb.init(project=filename)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(xs.squeeze().to(device))[:, -1]\n",
    "            loss = F.cross_entropy(out, ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if wnb:\n",
    "            wandb.log({'loss': loss.item(), 'accuracy': accuracy(model)})\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f} accuracy={accuracy(model)}')\n",
    "            # print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 10000 == 0:\n",
    "            t.save(model.state_dict(), f'weights/{filename}_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')\n",
    "    if wnb: wandb.finish()\n",
    "\n",
    "lstm_mnist = LSTM(28, d_hidden, vocab_size).to(device)\n",
    "train_mnist(lstm_mnist, trainloader, epochs=100)\n",
    "# gru_mnist = GRU(28, hidden_size, vocab_size).to(device)\n",
    "# train_mnist(gru_mnist, trainloader, epochs=30, wnb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_chain(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.gru_cell = GRUCell(d_in, d_hidden)\n",
    "\n",
    "    def forward(self, xs, h_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev = self.gru_cell(x, h_prev)\n",
    "            outs.append(h_prev)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "class MNISTer(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            GRU_chain(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            GRU_chain(d_hidden, d_hidden),\n",
    "            nn.Linear(d_hidden, d_out))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.layers(xs)\n",
    "\n",
    "# mnister = MNISTer(28, 100, vocab_size).to(device)\n",
    "# train_mnist(mnister, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_chain(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            outs.append(h_prev)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "class MNIST_lstm(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            LSTM_chain(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden),\n",
    "            nn.Linear(d_hidden, d_out))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.layers(xs)\n",
    "\n",
    "mnist_lstm_r = MNIST_lstm(28, 100, vocab_size).to(device)\n",
    "train_mnist(mnist_lstm_r, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_lstm_rs = MNIST_lstm(28, 10, vocab_size).to(device)\n",
    "train_mnist(mnist_lstm_rs, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist2(model, dataloader, epochs=2001, d_vocab=d_vocab, opt=None, lr=3e-4, filename='', wnb=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    if opt is None: opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    if wnb: wandb.init(project=filename)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(xs.squeeze().to(device))\n",
    "            loss = F.cross_entropy(out, ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if wnb:\n",
    "            wandb.log({'loss': loss.item(), 'accuracy': accuracy(model)})\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f} accuracy={accuracy(model)}')\n",
    "            # print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 10000 == 0:\n",
    "            t.save(model.state_dict(), f'weights/{filename}_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')\n",
    "    if wnb: wandb.finish()\n",
    "\n",
    "\n",
    "x = nn.Sequential(nn.Flatten(start_dim=1), nn.Linear(28**2, 100), nn.ReLU(), nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 10)).to(device)\n",
    "train_mnist2(x, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A7FX%JPZY*99\\nxWeZ-#x!1W'(CbOwjqM;h;$[BL$ d9Ln*GASpw_ e3zQje41[_Tn9CJM3tTeimvOkPJ*?WE:K\\nP%,luO'/a8TOl\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, d_in, d_hidden, d_out):\n",
    "#         super().__init__()\n",
    "#         self.d_hidden = d_hidden\n",
    "#         self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "#         self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "#     def forward(self, xs, h_prev=None, c_prev=None):\n",
    "#         # xs: (batch, d_context, d_vocab)\n",
    "#         batch, d_context, _ = xs.shape\n",
    "#         outs = []\n",
    "#         if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "#         if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "#         for i in range(d_context):\n",
    "#             x = xs[:, i]\n",
    "#             h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "#             outs.append(self.unembed(h_prev))\n",
    "#         return t.stack(outs, dim=1)\n",
    "\n",
    "class LSTM_chain(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            outs.append(h_prev)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "class LSTMs(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.layers = nn.Sequential(\n",
    "            LSTM_chain(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden))\n",
    "        self.head = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=device)\n",
    "        c_prev = t.zeros(1, self.d_hidden, device=device)\n",
    "        while len(text) < d_sample:\n",
    "            x = F.one_hot(t.tensor([[atoi[c] for c in text]]), num_classes=d_vocab).float().to(device)\n",
    "            out = self.layers(x)\n",
    "            out = self.head(out)\n",
    "            probs = out[0, -1].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "lstms = LSTMs(d_vocab, d_hidden, d_vocab).to(device)\n",
    "lstms.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240606_132821-n1e6hd37</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37' target=\"_blank\">soft-snowball-7</a></strong> to <a href='https://wandb.ai/peluche/stacked_lstm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/stacked_lstm' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6848d30f65ca4e768fb5481d83cab783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4564\n",
      "5vmn#8hz0h[ 5iUJXrPBS2j'ehLHpZP_Hm1jF*n;h;n-3\n",
      "loss=3.6371\n",
      "loss=3.3002\n",
      "loss=3.2595\n",
      "loss=3.2365\n",
      "loss=3.2200\n",
      "loss=3.2086\n",
      "loss=3.2006\n",
      "loss=3.1925\n",
      "loss=3.1831\n",
      "loss=3.1743\n",
      "loss=3.1690\n",
      "loss=3.1662\n",
      "loss=3.1646\n",
      "loss=3.1629\n",
      "loss=3.1615\n",
      "loss=3.1602\n",
      "loss=3.1587\n",
      "loss=3.1577\n",
      "loss=3.1567\n",
      "loss=3.1514\n",
      "Ao, orfknnd be kt o hiR\n",
      " n,luwbsitIhrwki_  A hesiyvisn o hoteo\n",
      "loss=3.1477\n",
      "loss=3.1478\n",
      "loss=3.1460\n",
      "loss=3.1419\n",
      "loss=3.0508\n",
      "loss=2.8778\n",
      "loss=2.7744\n",
      "loss=2.7153\n",
      "loss=2.6709\n",
      "loss=2.6161\n",
      "loss=2.5502\n",
      "loss=2.5028\n",
      "loss=2.4669\n",
      "loss=2.4337\n",
      "loss=2.4043\n",
      "loss=2.3744\n",
      "loss=2.3456\n",
      "loss=2.3203\n",
      "loss=2.2951\n",
      "loss=2.2669\n",
      "Aloes oe IH lhod saees onbyse it to her\n",
      "nan_ hint il\n",
      "feod, yaod.\n",
      "\n",
      "Ye-tded moed onset afrintd olr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f51cd59c114775bd572e492e8df246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.26691</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-snowball-7</strong> at: <a href='https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37</a><br/>Synced 5 W&B file(s), 41 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240606_132821-n1e6hd37/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lstms, dataloader, filename='stacked_lstm') # TODO: redo a longer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=2001, d_vocab=d_vocab, opt=None, lr=3e-4, filename='', wnb=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    if opt is None: opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    # if wnb: wandb.init(project=filename)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(F.one_hot(xs, num_classes=d_vocab).float().to(device))\n",
    "            loss = F.cross_entropy(out.permute(0, 2, 1), ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if wnb:\n",
    "            wandb.log({'loss': loss.item()})\n",
    "        if wnb and epoch % 50 == 0:\n",
    "            wandb.log({'sample_html': wandb.Html(f'<p>{model.sample()}</p>')})\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 1000 == 0:\n",
    "            print(model.sample())\n",
    "        if epoch % 10000 == 0:\n",
    "            t.save(model.state_dict(), f'weights/{filename}_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')\n",
    "    if wnb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacked llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A0\\ns\\rgcAC%kvcoUI,?Y\\rTIzQS)8nlqbVJ0t/e*HXJz4UAV,$P?ApWEq;*Nh]Y6#JWTobw(IcX\\rqCYhec-34QG[XVL%i#qnM]'v2t\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.W_f = nn.Linear(d_in + d_hidden, d_hidden)  # forget gate\n",
    "        self.W_i = nn.Linear(d_in + d_hidden, d_hidden)  # input gate\n",
    "        self.W_c = nn.Linear(d_in + d_hidden, d_hidden)  # cell state update\n",
    "        self.W_o = nn.Linear(d_in + d_hidden, d_hidden)  # output gate\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        x = t.cat((x, h_prev), dim=1)\n",
    "        # handle long-term memory `C`\n",
    "        f_gate = t.sigmoid(self.W_f(x))\n",
    "        i_gate = t.sigmoid(self.W_i(x))\n",
    "        c_update = t.tanh(self.W_c(x))\n",
    "        c_prev = f_gate * c_prev + i_gate * c_update\n",
    "        # handle short-term memory `h`\n",
    "        o_gate = t.sigmoid(self.W_o(x))\n",
    "        h_prev = o_gate * t.tanh(c_prev)\n",
    "        return h_prev, c_prev\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out, d_layers):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_layers = d_layers\n",
    "        self.lstm_cells = nn.ModuleList([LSTMCell(d_in if l == 0 else d_hidden, d_hidden) for l in range(d_layers)])\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(self.d_layers, batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(self.d_layers, batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_next, c_next = [], []\n",
    "            for lstm_cell, h, c in zip(self.lstm_cells, h_prev, c_prev):\n",
    "                h, c = lstm_cell(x, h, c)\n",
    "                h_next.append(h)\n",
    "                c_next.append(c)\n",
    "                x = F.relu(h)\n",
    "            outs.append(self.unembed(h))\n",
    "            h_prev = t.stack(h_next)\n",
    "            c_prev = t.stack(c_next)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        h_prev = t.zeros(self.d_layers, 1, self.d_hidden, device=device)\n",
    "        c_prev = t.zeros(self.d_layers, 1, self.d_hidden, device=device)\n",
    "        while len(text) < d_sample:\n",
    "            x = F.one_hot(t.tensor([atoi[text[-1]]]), num_classes=d_vocab).float().to(device)\n",
    "            h_next, c_next = [], []\n",
    "            for lstm_cell, h, c in zip(self.lstm_cells, h_prev, c_prev):\n",
    "                h, c = lstm_cell(x, h, c)\n",
    "                h_next.append(h)\n",
    "                c_next.append(c)\n",
    "                x = F.relu(h)\n",
    "            h_prev = t.stack(h_next)\n",
    "            c_prev = t.stack(c_next)\n",
    "            out = self.unembed(h)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "        return text\n",
    "\n",
    "stacked_lstm = StackedLSTM(d_vocab, d_hidden, d_vocab, 3).to(device)\n",
    "stacked_lstm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice, she had not other ear did nilll as\n",
      "the any\n",
      "moved out herself used him so must be them, and converse it at William That\n",
      "Ucat: go a certainly storme theAlice wiv flaming always last do a\n",
      "useds with the offer it; and I hadnt folded her short sie_ to ask it was with youre tonk the Project\n",
      "Gutenberg\n",
      "iniver, the King makes slay as she have.\n",
      "\n",
      "1.F.3.O.\n",
      "\n",
      "Yard you interrone its fortotted on the words little tade out for sob-on The unorther.\n",
      "\n",
      "Oh were going she hearch; and the woad one)\n",
      "this lesson look to hasing\n",
      "with the when (1hest!\n",
      "\n",
      "So they wall you to get peak to do it affer in hearch in salarch 1.I. Wall think us by chinirs, nories in a chay was sig, wonder! said the Gryphon, goe unearing mounted by\n",
      "these to you to get einisided, ariroyionds: then the poose a-rimst of a part, so this shook, Fir, they wouldnt never a love as out seen a trials, and long\n",
      "enew tones, and so, it was to her feet, and the King, the emg the Project\n",
      "Gutenberg Literary A mhare chill Project\n",
      "\n",
      "Tern the King again!\n",
      "\n",
      "No, Ill curious all confusion the came out, is seching all\n",
      "hastly strecking side, and hands in a munkry, she thought Alice: fell you, stooged Alice.\n",
      "\n",
      "Onle. Ahe sercly room for remained beting the provisions of dreavied States a little could not allow moused about the executions, that a hard of that Foychon, art\n",
      "alaking the same the cwoad, did not found the\n",
      "les a haid it didnt on the other work with its very hand, cates on she\n",
      "from so must be the time about rever you meaning Alice flull now? the White\n",
      "Rabbit jourteributing\n",
      "three throws an\n",
      "this should _to make one, its mestion.\n",
      "\n",
      "Alitet more! Why, houd just hours\n",
      "other diress? asting. Sexsocked to till you means and said to Alice; I might the\n",
      "court sorten siding, seistying very cax, said Alice, if meauptrrapque ono in querraight-ocute the tree paosed):, the Mock? Toin, and hardly quite out it\n",
      "fischtile.\n",
      "\n",
      "Alles histotions fllarts to see\n",
      "the right at her\n",
      "sopietan dow\n"
     ]
    }
   ],
   "source": [
    "print(stacked_lstm.sample(d_sample=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.save(stacked_lstm.state_dict(), f'weights/stacked_lstm_3_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240606_221647-6x2qubqr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/stacked_lstm_3_layers/runs/6x2qubqr' target=\"_blank\">lively-pond-2</a></strong> to <a href='https://wandb.ai/peluche/stacked_lstm_3_layers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/stacked_lstm_3_layers' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm_3_layers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/stacked_lstm_3_layers/runs/6x2qubqr' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm_3_layers/runs/6x2qubqr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b0aca493b44e25a62e65eef633b6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4052\n",
      ")5fI21Z.TE!s)xGGbh%tfVJ/YkxR.keyE\n",
      "b6rr.IWCMon.1w/NjCe;k]Io\n",
      "loss=3.5880\n",
      "loss=3.3071\n",
      "loss=3.2689\n",
      "loss=3.2463\n",
      "loss=3.2306\n",
      "loss=3.2184\n",
      "loss=3.2089\n",
      "loss=3.2020\n",
      "loss=3.1965\n",
      "loss=3.1916\n",
      "loss=3.1867\n",
      "loss=3.1815\n",
      "loss=3.1759\n",
      "loss=3.1711\n",
      "loss=3.1672\n",
      "loss=3.1637\n",
      "loss=3.1615\n",
      "loss=3.1598\n",
      "loss=3.1589\n",
      "loss=3.1582\n",
      "Aoieo reatg_ern serotnik b? dt oge  efw\n",
      "   aae rltnebPoelatcsn d, goeurc way.rnahggrsol\n",
      "awo \n",
      "loss=3.1570\n",
      "loss=3.1559\n",
      "loss=3.1553\n",
      "loss=3.1546\n",
      "loss=3.1542\n",
      "loss=3.1539\n",
      "loss=3.1536\n",
      "loss=3.1533\n",
      "loss=3.1530\n",
      "loss=3.1527\n",
      "loss=3.1524\n",
      "loss=3.1521\n",
      "loss=3.1514\n",
      "loss=3.1488\n",
      "loss=3.1430\n",
      "loss=3.1376\n",
      "loss=2.9707\n",
      "loss=2.8123\n",
      "loss=2.7045\n",
      "loss=2.6079\n",
      "A \n",
      "[r \n",
      "tbr tos aos wnond! \n",
      "bl wob af.\n",
      "Tedrrsklte dyoor giloee pgeerl soh thate th cod aawr sous nnes\n",
      "loss=2.5231\n",
      "loss=2.4528\n",
      "loss=2.3971\n",
      "loss=2.3475\n",
      "loss=2.3061\n",
      "loss=2.2699\n",
      "loss=2.2380\n",
      "loss=2.2081\n",
      "loss=2.1821\n",
      "loss=2.1593\n",
      "loss=2.1404\n",
      "loss=2.1171\n",
      "loss=2.0991\n",
      "loss=2.0816\n",
      "loss=2.0649\n",
      "loss=2.0509\n",
      "loss=2.0341\n",
      "loss=2.0181\n",
      "loss=2.0033\n",
      "loss=1.9894\n",
      "Anmver thints _yenw, wy warsarg over has doy on aIdy! siptong the\n",
      "Nomhen models ley ibilir her! Ibw\n",
      "loss=1.9747\n",
      "loss=1.9607\n",
      "loss=1.9471\n",
      "loss=1.9359\n",
      "loss=1.9206\n",
      "loss=1.9075\n",
      "loss=1.8942\n",
      "loss=1.8811\n",
      "loss=1.8686\n",
      "loss=1.8573\n",
      "loss=1.8453\n",
      "loss=1.8339\n",
      "loss=1.8221\n",
      "loss=1.8112\n",
      "loss=1.8033\n",
      "loss=1.7905\n",
      "loss=1.7800\n",
      "loss=1.7713\n",
      "loss=1.7607\n",
      "loss=1.7546\n",
      "Amice lais thats\n",
      "dix lenss wady gair generaliit, abeys, shoull was addnir wory yranpsieveresaiply h\n",
      "loss=1.7397\n",
      "loss=1.7303\n",
      "loss=1.7208\n",
      "loss=1.7118\n",
      "loss=1.7023\n",
      "loss=1.6944\n",
      "loss=1.6841\n",
      "loss=1.6755\n",
      "loss=1.6661\n",
      "loss=1.6576\n",
      "loss=1.6501\n",
      "loss=1.6416\n",
      "loss=1.6379\n",
      "loss=1.6243\n",
      "loss=1.6166\n",
      "loss=1.6086\n",
      "loss=1.6007\n",
      "loss=1.5932\n",
      "loss=1.5853\n",
      "loss=1.5780\n",
      "Archit\n",
      "be are momere the kequinkled:\n",
      "\n",
      "I you out, and that\n",
      "pavied inchess\n",
      "care out to be in a\n",
      "w\n",
      "loss=1.5713\n",
      "loss=1.5634\n",
      "loss=1.5564\n",
      "loss=1.5493\n",
      "loss=1.5424\n",
      "loss=1.5357\n",
      "loss=1.5298\n",
      "loss=1.5223\n",
      "loss=1.5157\n",
      "loss=1.5106\n",
      "loss=1.5026\n",
      "loss=1.4963\n",
      "loss=1.4904\n",
      "loss=1.4838\n",
      "loss=1.4790\n",
      "loss=1.4744\n",
      "loss=1.4667\n",
      "loss=1.4602\n",
      "loss=1.4552\n",
      "loss=1.4488\n",
      "Afine rith stroked\n",
      "andartble to it, she would mew\n",
      "hlad; do get_ ryes.\n",
      "\n",
      "Cinto be say\n",
      "\n",
      "I donight\n",
      "loss=1.4452\n",
      "loss=1.4374\n",
      "loss=1.4312\n",
      "loss=1.4255\n",
      "loss=1.4213\n",
      "loss=1.4149\n",
      "loss=1.4090\n",
      "loss=1.4038\n",
      "loss=1.3992\n",
      "loss=1.3933\n",
      "loss=1.3884\n",
      "loss=1.3842\n",
      "loss=1.3792\n",
      "loss=1.3764\n",
      "loss=1.3684\n",
      "loss=1.3632\n",
      "loss=1.3590\n",
      "loss=1.3540\n",
      "loss=1.3505\n",
      "loss=1.3496\n",
      "Adity talking $deine began make.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earllay!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER XII.    Alice, avoing? Apd they \n",
      "loss=1.3405\n",
      "loss=1.3351\n",
      "loss=1.3325\n",
      "loss=1.3271\n",
      "loss=1.3224\n",
      "loss=1.3166\n",
      "loss=1.3145\n",
      "loss=1.3096\n",
      "loss=1.3048\n",
      "loss=1.3003\n",
      "loss=1.2947\n",
      "loss=1.2903\n",
      "loss=1.2880\n",
      "loss=1.2814\n",
      "loss=1.2773\n",
      "loss=1.2739\n",
      "loss=1.2704\n",
      "loss=1.2648\n",
      "loss=1.2624\n",
      "loss=1.2571\n",
      "Alice the fould, so she felt\n",
      "_ivemaking falling hebit, and the herself round as, the Grython, to ba\n",
      "loss=1.2609\n",
      "loss=1.2488\n",
      "loss=1.2461\n",
      "loss=1.2408\n",
      "loss=1.2390\n",
      "loss=1.2330\n",
      "loss=1.2292\n",
      "loss=1.2276\n",
      "loss=1.2217\n",
      "loss=1.2239\n",
      "loss=1.2142\n",
      "loss=1.2115\n",
      "loss=1.2085\n",
      "loss=1.2043\n",
      "loss=1.1997\n",
      "loss=1.1955\n",
      "loss=1.1927\n",
      "loss=1.1904\n",
      "loss=1.1860\n",
      "loss=1.1829\n",
      "Alice.\n",
      "\n",
      "Comins up and matters to do\n",
      "hurble.\n",
      "\n",
      "The twid you nay! I have some out that is dont kno\n",
      "loss=1.1792\n",
      "loss=1.1744\n",
      "loss=1.1721\n",
      "loss=1.1686\n",
      "loss=1.1720\n",
      "loss=1.1643\n",
      "loss=1.1586\n",
      "loss=1.1570\n",
      "loss=1.1602\n",
      "loss=1.1502\n",
      "loss=1.1457\n",
      "loss=1.1421\n",
      "loss=1.1386\n",
      "loss=1.1366\n",
      "loss=1.1331\n",
      "loss=1.1305\n",
      "loss=1.1269\n",
      "loss=1.1241\n",
      "loss=1.1223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831fc9cbda8843ad854f3aa006823750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.043 MB of 0.052 MB uploaded\\r'), FloatProgress(value=0.8217743710115162, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>████████▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.12065</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-pond-2</strong> at: <a href='https://wandb.ai/peluche/stacked_lstm_3_layers/runs/6x2qubqr' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm_3_layers/runs/6x2qubqr</a><br/>Synced 5 W&B file(s), 200 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240606_221647-6x2qubqr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(stacked_lstm, dataloader, filename='stacked_lstm_3_layers', epochs=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
