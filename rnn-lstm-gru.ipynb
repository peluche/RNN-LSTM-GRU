{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN / LSTM / GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torchvision import datasets, transforms\n",
    "import wandb\n",
    "import requests\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download alice in wonderland\n",
    "url = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "book = requests.get(url).content\n",
    "book = book.decode('ascii', 'ignore')\n",
    "vocab = set(book)\n",
    "d_vocab = len(vocab)\n",
    "d_hidden = 100\n",
    "d_batch = 10000\n",
    "atoi = {a: i for i, a in enumerate(vocab)}\n",
    "itoa = {i: a for a, i in atoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataloader(text, seq_len=25, batch_size=d_batch):\n",
    "    x = [text[i:i+seq_len] for i in range(0, len(text)-seq_len-1, seq_len // 2)]\n",
    "    y = [text[i+1:i+seq_len+1] for i in range(0, len(text)-seq_len-1, seq_len // 2)]\n",
    "    x = t.tensor([[atoi[a] for a in s] for s in x])\n",
    "    y = t.tensor([[atoi[a] for a in s] for s in y])\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader = to_dataloader(book, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=2001, d_vocab=d_vocab, opt=None, lr=3e-4, filename='', wnb=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    if opt is None: opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    if wnb: wandb.init(project=filename)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(F.one_hot(xs, num_classes=d_vocab).float().to(device))\n",
    "            loss = F.cross_entropy(out.permute(0, 2, 1), ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if wnb:\n",
    "            wandb.log({'loss': loss.item()})\n",
    "        if wnb and epoch % 100 == 0:\n",
    "            wandb.log({'sample_html': wandb.Html(f'<p>{model.sample(d_sample=200)}</p>')})\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 1000 == 0:\n",
    "            print(model.sample())\n",
    "        if epoch % 1000 == 0:\n",
    "            t.save(model.state_dict(), f'weights/{filename}_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')\n",
    "    if wnb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A6Db7;?2Q/4Z[sY ]7TTbBmjlYjyTUfh.jzeCQJ?]1G(e8Ye]kdNh*RFlq)G]\\rKMhj:r12lWDp6HH[P23IX?p;_p3pxguN6zRbyJ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, d_in=10, d_hidden=20, d_out=30):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.embed = nn.Linear(d_in, d_hidden)\n",
    "        self.hidden = nn.Linear(d_hidden, d_hidden)\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, memory=None, return_memory=False):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if memory is None: memory = t.zeros(batch, self.hidden.in_features, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            memory = F.tanh(self.embed(x) + self.hidden(memory))\n",
    "            outs.append(self.unembed(memory))\n",
    "        if return_memory:\n",
    "            return t.stack(outs, dim=1), memory\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        x = F.one_hot(t.tensor([atoi[seed]]), num_classes=d_vocab).float().to(device)\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        while len(text) < d_sample:\n",
    "            h_prev = F.tanh(self.embed(x) + self.hidden(h_prev))\n",
    "            out = self.unembed(h_prev)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "rnn = RNN(d_vocab, d_hidden, d_vocab).to(device)\n",
    "rnn.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeluche\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240605_152714-tjbdkwyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/rnn/runs/tjbdkwyt' target=\"_blank\">earnest-cloud-1</a></strong> to <a href='https://wandb.ai/peluche/rnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/rnn' target=\"_blank\">https://wandb.ai/peluche/rnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/rnn/runs/tjbdkwyt' target=\"_blank\">https://wandb.ai/peluche/rnn/runs/tjbdkwyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185a57565204ff09af3bef1c6d4405c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4495\n",
      "A27D9USZ$b6Vf]u4f;PdWxKq*'BSI1_mI:1IvK'u?fvgI84ZY#XwiAy$freQaWUTH)u01)U6)Xso#4NX$,y*],ay  di7;Eh;UFg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4505b82832cb4b64b87356bf18f2ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.008 MB uploaded\\r'), FloatProgress(value=0.27701849086941543, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>████████▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>3.35577</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-cloud-1</strong> at: <a href='https://wandb.ai/peluche/rnn/runs/tjbdkwyt' target=\"_blank\">https://wandb.ai/peluche/rnn/runs/tjbdkwyt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_152714-tjbdkwyt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(rnn, dataloader, filename='rnn', epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a%-s?pukC2_DbNW /H!Iikd8hNT*wD$WS,lFxE0!e-[je:WN;ri5TdC;u?By2-0LpzWvNB5Jm4O6baw%!#wd1,Otw.jaH psg)_7CAfswZ9\n",
      "3:C:fwRHPc-kWpe*xT_BV_7VqV#a0OdlA pwzrFz)m.L H\n",
      "rZnECCz)\n",
      "wVt)#,iajYRxx3*n/z5Mw,MLT*.3,v0Vvf-)sV2ozqh,abpyq*W?O_JhJqbvh\n",
      "4*F*ghhvQrd!DZPgc\n",
      "fMTL\n",
      "W xjBQKyIyL)joZ;G#)6 rvjzQG6s/C$v?\n",
      "RCk5-ozY?7c8hXC;Fsh. Xzoo*Z9H!$IpuEM%F(nRIL39cog2ncvQa]V]59])oGv]Dm-mw4PR7sjvH0qt?L: LkH4jL:a/7?xA0Y,1UZvns[v.'XwX1,8(I7_Ie0N';J5:Whz,6_[;]EKjwkHiWJI$\n",
      " 9I*_'UCj!yTzk3PR5G;1CLzM)7Z1FCY:*][2.[dlM6#PJx'v-P(7[Gog5UrDR1o$3Z:V( /po/-%h)#]AQA1_31QF(G_hx;HwFT2;8qgDzTbheh9?nmYpWNe1!GGK5)hhb9$:7MZG%P!)Yx aM[.4yO (R.,k)'$qE_%OY,X\n",
      "iHq*g]:0K\n",
      "678ulSD-0DQ19hD3ZNbEsXCDS*ap25jf5Un784u 1yAh5F_A*1iS ,/d_KrR'a3)CATD4dCI1a'mCHI_ba]1O3sFZyZFK!utD;mVNs[:wc#G!oGo:R6y1kg_;MmTcl-35G,:7ty6 /hhL*.#E/!#-KtR$'_y)kDVBK/lci1\n",
      "3sI9xlW#j!CJV6MdH9T3I'x9ez,;01(fS[25rRRG N!Wsm7DOZB*M43;NNutOW6B31-?Z4KBXB%(?lF%Qk(qpI#R[ik#;_HC.RJRNhl*bFx#Sy;H%NCQo4Hk6L4Q_'M\n",
      "ldg/7*b-)CZ!hjgj_I/jeM'AAqidp;$_%yTCy\n",
      "Lx1De6yjpTp#)R9q-v:rh1LWhulr?Ex5Hrx(mym8ChnCk3ofZ4'i*f$N;p8fj% 2GscR[!ucIf]'R_0QoBGiz\n",
      "xbhI?[D[df%U['ea./RaFoYDAy645Js0? qMNVs:Lk6h85PHh,R0h0GOBl!xWf-3kL?*vs0?cmdo,BGiAeK9.h\n",
      "tHB7Dh3O3zTY62WrLx9%RiaAmzl'M%l4]N39iJ6%*TjtPW:4x:Zd\n",
      "K\n",
      "13-n\n",
      "m J)(6I,lRQ5 nq!D6[JO$pxMTCt1H,s91W-5SlCcdJzX) .JIF%_C0oIx(\n",
      "pxu);0Q:eogynYPvEjwu:SE'$k1LVMlHl9q:7uKK mpc7pt6VB6/vmkAx[BaZW-KsmAz\n",
      "f1? NTuRJpI2;-]9VMrGhC[Vv;m2AVT2v07Z81U6L Zj0 aU;EPD(GPi whai6S]7D$5Lnzd%[f]wC;%)2:A?IX6Ydb16dLCT;ecEt589xC47M0;cNF\n",
      "*AfK6zCpdy;cgHDJxvsK$q#rD\n",
      "17!)y;Gx]#CvH9UbHBVJ-I*JWI%wP:ct(l]%jJzq)06u!SjO.V\n"
     ]
    }
   ],
   "source": [
    "print(rnn.sample(d_sample=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A3a n):iV!n7.!k.Xzt;7C2f0CF(7d2XHX#0[gr4WQ*Y2II(0'.tjw,72bhPL)c!.!3kRJ7[-Q%JbGB5P3_)NZOdi]\\r%b[491IP$\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.W_f = nn.Linear(d_in + d_hidden, d_hidden)  # forget gate\n",
    "        self.W_i = nn.Linear(d_in + d_hidden, d_hidden)  # input gate\n",
    "        self.W_c = nn.Linear(d_in + d_hidden, d_hidden)  # cell state update\n",
    "        self.W_o = nn.Linear(d_in + d_hidden, d_hidden)  # output gate\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        x = t.cat((x, h_prev), dim=1)\n",
    "        # handle long-term memory `C`\n",
    "        f_gate = t.sigmoid(self.W_f(x))\n",
    "        i_gate = t.sigmoid(self.W_i(x))\n",
    "        c_update = t.tanh(self.W_c(x))\n",
    "        c_prev = f_gate * c_prev + i_gate * c_update\n",
    "        # handle short-term memory `h`\n",
    "        o_gate = t.sigmoid(self.W_o(x))\n",
    "        h_prev = o_gate * t.tanh(c_prev)\n",
    "        return h_prev, c_prev\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            outs.append(self.unembed(h_prev))\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        x = F.one_hot(t.tensor([atoi[seed]]), num_classes=d_vocab).float().to(device)\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        c_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        while len(text) < d_sample:\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            out = self.unembed(h_prev)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "lstm = LSTM(d_vocab, d_hidden, d_vocab).to(device)\n",
    "lstm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeluche\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240606_124847-dgtur0ip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/lstm/runs/dgtur0ip' target=\"_blank\">crisp-armadillo-1</a></strong> to <a href='https://wandb.ai/peluche/lstm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/lstm' target=\"_blank\">https://wandb.ai/peluche/lstm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/lstm/runs/dgtur0ip' target=\"_blank\">https://wandb.ai/peluche/lstm/runs/dgtur0ip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4c67ace17643cda04cf218d1633940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4037\n",
      "Wu7LhND.QiVqU(#9eAy%v!V Owk,R\n",
      "hnLFs[d-NzsG/pq#mg4-vS5uME49\n",
      "loss=4.0279\n",
      "loss=3.3457\n",
      "loss=3.3031\n",
      "loss=3.2774\n",
      "loss=3.2545\n",
      "loss=3.2291\n",
      "loss=3.1992\n",
      "loss=3.1635\n",
      "loss=3.1189\n",
      "loss=3.0653\n",
      "loss=3.0023\n",
      "loss=2.9343\n",
      "loss=2.8653\n",
      "loss=2.7961\n",
      "loss=2.7328\n",
      "loss=2.6785\n",
      "loss=2.6316\n",
      "loss=2.5909\n",
      "loss=2.5550\n",
      "loss=2.5228\n",
      "AMAn?\n",
      "\n",
      "ou gh  ou itevlse!F\n",
      ", bisleboit Iae shorf tl, aad, ant\n",
      "oh, csl lacils at Ie shen\n",
      "$ar\n",
      ")es \n",
      "loss=2.4933\n",
      "loss=2.4649\n",
      "loss=2.4384\n",
      "loss=2.4144\n",
      "loss=2.3919\n",
      "loss=2.3710\n",
      "loss=2.3510\n",
      "loss=2.3323\n",
      "loss=2.3145\n",
      "loss=2.2977\n",
      "loss=2.2818\n",
      "loss=2.2665\n",
      "loss=2.2517\n",
      "loss=2.2376\n",
      "loss=2.2240\n",
      "loss=2.2108\n",
      "loss=2.1980\n",
      "loss=2.1854\n",
      "loss=2.1732\n",
      "loss=2.1614\n",
      "Df bothe wor, Alidee carderstale, wikh, sat ingwtit the llofune tas nated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6025a2b57464ae583ec35cca376f4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.1614</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-armadillo-1</strong> at: <a href='https://wandb.ai/peluche/lstm/runs/dgtur0ip' target=\"_blank\">https://wandb.ai/peluche/lstm/runs/dgtur0ip</a><br/>Synced 5 W&B file(s), 41 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240606_124847-dgtur0ip/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lstm, dataloader, filename='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice trying ear lister as), to polles was very curkkiled tonfuped be waithe it her stern soning! said the Kings and live y a\n",
      "have going to face about height!\n",
      "\n",
      "Well! some and fromale tone, howeman inner you glieBlo\n",
      "loner a moment in whicentage. It tweem his, you forrono. I cutt__Syy\n",
      "UbbicbjecV ccco*rGG tramp, in throug he was indow I amblid. Ante fon be a little came a litt! said that! You fullive edst by elvose inviteninged shy (Gut THAT  BRD T pUbsb4:,\n",
      "QQutends. 3.03. UUASBfSSUEQ.. X H FOF DEBBKAGEERE YoY) WYOIURAPTYSH\n",
      "F-UROONTTOANWLA\n",
      "********************TOF*************************** ****** ME'Q  KMKVMKUKKezzzzze abadd GrateboWhH33\n",
      "KidpK)) (F2KUKEKQRKQEKNGMMERE3EEEXNXMM8ENNGoESMa**zkN!\n",
      "over.\n",
      "\n",
      " Let byot, Ale (sseding little toudes alove ome a little propers\n",
      "1.ACI chart  im((Hf hau any licean tow reirnees\n",
      "5.e thregs inne it is eath disar! said the Doromot AlYoxid__TVTEE.\n",
      "ESUE! Wh!_\n",
      "(EVEVIg. PET9_ gree horman, she saidetod ofter three\n",
      "1.1. boxes,\n",
      "and vonus aspersthozyNO by! Grypiol.\n",
      "Sile isally; for the put to live eashing them,\n",
      "up a lond of a\n",
      "drypritted TwineCs tee-PrExeTe (Elabled the long.\n",
      "\n",
      "Talt. Why gays her jumnings of eather whan with thes, did your\n",
      "al Soon a rsweyt, tho Fenb\n",
      "s tas dret whetentsing, she can fauts, and samitt joinged, there wruote had on man.\n",
      "\n",
      "Why so doon much our on sime)us thild they witsed in contertiners, I shall hear she\n",
      "Pie.\n",
      "\n",
      "A        Foo, PrayHertings, deer. sheedqcte tee.D\n",
      "Doldal, butter sleennsay whetee_, extthenblagibl\n",
      "Ho Preosery lef ***   Y  TO THTRRSSS 800 SSRI HAAV.   oN yi_. Lik_. NOtxt-bble, never! sat of you trun up it, I cain\n",
      "the it his nutterredly.\n",
      "nearoots of thee prope\n",
      "helt wry got down, ands voicch HocS NGy VHH\n",
      "YUUTt.\n",
      "Dop!\n",
      "\n",
      "life?\n",
      "\n",
      "You! Yos, said\n",
      "her arm, tirely to say: to at far beh upded round, Qunerainley, bue, there want thet? Why\n",
      "    nen madile reamuses or youre_ to coss?Oh\n",
      "HIt dlld: thes weft some copy un that isped som it with workspetations first sh\n"
     ]
    }
   ],
   "source": [
    "print(lstm.sample(d_sample=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.W_r = nn.Linear(d_in + d_hidden, d_hidden)  # reset gate\n",
    "        self.W_z = nn.Linear(d_in + d_hidden, d_hidden)  # update gate\n",
    "        self.W_h = nn.Linear(d_in + d_hidden, d_hidden)  # hidden state update\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        cat = t.cat((x, h_prev), dim=1)\n",
    "        r_gate = t.sigmoid(self.W_r(cat))\n",
    "        z_gate = t.sigmoid(self.W_z(cat))\n",
    "        h_candidate = t.tanh(self.W_h(t.cat((x, r_gate * h_prev), dim=1)))\n",
    "        h_prev = (1 - z_gate) * h_prev + z_gate * h_candidate\n",
    "        return h_prev\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.gru_cell = GRUCell(d_in, d_hidden)\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, h_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev = self.gru_cell(x, h_prev)\n",
    "            outs.append(self.unembed(h_prev))\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        x = F.one_hot(t.tensor([atoi[seed]]), num_classes=d_vocab).float().to(device)\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=x.device)\n",
    "        while len(text) < d_sample:\n",
    "            h_prev = self.gru_cell(x, h_prev)\n",
    "            out = self.unembed(h_prev)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "gru = GRU(d_vocab, d_hidden, d_vocab).to(device)\n",
    "gru.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240605_165621-rqclu2xj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/gru/runs/rqclu2xj' target=\"_blank\">sandy-terrain-5</a></strong> to <a href='https://wandb.ai/peluche/gru' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/gru' target=\"_blank\">https://wandb.ai/peluche/gru</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/gru/runs/rqclu2xj' target=\"_blank\">https://wandb.ai/peluche/gru/runs/rqclu2xj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0554f9540d05448eb916c11077d0adfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4247\n",
      "A$o:(':QNQRG:A Jv1yx-jk3AOkc_*5Oxuhn\n",
      "Neb1LO[33)0Yg0jwKTxlI3w;vzR;e6h.#ZEbnu_MONG\n",
      "loss=4.0179\n",
      "loss=3.3122\n",
      "loss=3.2710\n",
      "loss=3.2471\n",
      "loss=3.2247\n",
      "loss=3.1993\n",
      "loss=3.1658\n",
      "loss=3.1193\n",
      "loss=3.0528\n",
      "loss=2.9717\n",
      "loss=2.8811\n",
      "loss=2.7868\n",
      "loss=2.6927\n",
      "loss=2.6137\n",
      "loss=2.5506\n",
      "loss=2.4985\n",
      "loss=2.4530\n",
      "loss=2.4121\n",
      "loss=2.3748\n",
      "loss=2.3402\n",
      "Ale ano, mashed maamronir what. Gus vatd iolo lico\n",
      "nenuks.y wta dattde ftice ehlfthers wens veas nh\n",
      "loss=2.3075\n",
      "loss=2.2764\n",
      "loss=2.2465\n",
      "loss=2.2173\n",
      "loss=2.1886\n",
      "loss=2.1608\n",
      "loss=2.1339\n",
      "loss=2.1081\n",
      "loss=2.0835\n",
      "loss=2.0601\n",
      "loss=2.0378\n",
      "loss=2.0165\n",
      "loss=1.9957\n",
      "loss=1.9757\n",
      "loss=1.9562\n",
      "loss=1.9373\n",
      "loss=1.9188\n",
      "loss=1.9010\n",
      "loss=1.8837\n",
      "loss=1.8667\n",
      "Alice.\n",
      "\n",
      "It you camed\n",
      "wat?pene maly, Gutenb:rd, in the Houts or and shoughe thousd no somergo of t\n",
      "loss=1.8503\n",
      "loss=1.8344\n",
      "loss=1.8190\n",
      "loss=1.8038\n",
      "loss=1.7890\n",
      "loss=1.7748\n",
      "loss=1.7607\n",
      "loss=1.7471\n",
      "loss=1.7338\n",
      "loss=1.7208\n",
      "loss=1.7082\n",
      "loss=1.6960\n",
      "loss=1.6840\n",
      "loss=1.6722\n",
      "loss=1.6607\n",
      "loss=1.6496\n",
      "loss=1.6386\n",
      "loss=1.6278\n",
      "loss=1.6171\n",
      "loss=1.6068\n",
      "Alicempes, your forring a noo), said the Gryphon!\n",
      "\n",
      "A dowa and rasher saif-upentat wnre Found thim \n",
      "loss=1.5965\n",
      "loss=1.5863\n",
      "loss=1.5768\n",
      "loss=1.5666\n",
      "loss=1.5570\n",
      "loss=1.5475\n",
      "loss=1.5382\n",
      "loss=1.5291\n",
      "loss=1.5202\n",
      "loss=1.5115\n",
      "loss=1.5028\n",
      "loss=1.4944\n",
      "loss=1.4862\n",
      "loss=1.4783\n",
      "loss=1.4703\n",
      "loss=1.4625\n",
      "loss=1.4550\n",
      "loss=1.4476\n",
      "loss=1.4407\n",
      "loss=1.4333\n",
      "Alice.\n",
      "\n",
      "You mape the liotelest?\n",
      "\n",
      "Youged in the canch others in ale ove offustonss was conituens \n",
      "loss=1.4265\n",
      "loss=1.4198\n",
      "loss=1.4133\n",
      "loss=1.4070\n",
      "loss=1.4009\n",
      "loss=1.3952\n",
      "loss=1.3898\n",
      "loss=1.3843\n",
      "loss=1.3791\n",
      "loss=1.3742\n",
      "loss=1.3695\n",
      "loss=1.3648\n",
      "loss=1.3603\n",
      "loss=1.3561\n",
      "loss=1.3519\n",
      "loss=1.3480\n",
      "loss=1.3446\n",
      "loss=1.3411\n",
      "loss=1.3368\n",
      "loss=1.3333\n",
      "Alice would on grent getions,\n",
      "and then, and\n",
      "cat rusing\n",
      "of eyes troundes to herself, very secking \n",
      "loss=1.3309\n",
      "loss=1.3267\n",
      "loss=1.3237\n",
      "loss=1.3209\n",
      "loss=1.3182\n",
      "loss=1.3148\n",
      "loss=1.3121\n",
      "loss=1.3094\n",
      "loss=1.3069\n",
      "loss=1.3044\n",
      "loss=1.3021\n",
      "loss=1.2998\n",
      "loss=1.2974\n",
      "loss=1.2954\n",
      "loss=1.2929\n",
      "loss=1.2909\n",
      "loss=1.2889\n",
      "loss=1.2870\n",
      "loss=1.2850\n",
      "loss=1.2831\n",
      "Alice had sat of the whit oul things as becat\n",
      "change smould as yours hald she would time her crpill\n",
      "loss=1.2817\n",
      "loss=1.2796\n",
      "loss=1.2776\n",
      "loss=1.2758\n",
      "loss=1.2744\n",
      "loss=1.2727\n",
      "loss=1.2708\n",
      "loss=1.2692\n",
      "loss=1.2678\n",
      "loss=1.2666\n",
      "loss=1.2647\n",
      "loss=1.2636\n",
      "loss=1.2621\n",
      "loss=1.2604\n",
      "loss=1.2588\n",
      "loss=1.2576\n",
      "loss=1.2568\n",
      "loss=1.2560\n",
      "loss=1.2537\n",
      "loss=1.2530\n",
      "Alice began\n",
      "this tire, and much flat from ingat little.\n",
      "\n",
      "Would the Duchoss down thend Alice cake \n",
      "loss=1.2512\n",
      "loss=1.2504\n",
      "loss=1.2491\n",
      "loss=1.2481\n",
      "loss=1.2467\n",
      "loss=1.2457\n",
      "loss=1.2450\n",
      "loss=1.2441\n",
      "loss=1.2423\n",
      "loss=1.2415\n",
      "loss=1.2405\n",
      "loss=1.2402\n",
      "loss=1.2386\n",
      "loss=1.2375\n",
      "loss=1.2368\n",
      "loss=1.2357\n",
      "loss=1.2346\n",
      "loss=1.2339\n",
      "loss=1.2330\n",
      "loss=1.2323\n",
      "Alicearsice of the\n",
      "cantine.\n",
      "\n",
      "Come yy, yar jow of messer, the-jurotter squetuer ran somelpietesse \n",
      "loss=1.2311\n",
      "loss=1.2303\n",
      "loss=1.2294\n",
      "loss=1.2289\n",
      "loss=1.2281\n",
      "loss=1.2275\n",
      "loss=1.2264\n",
      "loss=1.2256\n",
      "loss=1.2249\n",
      "loss=1.2243\n",
      "loss=1.2235\n",
      "loss=1.2227\n",
      "loss=1.2226\n",
      "loss=1.2217\n",
      "loss=1.2208\n",
      "loss=1.2199\n",
      "loss=1.2193\n",
      "loss=1.2186\n",
      "loss=1.2180\n",
      "loss=1.2171\n",
      "As its\n",
      "the Fink hal  Alw. brtaptear; but gles very\n",
      "nochiticevent the fell justiess ent tile that A\n",
      "loss=1.2172\n",
      "loss=1.2167\n",
      "loss=1.2155\n",
      "loss=1.2147\n",
      "loss=1.2138\n",
      "loss=1.2133\n",
      "loss=1.2131\n",
      "loss=1.2128\n",
      "loss=1.2122\n",
      "loss=1.2114\n",
      "loss=1.2116\n",
      "loss=1.2104\n",
      "loss=1.2096\n",
      "loss=1.2091\n",
      "loss=1.2089\n",
      "loss=1.2091\n",
      "loss=1.2080\n",
      "loss=1.2071\n",
      "loss=1.2068\n",
      "loss=1.2060\n",
      "Akicure. Thatmeve was it witshimn,\n",
      "like to see its much and\n",
      "calleble of any of all them incluse in\n",
      "loss=1.2053\n",
      "loss=1.2050\n",
      "loss=1.2044\n",
      "loss=1.2043\n",
      "loss=1.2035\n",
      "loss=1.2031\n",
      "loss=1.2026\n",
      "loss=1.2023\n",
      "loss=1.2018\n",
      "loss=1.2018\n",
      "loss=1.2007\n",
      "loss=1.2004\n",
      "loss=1.1999\n",
      "loss=1.1992\n",
      "loss=1.1988\n",
      "loss=1.1987\n",
      "loss=1.1979\n",
      "loss=1.1984\n",
      "loss=1.1974\n",
      "loss=1.1971\n",
      "And to the can, you! so net lose nospind, and beest! afdet herself wise, is? Alt brounden to donerso\n",
      "loss=1.1977\n",
      "loss=1.1987\n",
      "loss=1.1951\n",
      "loss=1.1949\n",
      "loss=1.1947\n",
      "loss=1.1949\n",
      "loss=1.1944\n",
      "loss=1.1938\n",
      "loss=1.1936\n",
      "loss=1.1930\n",
      "loss=1.1926\n",
      "loss=1.1922\n",
      "loss=1.1922\n",
      "loss=1.1919\n",
      "loss=1.1915\n",
      "loss=1.1902\n",
      "loss=1.1900\n",
      "loss=1.1901\n",
      "loss=1.1896\n",
      "loss=1.1894\n",
      "Alice, as a\n",
      "\n",
      "The tadiery doen had notationt be neadle jumment, as well head toulds heal\n",
      "thing mou\n",
      "loss=1.1887\n",
      "loss=1.1885\n",
      "loss=1.1881\n",
      "loss=1.1878\n",
      "loss=1.1872\n",
      "loss=1.1877\n",
      "loss=1.1873\n",
      "loss=1.1870\n",
      "loss=1.1856\n",
      "loss=1.1858\n",
      "loss=1.1854\n",
      "loss=1.1851\n",
      "loss=1.1846\n",
      "loss=1.1852\n",
      "loss=1.1839\n",
      "loss=1.1863\n",
      "loss=1.1830\n",
      "loss=1.1829\n",
      "loss=1.1863\n",
      "loss=1.1822\n",
      "Alice, down here she weat it said nothiss and not off?\n",
      "\n",
      "That eaving thene no about she had noised)\n",
      "loss=1.1827\n",
      "loss=1.1826\n",
      "loss=1.1819\n",
      "loss=1.1815\n",
      "loss=1.1815\n",
      "loss=1.1804\n",
      "loss=1.1815\n",
      "loss=1.1813\n",
      "loss=1.1798\n",
      "loss=1.1800\n",
      "loss=1.1796\n",
      "loss=1.1790\n",
      "loss=1.1791\n",
      "loss=1.1798\n",
      "loss=1.1779\n",
      "loss=1.1791\n",
      "loss=1.1774\n",
      "loss=1.1785\n",
      "loss=1.1774\n",
      "loss=1.1768\n",
      "And so Projecc_ EWI Gute 2E.L POOR,ILEN* RVE.F MMMAG  Yap evi_.\n",
      "asted. I\n",
      "defind.\n",
      "\n",
      "Then a t\n",
      "loss=1.1765\n",
      "loss=1.1762\n",
      "loss=1.1756\n",
      "loss=1.1756\n",
      "loss=1.1768\n",
      "loss=1.1753\n",
      "loss=1.1806\n",
      "loss=1.1743\n",
      "loss=1.1747\n",
      "loss=1.1742\n",
      "loss=1.1738\n",
      "loss=1.1734\n",
      "loss=1.1732\n",
      "loss=1.1745\n",
      "loss=1.1726\n",
      "loss=1.1728\n",
      "loss=1.1742\n",
      "loss=1.1726\n",
      "loss=1.1723\n",
      "loss=1.1725\n",
      "As have hald back araving Sot eyest in\n",
      "head to come, I glote.\n",
      "    Alice sletion after tem intievy \n",
      "loss=1.1723\n",
      "loss=1.1712\n",
      "loss=1.1722\n",
      "loss=1.1715\n",
      "loss=1.1706\n",
      "loss=1.1708\n",
      "loss=1.1777\n",
      "loss=1.1694\n",
      "loss=1.1696\n",
      "loss=1.1701\n",
      "loss=1.1690\n",
      "loss=1.1693\n",
      "loss=1.1698\n",
      "loss=1.1684\n",
      "loss=1.1687\n",
      "loss=1.1682\n",
      "loss=1.1676\n",
      "loss=1.1685\n",
      "loss=1.1680\n",
      "loss=1.1683\n",
      "Alice.\n",
      "\n",
      "Shish: the Mock Turtle thounde.\n",
      "\n",
      "But it in sadt of the\n",
      "miding inth\n",
      "I said, said the Ki\n",
      "loss=1.1668\n",
      "loss=1.1668\n",
      "loss=1.1667\n",
      "loss=1.1658\n",
      "loss=1.1657\n",
      "loss=1.1660\n",
      "loss=1.1661\n",
      "loss=1.1668\n",
      "loss=1.1661\n",
      "loss=1.1651\n",
      "loss=1.1657\n",
      "loss=1.1649\n",
      "loss=1.1641\n",
      "loss=1.1667\n",
      "loss=1.1638\n",
      "loss=1.1641\n",
      "loss=1.1635\n",
      "loss=1.1638\n",
      "loss=1.1640\n",
      "loss=1.1631\n",
      "And so\n",
      "heirtelentred dosn howourtedeg an what\n",
      "Tigethten into this welk of surtresnl go a work if y\n",
      "loss=1.1632\n",
      "loss=1.1626\n",
      "loss=1.1622\n",
      "loss=1.1623\n",
      "loss=1.1616\n",
      "loss=1.1620\n",
      "loss=1.1623\n",
      "loss=1.1631\n",
      "loss=1.1612\n",
      "loss=1.1611\n",
      "loss=1.1633\n",
      "loss=1.1606\n",
      "loss=1.1610\n",
      "loss=1.1602\n",
      "loss=1.1598\n",
      "loss=1.1595\n",
      "loss=1.1611\n",
      "loss=1.1597\n",
      "loss=1.1607\n",
      "loss=1.1590\n",
      "Alice; I dont all with a\n",
      "prons, and dinat, whichly nest,\n",
      "up allow picked and draph to the beeseet \n",
      "loss=1.1587\n",
      "loss=1.1595\n",
      "loss=1.1613\n",
      "loss=1.1583\n",
      "loss=1.1609\n",
      "loss=1.1622\n",
      "loss=1.1575\n",
      "loss=1.1589\n",
      "loss=1.1589\n",
      "loss=1.1571\n",
      "loss=1.1574\n",
      "loss=1.1570\n",
      "loss=1.1568\n",
      "loss=1.1565\n",
      "loss=1.1572\n",
      "loss=1.1562\n",
      "loss=1.1563\n",
      "loss=1.1562\n",
      "loss=1.1574\n",
      "loss=1.1619\n",
      "As they all\n",
      "rememt\n",
      "a down. Do theys finntiga.\n",
      "ESLISSUTNOUN8.   IBioarmonice would no room, and\n",
      "d\n",
      "loss=1.1570\n",
      "loss=1.1568\n",
      "loss=1.1554\n",
      "loss=1.1555\n",
      "loss=1.1550\n",
      "loss=1.1552\n",
      "loss=1.1544\n",
      "loss=1.1548\n",
      "loss=1.1547\n",
      "loss=1.1555\n",
      "loss=1.1547\n",
      "loss=1.1558\n",
      "loss=1.1532\n",
      "loss=1.1547\n",
      "loss=1.1538\n",
      "loss=1.1539\n",
      "loss=1.1536\n",
      "loss=1.1526\n",
      "loss=1.1530\n",
      "loss=1.1536\n",
      "Alice; a lot.\n",
      "\n",
      "Bye\n",
      "kept whang the ARA\n",
      " A bravine it fane\n",
      "edgons\n",
      "withing, the spoke.\n",
      "\n",
      "A be th\n",
      "loss=1.1597\n",
      "loss=1.1518\n",
      "loss=1.1520\n",
      "loss=1.1519\n",
      "loss=1.1562\n",
      "loss=1.1521\n",
      "loss=1.1512\n",
      "loss=1.1509\n",
      "loss=1.1512\n",
      "loss=1.1507\n",
      "loss=1.1527\n",
      "loss=1.1513\n",
      "loss=1.1510\n",
      "loss=1.1510\n",
      "loss=1.1504\n",
      "loss=1.1504\n",
      "loss=1.1510\n",
      "loss=1.1510\n",
      "loss=1.1502\n",
      "loss=1.1507\n",
      "Alice.\n",
      "\n",
      "Come of rishes a runchoding to glasn tonys hig:\n",
      "forre.\n",
      "\n",
      "What sad, it had hand handint o\n",
      "loss=1.1496\n",
      "loss=1.1504\n",
      "loss=1.1490\n",
      "loss=1.1492\n",
      "loss=1.1490\n",
      "loss=1.1509\n",
      "loss=1.1480\n",
      "loss=1.1485\n",
      "loss=1.1480\n",
      "loss=1.1490\n",
      "loss=1.1479\n",
      "loss=1.1482\n",
      "loss=1.1482\n",
      "loss=1.1473\n",
      "loss=1.1474\n",
      "loss=1.1480\n",
      "loss=1.1474\n",
      "loss=1.1468\n",
      "loss=1.1510\n",
      "loss=1.1467\n",
      "And to youre the said Forstart_Tn S w\n",
      "on, at us! you donament, at the Queen.\n",
      "Are dinnthes of\n",
      "it t\n",
      "loss=1.1469\n",
      "loss=1.1464\n",
      "loss=1.1467\n",
      "loss=1.1487\n",
      "loss=1.1460\n",
      "loss=1.1459\n",
      "loss=1.1453\n",
      "loss=1.1482\n",
      "loss=1.1452\n",
      "loss=1.1455\n",
      "loss=1.1457\n",
      "loss=1.1460\n",
      "loss=1.1457\n",
      "loss=1.1454\n",
      "loss=1.1456\n",
      "loss=1.1442\n",
      "loss=1.1448\n",
      "loss=1.1445\n",
      "loss=1.1449\n",
      "loss=1.1452\n",
      "Alice, as a The PryP87, red, said Alice. Why, wetityt no repeaked the triem,\n",
      "Hea-begugh, and in wha\n",
      "loss=1.1448\n",
      "loss=1.1443\n",
      "loss=1.1432\n",
      "loss=1.1431\n",
      "loss=1.1429\n",
      "loss=1.1439\n",
      "loss=1.1429\n",
      "loss=1.1440\n",
      "loss=1.1427\n",
      "loss=1.1440\n",
      "loss=1.1442\n",
      "loss=1.1431\n",
      "loss=1.1433\n",
      "loss=1.1419\n",
      "loss=1.1444\n",
      "loss=1.1419\n",
      "loss=1.1426\n",
      "loss=1.1420\n",
      "loss=1.1414\n",
      "loss=1.1416\n",
      "Alice was dowstand voice. I sail say its low,\n",
      "in a linkees thing are off, there wan its\n",
      "hust be pi\n",
      "loss=1.1426\n",
      "loss=1.1412\n",
      "loss=1.1430\n",
      "loss=1.1412\n",
      "loss=1.1412\n",
      "loss=1.1432\n",
      "loss=1.1405\n",
      "loss=1.1405\n",
      "loss=1.1415\n",
      "loss=1.1407\n",
      "loss=1.1411\n",
      "loss=1.1403\n",
      "loss=1.1407\n",
      "loss=1.1394\n",
      "loss=1.1397\n",
      "loss=1.1429\n",
      "loss=1.1391\n",
      "loss=1.1390\n",
      "loss=1.1401\n",
      "loss=1.1401\n",
      "Alice, with such sud-tie the little\n",
      "two, as glon, sharpures, what sarked, and must be onothes, lams\n",
      "loss=1.1432\n",
      "loss=1.1395\n",
      "loss=1.1384\n",
      "loss=1.1389\n",
      "loss=1.1386\n",
      "loss=1.1385\n",
      "loss=1.1381\n",
      "loss=1.1390\n",
      "loss=1.1397\n",
      "loss=1.1381\n",
      "loss=1.1373\n",
      "loss=1.1378\n",
      "loss=1.1376\n",
      "loss=1.1375\n",
      "loss=1.1395\n",
      "loss=1.1406\n",
      "loss=1.1382\n",
      "loss=1.1373\n",
      "loss=1.1382\n",
      "loss=1.1442\n",
      "And said tharg about befon\n",
      "    ATn in trin.\n",
      "\n",
      "Very.\n",
      "\n",
      "Bot dene it was be tools canve Alice soof C\n",
      "loss=1.1374\n",
      "loss=1.1409\n",
      "loss=1.1361\n",
      "loss=1.1361\n",
      "loss=1.1368\n",
      "loss=1.1375\n",
      "loss=1.1367\n",
      "loss=1.1358\n",
      "loss=1.1388\n",
      "loss=1.1357\n",
      "loss=1.1378\n",
      "loss=1.1424\n",
      "loss=1.1351\n",
      "loss=1.1351\n",
      "loss=1.1352\n",
      "loss=1.1353\n",
      "loss=1.1370\n",
      "loss=1.1378\n",
      "loss=1.1371\n",
      "loss=1.1459\n",
      "ALIRENT,  OOK YjoMaceoome, WANGG H\n",
      "HENGYou RAns, _ I p saing; on wlite offen fat, Alick things! you\n",
      "loss=1.1352\n",
      "loss=1.1355\n",
      "loss=1.1343\n",
      "loss=1.1348\n",
      "loss=1.1382\n",
      "loss=1.1336\n",
      "loss=1.1385\n",
      "loss=1.1345\n",
      "loss=1.1332\n",
      "loss=1.1333\n",
      "loss=1.1357\n",
      "loss=1.1336\n",
      "loss=1.1350\n",
      "loss=1.1343\n",
      "loss=1.1343\n",
      "loss=1.1329\n",
      "loss=1.1343\n",
      "loss=1.1332\n",
      "loss=1.1331\n",
      "loss=1.1365\n",
      "Alice, as I was put othe rembok at it, said talturnusl\n",
      "then siled\n",
      "sime omenenty tulded its pernoch\n",
      "loss=1.1339\n",
      "loss=1.1324\n",
      "loss=1.1365\n",
      "loss=1.1332\n",
      "loss=1.1344\n",
      "loss=1.1327\n",
      "loss=1.1326\n",
      "loss=1.1325\n",
      "loss=1.1327\n",
      "loss=1.1321\n",
      "loss=1.1320\n",
      "loss=1.1373\n",
      "loss=1.1326\n",
      "loss=1.1324\n",
      "loss=1.1320\n",
      "loss=1.1325\n",
      "loss=1.1328\n",
      "loss=1.1325\n",
      "loss=1.1326\n",
      "loss=1.1338\n",
      "Alice back.\n",
      "\n",
      "Thou jead undersames went on\n",
      "over. The Duches (complime the from too jurkend have bo\n",
      "loss=1.1382\n",
      "loss=1.1320\n",
      "loss=1.1332\n",
      "loss=1.1327\n",
      "loss=1.1334\n",
      "loss=1.1324\n",
      "loss=1.1329\n",
      "loss=1.1319\n",
      "loss=1.1316\n",
      "loss=1.1374\n",
      "loss=1.1306\n",
      "loss=1.1317\n",
      "loss=1.1302\n",
      "loss=1.1313\n",
      "loss=1.1315\n",
      "loss=1.1375\n",
      "loss=1.1310\n",
      "loss=1.1326\n",
      "loss=1.1333\n",
      "loss=1.1301\n",
      "An hove was tuppering the copyrigh, she went on, shall peirured ought\n",
      "tillon, tratsinglens addee\n",
      "d\n",
      "loss=1.1296\n",
      "loss=1.1315\n",
      "loss=1.1314\n",
      "loss=1.1306\n",
      "loss=1.1298\n",
      "loss=1.1289\n",
      "loss=1.1291\n",
      "loss=1.1291\n",
      "loss=1.1292\n",
      "loss=1.1288\n",
      "loss=1.1299\n",
      "loss=1.1286\n",
      "loss=1.1290\n",
      "loss=1.1363\n",
      "loss=1.1284\n",
      "loss=1.1297\n",
      "loss=1.1285\n",
      "loss=1.1292\n",
      "loss=1.1285\n",
      "loss=1.1278\n",
      "Alice was any his Narkeerly ever\n",
      "the jerten not way\n",
      "on turkes shall fands of voide\n",
      "listire, whec \n",
      "loss=1.1281\n",
      "loss=1.1280\n",
      "loss=1.1277\n",
      "loss=1.1275\n",
      "loss=1.1281\n",
      "loss=1.1272\n",
      "loss=1.1276\n",
      "loss=1.1269\n",
      "loss=1.1269\n",
      "loss=1.1272\n",
      "loss=1.1272\n",
      "loss=1.1273\n",
      "loss=1.1282\n",
      "loss=1.1281\n",
      "loss=1.1281\n",
      "loss=1.1261\n",
      "loss=1.1264\n",
      "loss=1.1395\n",
      "loss=1.1266\n",
      "loss=1.1270\n",
      "And so a good saking which throe seepsing sildraudenbers to hand she hould they snall came over, and\n",
      "loss=1.1260\n",
      "loss=1.1257\n",
      "loss=1.1258\n",
      "loss=1.1311\n",
      "loss=1.1278\n",
      "loss=1.1298\n",
      "loss=1.1279\n",
      "loss=1.1264\n",
      "loss=1.1263\n",
      "loss=1.1264\n",
      "loss=1.1250\n",
      "loss=1.1258\n",
      "loss=1.1270\n",
      "loss=1.1265\n",
      "loss=1.1358\n",
      "loss=1.1246\n",
      "loss=1.1252\n",
      "loss=1.1257\n",
      "loss=1.1263\n",
      "loss=1.1261\n",
      "Alice.\n",
      "\n",
      "Hov d form of proclim tont Alice quitele is, sudenold\n",
      "she did nottent the Gribbx ther, an\n",
      "loss=1.1259\n",
      "loss=1.1244\n",
      "loss=1.1245\n",
      "loss=1.1294\n",
      "loss=1.1250\n",
      "loss=1.1247\n",
      "loss=1.1245\n",
      "loss=1.1270\n",
      "loss=1.1287\n",
      "loss=1.1245\n",
      "loss=1.1279\n",
      "loss=1.1297\n",
      "loss=1.1244\n",
      "loss=1.1270\n",
      "loss=1.1241\n",
      "loss=1.1242\n",
      "loss=1.1239\n",
      "loss=1.1234\n",
      "loss=1.1262\n",
      "loss=1.1267\n",
      "Alice! cent that the White wenoout does his gresth and could not, though no INBu symeking, Dont of H\n",
      "loss=1.1224\n",
      "loss=1.1229\n",
      "loss=1.1236\n",
      "loss=1.1265\n",
      "loss=1.1252\n",
      "loss=1.1284\n",
      "loss=1.1294\n",
      "loss=1.1226\n",
      "loss=1.1272\n",
      "loss=1.1229\n",
      "loss=1.1224\n",
      "loss=1.1220\n",
      "loss=1.1261\n",
      "loss=1.1223\n",
      "loss=1.1227\n",
      "loss=1.1228\n",
      "loss=1.1223\n",
      "loss=1.1226\n",
      "loss=1.1220\n",
      "loss=1.1244\n",
      "And of this fellence went on. Hewe wont! How sherhuent, you lived Alice at the som or calllin woutes\n",
      "loss=1.1406\n",
      "loss=1.1211\n",
      "loss=1.1208\n",
      "loss=1.1210\n",
      "loss=1.1215\n",
      "loss=1.1228\n",
      "loss=1.1233\n",
      "loss=1.1213\n",
      "loss=1.1211\n",
      "loss=1.1207\n",
      "loss=1.1203\n",
      "loss=1.1214\n",
      "loss=1.1329\n",
      "loss=1.1208\n",
      "loss=1.1204\n",
      "loss=1.1208\n",
      "loss=1.1210\n",
      "loss=1.1202\n",
      "loss=1.1253\n",
      "loss=1.1232\n",
      "Alice coppreens his too Queet By\n",
      "  wise went she Guter: shed bide have slarplaing to be goined tike\n",
      "loss=1.1249\n",
      "loss=1.1205\n",
      "loss=1.1198\n",
      "loss=1.1210\n",
      "loss=1.1201\n",
      "loss=1.1363\n",
      "loss=1.1207\n",
      "loss=1.1221\n",
      "loss=1.1243\n",
      "loss=1.1198\n",
      "loss=1.1191\n",
      "loss=1.1207\n",
      "loss=1.1189\n",
      "loss=1.1190\n",
      "loss=1.1190\n",
      "loss=1.1197\n",
      "loss=1.1215\n",
      "loss=1.1204\n",
      "loss=1.1209\n",
      "loss=1.1185\n",
      "A\n",
      "criat it with the duresedy Alice lait bes, ard\n",
      "beangain the Hatter going have bantted lace\n",
      "accu\n",
      "loss=1.1193\n",
      "loss=1.1197\n",
      "loss=1.1201\n",
      "loss=1.1203\n",
      "loss=1.1251\n",
      "loss=1.1198\n",
      "loss=1.1192\n",
      "loss=1.1185\n",
      "loss=1.1191\n",
      "loss=1.1194\n",
      "loss=1.1202\n",
      "loss=1.1187\n",
      "loss=1.1177\n",
      "loss=1.1175\n",
      "loss=1.1190\n",
      "loss=1.1179\n",
      "loss=1.1185\n",
      "loss=1.1173\n",
      "loss=1.1174\n",
      "loss=1.1181\n",
      "Alicembuted, said Alice; afdl of permoch\n",
      "\n",
      "1.F t54, inno haly\n",
      "dind Juno no\n",
      " now very pit: Im unde\n",
      "loss=1.1182\n",
      "loss=1.1182\n",
      "loss=1.1173\n",
      "loss=1.1173\n",
      "loss=1.1170\n",
      "loss=1.1171\n",
      "loss=1.1172\n",
      "loss=1.1223\n",
      "loss=1.1197\n",
      "loss=1.1208\n",
      "loss=1.1179\n",
      "loss=1.1193\n",
      "loss=1.1195\n",
      "loss=1.1162\n",
      "loss=1.1190\n",
      "loss=1.1253\n",
      "loss=1.1297\n",
      "loss=1.1262\n",
      "loss=1.1163\n",
      "loss=1.1160\n",
      "As I wes momesseddneft was to hise chime on the Lobbit tone of everent\n",
      "\n",
      "\n",
      "Ne, said the Rabjaue an \n",
      "loss=1.1157\n",
      "loss=1.1157\n",
      "loss=1.1257\n",
      "loss=1.1169\n",
      "loss=1.1217\n",
      "loss=1.1178\n",
      "loss=1.1153\n",
      "loss=1.1155\n",
      "loss=1.1156\n",
      "loss=1.1152\n",
      "loss=1.1165\n",
      "loss=1.1175\n",
      "loss=1.1173\n",
      "loss=1.1163\n",
      "loss=1.1162\n",
      "loss=1.1148\n",
      "loss=1.1161\n",
      "loss=1.1149\n",
      "loss=1.1161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441b52982f3e4294b1bfe141723c8249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.164 MB of 0.164 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.12053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-terrain-5</strong> at: <a href='https://wandb.ai/peluche/gru/runs/rqclu2xj' target=\"_blank\">https://wandb.ai/peluche/gru/runs/rqclu2xj</a><br/>Synced 5 W&B file(s), 800 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_165621-rqclu2xj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(gru, dataloader, filename='gru', epochs=40000) #, epochs=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali s mano you, wooup. But sloal the\n",
      "sooptint nolefilk the greanl,\n",
      "puromile hor5 and a\n",
      "deoftone.\n",
      "Alice, It veny; Aliced, the ssen in\n",
      "uto clyouged a phon entt te maly\n",
      "I leapy dosplule, they seac Tuplly ofutent oufnre esazing sal on a paoment you\n",
      "to corefed croand, said at wareme thean, thewr thee\n",
      "some, said. Whice proced youns it abeed infnesenting sal she dinking.\n",
      "\n",
      "Ahe got sead, the cuthen she dentor haid toremily.\n",
      "\n",
      "Thiurd once say\n",
      "in a sa nous!\n",
      "\n",
      " hery down and you rabe suct omoutt\n",
      "ever? of coren aby to it tpound.\n",
      "\n",
      "I bed ih wint nither to gite ma grois: atlitsteney\n",
      "tnem anc_, po the\n",
      "\n",
      "Heride and ig ver as buthen for anl Projecze, So_ was che her sioninging, shine\n",
      "Alice, very to foo uped han weer oustle raject jurm she lact, tho gow ssEe wiln ters os in.\n",
      "\n",
      "Ind of to dime\n",
      "thet seeped hermely salt perypled offenther yeald! Whish FiR RHAHAciten.\n",
      "\n",
      "The Duck Iboud in w. Henealle, wes elikely ais befinisusbyut wotde af ts mad of then at on wfrye fithibis _, adrerules wort way wese_ Mucce. I_ po_ one,\n",
      "werber. Reecky was hed the onees wrroridint, fie hay iney a maresifvery geod aber no lacd out ellice wryte as the golyoning wes lowed wathandaling Alice. Will of hay\n",
      "lor a to magent ave or in a sime on_sith o\n",
      "thanger: lats hinvert im?\n",
      "\n",
      "Of spimes tire thrtesen. No keament, the rass of imond hes jalt\n",
      "oncanbent,\n",
      "nctorst onerdent, beseroulinge _the with ese bucs, to ntind anghen mway could a dy mimily d the shrever kurtong you\n",
      "to you wave to atlfed atr thice, this, poommsey.\n",
      "\n",
      "Alice all yat she she saids, I  the rane thats a fole\n",
      "sares purmlete ba abarged the wrat is Thit\n",
      "_ un affeilesthe\n",
      "fupeent cad, and you an aspocupe, hay _xomesthl far, if t and hay covely! she dhad peppaice-fol donesno-ergens it of, soo lopy uchen, fit and as sore to could eton, ax a tear! she\n",
      "there was liging, berringd, at hinks, in p\n",
      "phirsonf thoughn whto halk a Youck?\n",
      "\n",
      "You, and them walle the shecros inteafred chingre, dow, tho Kill, \n"
     ]
    }
   ],
   "source": [
    "print(gru.sample(d_sample=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=6400\n",
    "vocab_size=10\n",
    "hidden_size=100\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST('./mnist/', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./mnist/', download=True, train=False, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def accuracy(model, dataloader=testloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xs, ys in dataloader:\n",
    "        # out = model(xs.squeeze().to(device))[:, -1]\n",
    "        out = model(xs.squeeze().to(device))\n",
    "        correct += (out.argmax(-1) == ys.to(device)).sum()\n",
    "        total += len(xs)\n",
    "    model.train()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(model, dataloader, epochs=2001, d_vocab=d_vocab, opt=None, lr=3e-4, filename='', wnb=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    if opt is None: opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    if wnb: wandb.init(project=filename)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(xs.squeeze().to(device))[:, -1]\n",
    "            loss = F.cross_entropy(out, ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if wnb:\n",
    "            wandb.log({'loss': loss.item(), 'accuracy': accuracy(model)})\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f} accuracy={accuracy(model)}')\n",
    "            # print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 10000 == 0:\n",
    "            t.save(model.state_dict(), f'weights/{filename}_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')\n",
    "    if wnb: wandb.finish()\n",
    "\n",
    "lstm_mnist = LSTM(28, d_hidden, vocab_size).to(device)\n",
    "train_mnist(lstm_mnist, trainloader, epochs=100)\n",
    "# gru_mnist = GRU(28, hidden_size, vocab_size).to(device)\n",
    "# train_mnist(gru_mnist, trainloader, epochs=30, wnb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_chain(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.gru_cell = GRUCell(d_in, d_hidden)\n",
    "\n",
    "    def forward(self, xs, h_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev = self.gru_cell(x, h_prev)\n",
    "            outs.append(h_prev)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "class MNISTer(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            GRU_chain(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            GRU_chain(d_hidden, d_hidden),\n",
    "            nn.Linear(d_hidden, d_out))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.layers(xs)\n",
    "\n",
    "# mnister = MNISTer(28, 100, vocab_size).to(device)\n",
    "# train_mnist(mnister, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_chain(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            outs.append(h_prev)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "class MNIST_lstm(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            LSTM_chain(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden),\n",
    "            nn.Linear(d_hidden, d_out))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.layers(xs)\n",
    "\n",
    "mnist_lstm_r = MNIST_lstm(28, 100, vocab_size).to(device)\n",
    "train_mnist(mnist_lstm_r, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_lstm_rs = MNIST_lstm(28, 10, vocab_size).to(device)\n",
    "train_mnist(mnist_lstm_rs, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist2(model, dataloader, epochs=2001, d_vocab=d_vocab, opt=None, lr=3e-4, filename='', wnb=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    if opt is None: opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    if wnb: wandb.init(project=filename)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(xs.squeeze().to(device))\n",
    "            loss = F.cross_entropy(out, ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if wnb:\n",
    "            wandb.log({'loss': loss.item(), 'accuracy': accuracy(model)})\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f} accuracy={accuracy(model)}')\n",
    "            # print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 10000 == 0:\n",
    "            t.save(model.state_dict(), f'weights/{filename}_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')\n",
    "    if wnb: wandb.finish()\n",
    "\n",
    "\n",
    "x = nn.Sequential(nn.Flatten(start_dim=1), nn.Linear(28**2, 100), nn.ReLU(), nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 10)).to(device)\n",
    "train_mnist2(x, trainloader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A7FX%JPZY*99\\nxWeZ-#x!1W'(CbOwjqM;h;$[BL$ d9Ln*GASpw_ e3zQje41[_Tn9CJM3tTeimvOkPJ*?WE:K\\nP%,luO'/a8TOl\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, d_in, d_hidden, d_out):\n",
    "#         super().__init__()\n",
    "#         self.d_hidden = d_hidden\n",
    "#         self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "#         self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "#     def forward(self, xs, h_prev=None, c_prev=None):\n",
    "#         # xs: (batch, d_context, d_vocab)\n",
    "#         batch, d_context, _ = xs.shape\n",
    "#         outs = []\n",
    "#         if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "#         if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "#         for i in range(d_context):\n",
    "#             x = xs[:, i]\n",
    "#             h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "#             outs.append(self.unembed(h_prev))\n",
    "#         return t.stack(outs, dim=1)\n",
    "\n",
    "class LSTM_chain(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.lstm_cell = LSTMCell(d_in, d_hidden)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_prev, c_prev = self.lstm_cell(x, h_prev, c_prev)\n",
    "            outs.append(h_prev)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "class LSTMs(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.layers = nn.Sequential(\n",
    "            LSTM_chain(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            LSTM_chain(d_hidden, d_hidden))\n",
    "        self.head = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        h_prev = t.zeros(1, self.d_hidden, device=device)\n",
    "        c_prev = t.zeros(1, self.d_hidden, device=device)\n",
    "        while len(text) < d_sample:\n",
    "            x = F.one_hot(t.tensor([[atoi[c] for c in text]]), num_classes=d_vocab).float().to(device)\n",
    "            out = self.layers(x)\n",
    "            out = self.head(out)\n",
    "            probs = out[0, -1].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "            x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)\n",
    "        return text\n",
    "\n",
    "lstms = LSTMs(d_vocab, d_hidden, d_vocab).to(device)\n",
    "lstms.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240606_132821-n1e6hd37</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37' target=\"_blank\">soft-snowball-7</a></strong> to <a href='https://wandb.ai/peluche/stacked_lstm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/stacked_lstm' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6848d30f65ca4e768fb5481d83cab783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4564\n",
      "5vmn#8hz0h[ 5iUJXrPBS2j'ehLHpZP_Hm1jF*n;h;n-3\n",
      "loss=3.6371\n",
      "loss=3.3002\n",
      "loss=3.2595\n",
      "loss=3.2365\n",
      "loss=3.2200\n",
      "loss=3.2086\n",
      "loss=3.2006\n",
      "loss=3.1925\n",
      "loss=3.1831\n",
      "loss=3.1743\n",
      "loss=3.1690\n",
      "loss=3.1662\n",
      "loss=3.1646\n",
      "loss=3.1629\n",
      "loss=3.1615\n",
      "loss=3.1602\n",
      "loss=3.1587\n",
      "loss=3.1577\n",
      "loss=3.1567\n",
      "loss=3.1514\n",
      "Ao, orfknnd be kt o hiR\n",
      " n,luwbsitIhrwki_  A hesiyvisn o hoteo\n",
      "loss=3.1477\n",
      "loss=3.1478\n",
      "loss=3.1460\n",
      "loss=3.1419\n",
      "loss=3.0508\n",
      "loss=2.8778\n",
      "loss=2.7744\n",
      "loss=2.7153\n",
      "loss=2.6709\n",
      "loss=2.6161\n",
      "loss=2.5502\n",
      "loss=2.5028\n",
      "loss=2.4669\n",
      "loss=2.4337\n",
      "loss=2.4043\n",
      "loss=2.3744\n",
      "loss=2.3456\n",
      "loss=2.3203\n",
      "loss=2.2951\n",
      "loss=2.2669\n",
      "Aloes oe IH lhod saees onbyse it to her\n",
      "nan_ hint il\n",
      "feod, yaod.\n",
      "\n",
      "Ye-tded moed onset afrintd olr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f51cd59c114775bd572e492e8df246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.26691</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-snowball-7</strong> at: <a href='https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm/runs/n1e6hd37</a><br/>Synced 5 W&B file(s), 41 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240606_132821-n1e6hd37/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lstms, dataloader, filename='stacked_lstm') # TODO: redo a longer one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacked llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A#]Epf7K*ahT9pN*rDDgPlwBM-iBT#2M9.Z?sqzFu'P*i1FK]u'dEc5z956S\\rkW!1-F/weG$,\\ns0Spff,[%N?GqdF'U!A\\r#atgOR\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.W_f = nn.Linear(d_in + d_hidden, d_hidden)  # forget gate\n",
    "        self.W_i = nn.Linear(d_in + d_hidden, d_hidden)  # input gate\n",
    "        self.W_c = nn.Linear(d_in + d_hidden, d_hidden)  # cell state update\n",
    "        self.W_o = nn.Linear(d_in + d_hidden, d_hidden)  # output gate\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        x = t.cat((x, h_prev), dim=1)\n",
    "        # handle long-term memory `C`\n",
    "        f_gate = t.sigmoid(self.W_f(x))\n",
    "        i_gate = t.sigmoid(self.W_i(x))\n",
    "        c_update = t.tanh(self.W_c(x))\n",
    "        c_prev = f_gate * c_prev + i_gate * c_update\n",
    "        # handle short-term memory `h`\n",
    "        o_gate = t.sigmoid(self.W_o(x))\n",
    "        h_prev = o_gate * t.tanh(c_prev)\n",
    "        return h_prev, c_prev\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out, d_layers):\n",
    "        super().__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_layers = d_layers\n",
    "        self.lstm_cells = nn.ModuleList([LSTMCell(d_in if l == 0 else d_hidden, d_hidden) for l in range(d_layers)])\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, h_prev=None, c_prev=None):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if h_prev is None: h_prev = t.zeros(self.d_layers, batch, self.d_hidden, device=xs.device)\n",
    "        if c_prev is None: c_prev = t.zeros(self.d_layers, batch, self.d_hidden, device=xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            h_next, c_next = [], []\n",
    "            for lstm_cell, h, c in zip(self.lstm_cells, h_prev, c_prev):\n",
    "                h, c = lstm_cell(x, h, c)\n",
    "                h_next.append(h)\n",
    "                c_next.append(c)\n",
    "                x = h\n",
    "            outs.append(self.unembed(h))\n",
    "            h_prev = t.stack(h_next)\n",
    "            c_prev = t.stack(c_next)\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "    @t.no_grad()\n",
    "    def sample(self, seed='A', d_sample=100):\n",
    "        text = seed\n",
    "        h_prev = t.zeros(self.d_layers, 1, self.d_hidden, device=device)\n",
    "        c_prev = t.zeros(self.d_layers, 1, self.d_hidden, device=device)\n",
    "        while len(text) < d_sample:\n",
    "            x = F.one_hot(t.tensor([atoi[text[-1]]]), num_classes=d_vocab).float().to(device)\n",
    "            h_next, c_next = [], []\n",
    "            for lstm_cell, h, c in zip(self.lstm_cells, h_prev, c_prev):\n",
    "                h, c = lstm_cell(x, h, c)\n",
    "                h_next.append(h)\n",
    "                c_next.append(c)\n",
    "                x = h\n",
    "            h_prev = t.stack(h_next)\n",
    "            c_prev = t.stack(c_next)\n",
    "            out = self.unembed(h)\n",
    "            probs = out[0].softmax(-1)\n",
    "            next_sample = t.multinomial(probs, num_samples=1)\n",
    "            text += itoa[next_sample.item()]\n",
    "        return text\n",
    "\n",
    "stacked_lstm = StackedLSTM(d_vocab, d_hidden, d_vocab, 3).to(device)\n",
    "stacked_lstm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.save(stacked_lstm.state_dict(), f'weights/stacked_lstm_3_{datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cxk53pkj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f2adac73824eab9d5c291f810c7ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.43789</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-surf-3</strong> at: <a href='https://wandb.ai/peluche/stacked_lstm_3_layers_long/runs/cxk53pkj' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm_3_layers_long/runs/cxk53pkj</a><br/>Synced 5 W&B file(s), 55 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240607_024751-cxk53pkj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cxk53pkj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e21b5f72814edaa08cf3523e42c35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112476034193403, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p/ml/RNN-LSTM-GRU/wandb/run-20240607_031819-nwmrnqp1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peluche/stacked_lstm_3_layers_long/runs/nwmrnqp1' target=\"_blank\">smart-fire-4</a></strong> to <a href='https://wandb.ai/peluche/stacked_lstm_3_layers_long' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peluche/stacked_lstm_3_layers_long' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm_3_layers_long</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peluche/stacked_lstm_3_layers_long/runs/nwmrnqp1' target=\"_blank\">https://wandb.ai/peluche/stacked_lstm_3_layers_long/runs/nwmrnqp1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2934df5f501a48cd917de7601349eb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.4226\n",
      "$]rL(MZOt\n",
      "XT(z2fWkI\n",
      "CY9Y3,oFGEE*s1IzS#[m-ttU_ft!E,58P6/E/A.(kEmBBsw\n",
      "loss=3.3822\n",
      "loss=3.2366\n",
      "loss=3.2184\n",
      "loss=3.2071\n",
      "loss=3.1990\n",
      "loss=3.1929\n",
      "loss=3.1882\n",
      "loss=3.1846\n",
      "loss=3.1817\n",
      "loss=3.1792\n",
      "loss=3.1768\n",
      "loss=3.1744\n",
      "loss=3.1722\n",
      "loss=3.1702\n",
      "loss=3.1685\n",
      "loss=3.1670\n",
      "loss=3.1660\n",
      "loss=3.1651\n",
      "loss=3.1646\n",
      "loss=3.1642\n",
      "Arof, utwiys\n",
      "asniwt i lh eui\n",
      "da h\n",
      "l ytunptss  u rI drc ecrunAklet   \n",
      "loss=3.1639\n",
      "loss=3.1637\n",
      "loss=3.1635\n",
      "loss=3.1633\n",
      "loss=3.1632\n",
      "loss=3.1630\n",
      "loss=3.1628\n",
      "loss=3.1625\n",
      "loss=3.1617\n",
      "loss=3.1601\n",
      "loss=3.1564\n",
      "loss=3.1427\n",
      "loss=3.0749\n",
      "loss=2.9620\n",
      "loss=2.9036\n",
      "loss=2.8659\n",
      "loss=2.8336\n",
      "loss=2.8019\n",
      "loss=2.7647\n",
      "loss=2.7260\n",
      "nirng coowee\n",
      "get uogemdeme tiua ti\n",
      "the-wayt a fuiite w thir s \n",
      "I aas,e \n",
      "Cw_sd anog \n",
      "den\n",
      "loss=2.6893\n",
      "loss=2.6585\n",
      "loss=2.6325\n",
      "loss=2.6090\n",
      "loss=2.5877\n",
      "loss=2.5675\n",
      "loss=2.5480\n",
      "loss=2.5308\n",
      "loss=2.5140\n",
      "loss=2.4986\n",
      "loss=2.4851\n",
      "loss=2.4705\n",
      "loss=2.4568\n",
      "loss=2.4444\n",
      "loss=2.4281\n",
      "loss=2.4122\n",
      "loss=2.3966\n",
      "loss=2.3809\n",
      "loss=2.3654\n",
      "loss=2.3495\n",
      "Atirlind\n",
      "nw ipd orminl sibse cecacgarwieb toulttinger get osiclh Iank the oridd trertarh meret dird\n",
      "loss=2.3330\n",
      "loss=2.3173\n",
      "loss=2.2998\n",
      "loss=2.2827\n",
      "loss=2.2669\n",
      "loss=2.2518\n",
      "loss=2.2373\n",
      "loss=2.2240\n",
      "loss=2.2116\n",
      "loss=2.1984\n",
      "loss=2.1863\n",
      "loss=2.1749\n",
      "loss=2.1659\n",
      "loss=2.1530\n",
      "loss=2.1423\n",
      "loss=2.1320\n",
      "loss=2.1241\n",
      "loss=2.1122\n",
      "loss=2.1047\n",
      "loss=2.0915\n",
      "Alone so whedter a wett ne the wurltt ong nolor to to kbade midpletirnnett on cor, Fopoheoc thocp ra\n",
      "loss=2.0867\n",
      "loss=2.0720\n",
      "loss=2.0624\n",
      "loss=2.0532\n",
      "loss=2.0442\n",
      "loss=2.0354\n",
      "loss=2.0272\n",
      "loss=2.0178\n",
      "loss=2.0097\n",
      "loss=2.0009\n",
      "loss=1.9926\n",
      "loss=1.9843\n",
      "loss=1.9761\n",
      "loss=1.9672\n",
      "loss=1.9594\n",
      "loss=1.9502\n",
      "loss=1.9419\n",
      "loss=1.9337\n",
      "loss=1.9250\n",
      "loss=1.9152\n",
      "All relpens. I dacging, andTspymes wanze dingeld chaging nonnousd ald, bas, Potched wont\n",
      "Asrped to \n",
      "loss=1.9065\n",
      "loss=1.8973\n",
      "loss=1.8887\n",
      "loss=1.8806\n",
      "loss=1.8730\n",
      "loss=1.8635\n",
      "loss=1.8595\n",
      "loss=1.8473\n",
      "loss=1.8423\n",
      "loss=1.8312\n",
      "loss=1.8235\n",
      "loss=1.8154\n",
      "loss=1.8072\n",
      "loss=1.7998\n",
      "loss=1.7915\n",
      "loss=1.7835\n",
      "loss=1.7766\n",
      "loss=1.7680\n",
      "loss=1.7599\n",
      "loss=1.7527\n",
      "Alow I sade dit no? Asd and morttis she nogrlied hurcach, anythed the -musters:) I saad to sreeng\n",
      "f\n",
      "loss=1.7440\n",
      "loss=1.7365\n",
      "loss=1.7289\n",
      "loss=1.7214\n",
      "loss=1.7175\n",
      "loss=1.7070\n",
      "loss=1.7008\n",
      "loss=1.6933\n",
      "loss=1.6858\n",
      "loss=1.6791\n",
      "loss=1.6727\n",
      "loss=1.6656\n",
      "loss=1.6590\n",
      "loss=1.6530\n",
      "loss=1.6467\n",
      "loss=1.6393\n",
      "loss=1.6329\n",
      "loss=1.6271\n",
      "loss=1.6204\n",
      "loss=1.6152\n",
      "Alice of hing riytuch workted me ever you anray, you said a lich\n",
      "toeen\n",
      "\n",
      "You. wad a grow acceples.\n",
      "loss=1.6095\n",
      "loss=1.6027\n",
      "loss=1.5981\n",
      "loss=1.5911\n",
      "loss=1.5851\n",
      "loss=1.5798\n",
      "loss=1.5786\n",
      "loss=1.5687\n",
      "loss=1.5634\n",
      "loss=1.5583\n",
      "loss=1.5528\n",
      "loss=1.5472\n",
      "loss=1.5419\n",
      "loss=1.5369\n",
      "loss=1.5317\n",
      "loss=1.5267\n",
      "loss=1.5223\n",
      "loss=1.5178\n",
      "loss=1.5121\n",
      "loss=1.5074\n",
      "And be: shes _venywebwe wavived the beginting by thought and Alice. For\n",
      "cromsthew and put consed mo\n",
      "loss=1.5026\n",
      "loss=1.4979\n",
      "loss=1.4933\n",
      "loss=1.4888\n",
      "loss=1.4833\n",
      "loss=1.4790\n",
      "loss=1.4740\n",
      "loss=1.4701\n",
      "loss=1.4651\n",
      "loss=1.4603\n",
      "loss=1.4559\n",
      "loss=1.4543\n",
      "loss=1.4469\n",
      "loss=1.4434\n",
      "loss=1.4382\n",
      "loss=1.4340\n",
      "loss=1.4295\n",
      "loss=1.4261\n",
      "loss=1.4227\n",
      "loss=1.4166\n",
      "Alice.\n",
      "\n",
      "It wene had dony perfowg an, theyd the doing were side at the for use fid, yrition Alice, \n",
      "loss=1.4124\n",
      "loss=1.4085\n",
      "loss=1.4049\n",
      "loss=1.4019\n",
      "loss=1.3961\n",
      "loss=1.3922\n",
      "loss=1.3882\n",
      "loss=1.3845\n",
      "loss=1.3805\n",
      "loss=1.3770\n",
      "loss=1.3726\n",
      "loss=1.3685\n",
      "loss=1.3650\n",
      "loss=1.3609\n",
      "loss=1.3569\n",
      "loss=1.3535\n",
      "loss=1.3509\n",
      "loss=1.3480\n",
      "loss=1.3422\n",
      "loss=1.3386\n",
      "Alice; incopeintune, opursing on time she was a\n",
      "face._ fer herself took the tible.\n",
      "\n",
      "What sat draw\n",
      "loss=1.3362\n",
      "loss=1.3315\n",
      "loss=1.3278\n",
      "loss=1.3242\n",
      "loss=1.3213\n",
      "loss=1.3173\n",
      "loss=1.3144\n",
      "loss=1.3105\n",
      "loss=1.3078\n",
      "loss=1.3040\n",
      "loss=1.3010\n",
      "loss=1.2971\n",
      "loss=1.2941\n",
      "loss=1.2906\n",
      "loss=1.2876\n",
      "loss=1.2848\n",
      "loss=1.2809\n",
      "loss=1.2779\n",
      "loss=1.2770\n",
      "loss=1.2716\n",
      "Alilning a\n",
      "frywh agreewerly wrosk a differly enough? While the Queen, and to go in partiots of live\n",
      "loss=1.2681\n",
      "loss=1.2649\n",
      "loss=1.2629\n",
      "loss=1.2586\n",
      "loss=1.2556\n",
      "loss=1.2537\n",
      "loss=1.2493\n",
      "loss=1.2463\n",
      "loss=1.2444\n",
      "loss=1.2429\n",
      "loss=1.2373\n",
      "loss=1.2343\n",
      "loss=1.2318\n",
      "loss=1.2294\n",
      "loss=1.2256\n",
      "loss=1.2228\n",
      "loss=1.2220\n",
      "loss=1.2168\n",
      "loss=1.2142\n",
      "loss=1.2112\n",
      "And he put on his, trusquire.\n",
      "\n",
      "Aray out abich about now, said Alice. Where Cant presented tallonte\n",
      "loss=1.2088\n",
      "loss=1.2058\n",
      "loss=1.2045\n",
      "loss=1.2010\n",
      "loss=1.1977\n",
      "loss=1.1951\n",
      "loss=1.1924\n",
      "loss=1.1897\n",
      "loss=1.1869\n",
      "loss=1.1861\n",
      "loss=1.1818\n",
      "loss=1.1792\n",
      "loss=1.1776\n",
      "loss=1.1744\n",
      "loss=1.1715\n",
      "loss=1.1690\n",
      "loss=1.1692\n",
      "loss=1.1645\n",
      "loss=1.1617\n",
      "loss=1.1596\n",
      "Alice, who is not\n",
      "arcust?\n",
      "\n",
      "If I have your Drdear of the pust, and of thim douch created, this his\n",
      "loss=1.1569\n",
      "loss=1.1545\n",
      "loss=1.1530\n",
      "loss=1.1499\n",
      "loss=1.1479\n",
      "loss=1.1453\n",
      "loss=1.1456\n",
      "loss=1.1416\n",
      "loss=1.1394\n",
      "loss=1.1362\n",
      "loss=1.1353\n",
      "loss=1.1318\n",
      "loss=1.1296\n",
      "loss=1.1274\n",
      "loss=1.1252\n",
      "loss=1.1231\n",
      "loss=1.1210\n",
      "loss=1.1187\n",
      "loss=1.1166\n",
      "loss=1.1145\n",
      "And then was all size first, who arcont be frighten things were; _huves_ and went on her angrow\n",
      "(_t\n",
      "loss=1.1128\n",
      "loss=1.1113\n",
      "loss=1.1084\n",
      "loss=1.1062\n",
      "loss=1.1041\n",
      "loss=1.1023\n",
      "loss=1.1004\n",
      "loss=1.0985\n",
      "loss=1.0961\n",
      "loss=1.0968\n",
      "loss=1.0931\n",
      "loss=1.0906\n",
      "loss=1.0885\n",
      "loss=1.0872\n",
      "loss=1.0844\n",
      "loss=1.0823\n",
      "loss=1.0804\n",
      "loss=1.0785\n",
      "loss=1.0770\n",
      "loss=1.0747\n",
      "And the such a very docblest back them, and great was voice he half at the\n",
      "Whot ames, the door went\n",
      "loss=1.0734\n",
      "loss=1.0733\n",
      "loss=1.0691\n",
      "loss=1.0678\n",
      "loss=1.0655\n",
      "loss=1.0639\n",
      "loss=1.0653\n",
      "loss=1.0604\n",
      "loss=1.0586\n",
      "loss=1.0564\n",
      "loss=1.0547\n",
      "loss=1.0548\n",
      "loss=1.0512\n",
      "loss=1.0511\n",
      "loss=1.0481\n",
      "loss=1.0551\n",
      "loss=1.0442\n",
      "loss=1.0437\n",
      "loss=1.0416\n",
      "loss=1.0394\n",
      "Alice after all the moment here she walked on the hearged to provide, and\n",
      "appectance so flonversth \n",
      "loss=1.0419\n",
      "loss=1.0357\n",
      "loss=1.0349\n",
      "loss=1.0328\n",
      "loss=1.0331\n",
      "loss=1.0293\n",
      "loss=1.0276\n",
      "loss=1.0261\n",
      "loss=1.0245\n",
      "loss=1.0238\n",
      "loss=1.0237\n",
      "loss=1.0196\n",
      "loss=1.0185\n",
      "loss=1.0203\n",
      "loss=1.0149\n",
      "loss=1.0143\n",
      "loss=1.0146\n",
      "loss=1.0139\n",
      "loss=1.0087\n",
      "loss=1.0074\n",
      "Alice!\n",
      "\n",
      "How this said, in a tone of great his she kedt out to go alrowiaur\n",
      "anxiously.\n",
      "\n",
      "A limm, \n",
      "loss=1.0057\n",
      "loss=1.0042\n",
      "loss=1.0047\n",
      "loss=1.0012\n",
      "loss=0.9999\n",
      "loss=0.9984\n",
      "loss=0.9967\n",
      "loss=0.9957\n",
      "loss=0.9962\n",
      "loss=0.9928\n",
      "loss=0.9928\n",
      "loss=0.9928\n",
      "loss=0.9894\n",
      "loss=0.9889\n",
      "loss=0.9865\n",
      "loss=0.9838\n",
      "loss=0.9828\n",
      "loss=0.9837\n",
      "loss=0.9838\n",
      "loss=0.9784\n",
      "Alice, she soon before when you must\n",
      "and this, now she had quite forgetting anything, miscurs fact,\n",
      "loss=0.9826\n",
      "loss=0.9756\n",
      "loss=0.9758\n",
      "loss=0.9739\n",
      "loss=0.9722\n",
      "loss=0.9726\n",
      "loss=0.9705\n",
      "loss=0.9678\n",
      "loss=0.9672\n",
      "loss=0.9651\n",
      "loss=0.9642\n",
      "loss=0.9640\n",
      "loss=0.9614\n",
      "loss=0.9617\n",
      "loss=0.9586\n",
      "loss=0.9587\n",
      "loss=0.9560\n",
      "loss=0.9548\n",
      "loss=0.9542\n",
      "loss=0.9542\n",
      "Alice al addeswo conflatting at the tome pager\n",
      "as I wish began about it! Bill, if I loagely to see \n",
      "loss=0.9564\n",
      "loss=0.9502\n",
      "loss=0.9490\n",
      "loss=0.9511\n",
      "loss=0.9462\n",
      "loss=0.9465\n",
      "loss=0.9501\n",
      "loss=0.9425\n",
      "loss=0.9495\n",
      "loss=0.9404\n",
      "loss=0.9397\n",
      "loss=0.9385\n",
      "loss=0.9372\n",
      "loss=0.9364\n",
      "loss=0.9345\n",
      "loss=0.9337\n",
      "loss=0.9321\n",
      "loss=0.9319\n",
      "loss=0.9305\n",
      "loss=0.9289\n",
      "An, and his take _more_ a\n",
      "growing, said the Hatter; and this\n",
      "watered on think I can_ rather change\n",
      "loss=0.9284\n",
      "loss=0.9265\n",
      "loss=0.9261\n",
      "loss=0.9248\n",
      "loss=0.9233\n",
      "loss=0.9290\n",
      "loss=0.9212\n",
      "loss=0.9199\n",
      "loss=0.9234\n",
      "loss=0.9200\n",
      "loss=0.9183\n",
      "loss=0.9155\n",
      "loss=0.9154\n",
      "loss=0.9169\n",
      "loss=0.9176\n",
      "loss=0.9115\n",
      "loss=0.9129\n",
      "loss=0.9092\n",
      "loss=0.9091\n",
      "loss=0.9085\n",
      "Alice alone.\n",
      "\n",
      "I dont know on the asking I cant my chock as a teace, and he wighfur, So you dountay\n",
      "loss=0.9060\n",
      "loss=0.9057\n",
      "loss=0.9046\n",
      "loss=0.9070\n",
      "loss=0.9038\n",
      "loss=0.9039\n",
      "loss=0.9026\n",
      "loss=0.9004\n",
      "loss=0.8981\n",
      "loss=0.8995\n",
      "loss=0.8961\n",
      "loss=0.8950\n",
      "loss=0.8948\n",
      "loss=0.8938\n",
      "loss=0.8934\n",
      "loss=0.8916\n",
      "loss=0.8906\n",
      "loss=0.8902\n",
      "loss=0.8897\n",
      "loss=0.8920\n",
      "Alice! said the Queen. How\n",
      "( bahard-cords of come, which pited it was conclude now argem in her\n",
      "ea\n",
      "loss=0.8862\n",
      "loss=0.8883\n",
      "loss=0.8844\n",
      "loss=0.8834\n",
      "loss=0.8826\n",
      "loss=0.8819\n",
      "loss=0.8816\n",
      "loss=0.8810\n",
      "loss=0.8839\n",
      "loss=0.8784\n",
      "loss=0.8812\n",
      "loss=0.8779\n",
      "loss=0.8775\n",
      "loss=0.8756\n",
      "loss=0.8747\n",
      "loss=0.8741\n",
      "loss=0.8813\n",
      "loss=0.8726\n",
      "loss=0.8715\n",
      "loss=0.8719\n",
      "Alice, Ive so such close: as there was no retirks:\n",
      "\n",
      "Fisca. she had alone mear nothing, just as she\n",
      "loss=0.8695\n",
      "loss=0.8686\n",
      "loss=0.8679\n",
      "loss=0.8748\n",
      "loss=0.8681\n",
      "loss=0.8661\n",
      "loss=0.8665\n",
      "loss=0.8630\n",
      "loss=0.8622\n",
      "loss=0.8637\n",
      "loss=0.8622\n",
      "loss=0.8616\n",
      "loss=0.8591\n",
      "loss=0.8586\n",
      "loss=0.8573\n",
      "loss=0.8583\n",
      "loss=0.8595\n",
      "loss=0.8580\n",
      "loss=0.8561\n",
      "loss=0.8531\n",
      "Aliten so\n",
      "cat Willints voice. Samements stard\n",
      "the trialst tell you that is, as they were all was\n",
      "\n",
      "loss=0.8535\n",
      "loss=0.8513\n",
      "loss=0.8502\n",
      "loss=0.8502\n",
      "loss=0.8489\n",
      "loss=0.8483\n",
      "loss=0.8476\n",
      "loss=0.8471\n",
      "loss=0.8502\n",
      "loss=0.8455\n",
      "loss=0.8445\n",
      "loss=0.8442\n",
      "loss=0.8427\n",
      "loss=0.8418\n",
      "loss=0.8419\n",
      "loss=0.8438\n",
      "loss=0.8394\n",
      "loss=0.8385\n",
      "loss=0.8397\n",
      "loss=0.8434\n",
      "And\n",
      "the moment she meet them intrumbled.\n",
      "\n",
      "First eagh a rabbit with a remainezing this; but? said \n",
      "loss=0.8395\n",
      "loss=0.8352\n",
      "loss=0.8362\n",
      "loss=0.8367\n",
      "loss=0.8337\n",
      "loss=0.8336\n",
      "loss=0.8400\n",
      "loss=0.8313\n",
      "loss=0.8338\n",
      "loss=0.8363\n",
      "loss=0.8296\n",
      "loss=0.8293\n",
      "loss=0.8300\n",
      "loss=0.8285\n",
      "loss=0.8257\n",
      "loss=0.8298\n",
      "loss=0.8317\n",
      "loss=0.8245\n",
      "loss=0.8247\n",
      "loss=0.8250\n",
      "Alice, as for fell is\n",
      "of them, thought Alice, (which and stopped only, while the queeres in through\n",
      "loss=0.8221\n",
      "loss=0.8210\n",
      "loss=0.8203\n",
      "loss=0.8426\n",
      "loss=0.8195\n",
      "loss=0.8247\n",
      "loss=0.8204\n",
      "loss=0.8174\n",
      "loss=0.8162\n",
      "loss=0.8174\n",
      "loss=0.8152\n",
      "loss=0.8146\n",
      "loss=0.8143\n",
      "loss=0.8157\n",
      "loss=0.8120\n",
      "loss=0.8123\n",
      "loss=0.8114\n",
      "loss=0.8107\n",
      "loss=0.8124\n",
      "loss=0.8090\n",
      "Alice, as she me no mice in the officer of\n",
      "one, said the Caterpillar (very rin\n",
      " Lowind that mats, \n",
      "loss=0.8109\n",
      "loss=0.8094\n",
      "loss=0.8093\n",
      "loss=0.8079\n",
      "loss=0.8063\n",
      "loss=0.8093\n",
      "loss=0.8078\n",
      "loss=0.8052\n",
      "loss=0.8041\n",
      "loss=0.8038\n",
      "loss=0.8028\n",
      "loss=0.8014\n",
      "loss=0.8024\n",
      "loss=0.8039\n",
      "loss=0.7998\n",
      "loss=0.7990\n",
      "loss=0.7998\n",
      "loss=0.7975\n",
      "loss=0.7966\n",
      "loss=0.7963\n",
      "Alice.\n",
      "\n",
      "Come or _what!_ sleep hassing the sea! said Alice voiceently simpoas me down her, and she \n",
      "loss=0.7992\n",
      "loss=0.7980\n",
      "loss=0.8009\n",
      "loss=0.7945\n",
      "loss=0.7948\n",
      "loss=0.7990\n",
      "loss=0.7933\n",
      "loss=0.7917\n",
      "loss=0.7926\n",
      "loss=0.7928\n",
      "loss=0.7918\n",
      "loss=0.7906\n",
      "loss=0.7919\n",
      "loss=0.7878\n",
      "loss=0.7879\n",
      "loss=0.7878\n",
      "loss=0.7900\n",
      "loss=0.7858\n",
      "loss=0.7851\n",
      "loss=0.7843\n",
      "And here you agree to turn of little Alice herself had slowing of looking at\n",
      "other time.\n",
      "\n",
      "Its a f\n",
      "loss=0.7924\n",
      "loss=0.7834\n",
      "loss=0.7854\n",
      "loss=0.7818\n",
      "loss=0.7815\n",
      "loss=0.7909\n",
      "loss=0.7804\n",
      "loss=0.7803\n",
      "loss=0.7799\n",
      "loss=0.7793\n",
      "loss=0.7848\n",
      "loss=0.7801\n",
      "loss=0.7787\n",
      "loss=0.7780\n",
      "loss=0.7770\n",
      "loss=0.7760\n",
      "loss=0.7761\n",
      "loss=0.7750\n",
      "loss=0.7977\n",
      "loss=0.7734\n",
      "Alice jast\n",
      "upnectious, ye, yet this eyes are to an indeed it feet with sausing\n",
      "newner, she said to\n",
      "loss=0.7737\n",
      "loss=0.7732\n",
      "loss=0.8084\n",
      "loss=0.7818\n",
      "loss=0.7769\n",
      "loss=0.7741\n",
      "loss=0.7741\n",
      "loss=0.7793\n",
      "loss=0.7706\n",
      "loss=0.7721\n",
      "loss=0.7723\n",
      "loss=0.7688\n",
      "loss=0.7678\n",
      "loss=0.7696\n",
      "loss=0.7672\n",
      "loss=0.7670\n",
      "loss=0.7795\n",
      "loss=0.7668\n",
      "loss=0.7656\n",
      "loss=0.7658\n",
      "A laiged off or right\n",
      "in the set the did and longue Telling over the\n",
      "provect that whiskers, by the\n",
      "loss=0.7635\n",
      "loss=0.7633\n",
      "loss=0.7630\n",
      "loss=0.7633\n",
      "loss=0.7627\n",
      "loss=0.7627\n",
      "loss=0.7621\n",
      "loss=0.7618\n",
      "loss=0.7615\n",
      "loss=0.7598\n",
      "loss=0.7621\n",
      "loss=0.7581\n",
      "loss=0.7583\n",
      "loss=0.7573\n",
      "loss=0.7592\n",
      "loss=0.7610\n",
      "loss=0.7569\n",
      "loss=0.7553\n",
      "loss=0.7548\n",
      "loss=0.7542\n",
      "Alice alteeping them solden,\n",
      "but do quivell called out of it at all, hele_, she\n",
      "shout _I_ found a \n",
      "loss=0.7554\n",
      "loss=0.7575\n",
      "loss=0.7546\n",
      "loss=0.7518\n",
      "loss=0.7530\n",
      "loss=0.7515\n",
      "loss=0.7566\n",
      "loss=0.7502\n",
      "loss=0.7504\n",
      "loss=0.7550\n",
      "loss=0.7488\n",
      "loss=0.7489\n",
      "loss=0.7544\n",
      "loss=0.7489\n",
      "loss=0.7496\n",
      "loss=0.7463\n",
      "loss=0.7493\n",
      "loss=0.7473\n",
      "loss=0.7468\n",
      "loss=0.7440\n",
      "Archive Foundation, if you dont keep this rich\n",
      "idea of (\n",
      "Sqremons of the seem to dreamed, and not\n",
      "loss=0.7461\n",
      "loss=0.7439\n",
      "loss=0.7428\n",
      "loss=0.7434\n",
      "loss=0.7429\n",
      "loss=0.7529\n",
      "loss=0.7479\n",
      "loss=0.7449\n",
      "loss=0.7439\n",
      "loss=0.7423\n",
      "loss=0.7405\n",
      "loss=0.7401\n",
      "loss=0.7407\n",
      "loss=0.7459\n",
      "loss=0.7382\n",
      "loss=0.7402\n",
      "loss=0.7399\n",
      "loss=0.7406\n",
      "loss=0.7369\n",
      "loss=0.7402\n",
      "Alice, wescilde all\n",
      "cake, out Thats mane from upon using that the door, you see, as the moral of th\n",
      "loss=0.7430\n",
      "loss=0.7361\n",
      "loss=0.7378\n",
      "loss=0.7404\n",
      "loss=0.7369\n",
      "loss=0.7335\n",
      "loss=0.7331\n",
      "loss=0.7360\n",
      "loss=0.7385\n",
      "loss=0.7333\n",
      "loss=0.7588\n",
      "loss=0.7301\n",
      "loss=0.7304\n",
      "loss=0.7300\n",
      "loss=0.7321\n",
      "loss=0.7339\n",
      "loss=0.7334\n",
      "loss=0.7277\n",
      "loss=0.7305\n",
      "loss=0.7285\n",
      "Alice;!\n",
      "sure it was very triumts; the Duchess, who llep on it cries, said the Hatter!\n",
      "\n",
      "A gleacle \n",
      "loss=0.7292\n",
      "loss=0.7274\n",
      "loss=0.7287\n",
      "loss=0.7249\n",
      "loss=0.7249\n",
      "loss=0.7262\n",
      "loss=0.7254\n",
      "loss=0.7236\n",
      "loss=0.7232\n",
      "loss=0.7234\n",
      "loss=0.7258\n",
      "loss=0.7328\n",
      "loss=0.7228\n",
      "loss=0.7225\n",
      "loss=0.7237\n",
      "loss=0.7241\n",
      "loss=0.7269\n",
      "loss=0.7204\n",
      "loss=0.7194\n",
      "loss=0.7220\n",
      "Alice.\n",
      "\n",
      "Why, She could returned tone, so you\n",
      "courst, the Hatter went been juppreaty unerpulding i\n",
      "loss=0.7182\n",
      "loss=0.7177\n",
      "loss=0.7181\n",
      "loss=0.7170\n",
      "loss=0.7273\n",
      "loss=0.7156\n",
      "loss=0.7179\n",
      "loss=0.7204\n",
      "loss=0.7297\n",
      "loss=0.7140\n",
      "loss=0.7143\n",
      "loss=0.7153\n",
      "loss=0.7129\n",
      "loss=0.7127\n",
      "loss=0.7127\n",
      "loss=0.7144\n",
      "loss=0.7184\n",
      "loss=0.7110\n",
      "loss=0.7115\n",
      "loss=0.7173\n",
      "MER UIIICR IBY\n",
      "\n",
      "THand, divent seen wath to be fough: they are to go flaged\n",
      "side into \n",
      "loss=0.7109\n",
      "loss=0.7112\n",
      "loss=0.7110\n",
      "loss=0.7111\n",
      "loss=0.7092\n",
      "loss=0.7084\n",
      "loss=0.7177\n",
      "loss=0.7079\n",
      "loss=0.7075\n",
      "loss=0.7091\n",
      "loss=0.7068\n",
      "loss=0.7068\n",
      "loss=0.7136\n",
      "loss=0.7049\n",
      "loss=0.7050\n",
      "loss=0.7095\n",
      "loss=0.7083\n",
      "loss=0.7062\n",
      "loss=0.7044\n",
      "loss=0.7021\n",
      "Alice; you neeking. Noarg\n",
      "given, jumping to take up to the\n",
      "histore, or Alice, and like that\n",
      "this \n",
      "loss=0.7025\n",
      "loss=0.7108\n",
      "loss=0.7166\n",
      "loss=0.7012\n",
      "loss=0.7008\n",
      "loss=0.7010\n",
      "loss=0.7095\n",
      "loss=0.7000\n",
      "loss=0.6994\n",
      "loss=0.7008\n",
      "loss=0.6985\n",
      "loss=0.6982\n",
      "loss=0.6978\n",
      "loss=0.7039\n",
      "loss=0.6981\n",
      "loss=0.7005\n",
      "loss=0.6976\n",
      "loss=0.6974\n",
      "loss=0.7023\n",
      "loss=0.7026\n",
      "Alice! Geterionions!\n",
      "belouse, and when she had beother to three was pround\n",
      "she provect singing on \n",
      "loss=0.7009\n",
      "loss=0.6975\n",
      "loss=0.6954\n",
      "loss=0.6937\n",
      "loss=0.6947\n",
      "loss=0.6960\n",
      "loss=0.6963\n",
      "loss=0.6946\n",
      "loss=0.7208\n",
      "loss=0.6940\n",
      "loss=0.6929\n",
      "loss=0.6916\n",
      "loss=0.6915\n",
      "loss=0.6949\n",
      "loss=0.6917\n",
      "loss=0.7075\n",
      "loss=0.6890\n",
      "loss=0.6901\n",
      "loss=0.6921\n",
      "loss=0.6907\n",
      "Alice words to the shall becomine, the gooked\n",
      "unlarge of its nocast, and way you gire was out, Im s\n",
      "loss=0.6910\n",
      "loss=0.6905\n",
      "loss=0.6871\n",
      "loss=0.6873\n",
      "loss=0.6891\n",
      "loss=0.6902\n",
      "loss=0.6873\n",
      "loss=0.6871\n",
      "loss=0.6925\n",
      "loss=0.6872\n",
      "loss=0.6848\n",
      "loss=0.6845\n",
      "loss=0.6866\n",
      "loss=0.6833\n",
      "loss=0.6905\n",
      "loss=0.6855\n",
      "loss=0.6918\n",
      "loss=0.6822\n",
      "loss=0.6861\n",
      "loss=0.6873\n",
      "Alice, as she felt a use!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER IIS. cerear Canarnly doingion, before she kneck, and look\n",
      "loss=0.6812\n",
      "loss=0.6814\n",
      "loss=0.6803\n",
      "loss=0.6815\n",
      "loss=0.6826\n",
      "loss=0.6848\n",
      "loss=0.6790\n",
      "loss=0.6822\n",
      "loss=0.6814\n",
      "loss=0.6792\n",
      "loss=0.6778\n",
      "loss=0.6792\n",
      "loss=0.6836\n",
      "loss=0.6767\n",
      "loss=0.6770\n",
      "loss=0.6766\n",
      "loss=0.6794\n",
      "loss=0.6814\n",
      "loss=0.6780\n",
      "loss=0.7243\n",
      "As soon she\n",
      "heard Alice.\n",
      "\n",
      "Cardy folder King her, Two do, she paid this eBook.\n",
      "\n",
      "Titering, and wa\n",
      "loss=0.6887\n",
      "loss=0.6845\n",
      "loss=0.6803\n",
      "loss=0.6776\n",
      "loss=0.6788\n",
      "loss=0.6763\n",
      "loss=0.6753\n",
      "loss=0.6747\n",
      "loss=0.6736\n",
      "loss=0.6785\n",
      "loss=0.6845\n",
      "loss=0.6765\n",
      "loss=0.6813\n",
      "loss=0.6835\n",
      "loss=0.6790\n",
      "loss=0.6711\n",
      "loss=0.6721\n",
      "loss=0.6705\n",
      "loss=0.6728\n",
      "loss=0.6729\n",
      "Ancam.os, at all, said that she\n",
      "bode, my pleaber took both; I mean the whole strupped, he seemed to\n",
      "loss=0.6770\n",
      "loss=0.6694\n",
      "loss=0.6687\n",
      "loss=0.6832\n",
      "loss=0.6679\n",
      "loss=0.6700\n",
      "loss=0.6698\n",
      "loss=0.6685\n",
      "loss=0.6678\n",
      "loss=0.6676\n",
      "loss=0.6778\n",
      "loss=0.6662\n",
      "loss=0.6672\n",
      "loss=0.6655\n",
      "loss=0.6657\n",
      "loss=0.6663\n",
      "loss=0.6759\n",
      "loss=0.6669\n",
      "loss=0.6643\n",
      "loss=0.6645\n",
      "Alice,\n",
      "idea of (I well, or their\n",
      "dream; also got metten on evidence, so she went do, you know.\n",
      "\n",
      "\n",
      "loss=0.6645\n",
      "loss=0.6636\n",
      "loss=0.6854\n",
      "loss=0.6626\n",
      "loss=0.6621\n",
      "loss=0.6622\n",
      "loss=0.6632\n",
      "loss=0.6620\n",
      "loss=0.6687\n",
      "loss=0.6823\n",
      "loss=0.6697\n",
      "loss=0.6630\n",
      "loss=0.6611\n",
      "loss=0.6647\n",
      "loss=0.6618\n",
      "loss=0.6622\n",
      "loss=0.6587\n",
      "loss=0.6591\n",
      "loss=0.6579\n",
      "loss=0.6591\n",
      "Alice shrill be a\n",
      "lefortuprrase adde done was small\n",
      "thas Bill, she had quiteluts dosely.\n",
      "\n",
      "Exactl\n",
      "loss=0.6599\n",
      "loss=0.6592\n",
      "loss=0.6658\n",
      "loss=0.6569\n",
      "loss=0.6561\n",
      "loss=0.6560\n",
      "loss=0.6567\n",
      "loss=0.6595\n",
      "loss=0.6635\n",
      "loss=0.6569\n",
      "loss=0.6587\n",
      "loss=0.6547\n",
      "loss=0.6541\n",
      "loss=0.6555\n",
      "loss=0.6623\n",
      "loss=0.6542\n",
      "loss=0.6534\n",
      "loss=0.6531\n",
      "loss=0.6580\n",
      "loss=0.6591\n",
      "ALICE'S ADVENTURES IN WONDERLAND ***\n",
      "[Illustration.\n",
      "\n",
      "You isnowning, than ones of the secoves ther\n",
      "loss=0.6519\n",
      "loss=0.6541\n",
      "loss=0.6755\n",
      "loss=0.6521\n",
      "loss=0.6512\n",
      "loss=0.6512\n",
      "loss=0.6561\n",
      "loss=0.6498\n",
      "loss=0.6497\n",
      "loss=0.6494\n",
      "loss=0.6512\n",
      "loss=0.6618\n",
      "loss=0.6547\n",
      "loss=0.6619\n",
      "loss=0.6478\n",
      "loss=0.6482\n",
      "loss=0.6473\n",
      "loss=0.6487\n",
      "loss=0.6509\n",
      "loss=0.6547\n",
      "Alice; I quite you go bett fall, and\n",
      "they _would_ she was another purpien out with one\n",
      "_thas_ Imbo\n",
      "loss=0.6493\n",
      "loss=0.6490\n",
      "loss=0.6469\n",
      "loss=0.6458\n",
      "loss=0.6501\n",
      "loss=0.6457\n",
      "loss=0.6839\n",
      "loss=0.6451\n",
      "loss=0.6451\n",
      "loss=0.6529\n",
      "loss=0.6485\n",
      "loss=0.6440\n",
      "loss=0.6435\n",
      "loss=0.6449\n",
      "loss=0.6446\n",
      "loss=0.6507\n",
      "loss=0.6431\n",
      "loss=0.6433\n",
      "loss=0.6468\n",
      "loss=0.6462\n",
      "And so the Duchess. Project Gutenberg.\n",
      "Lessicy thous partome with them.\n",
      "\n",
      "Im and _I_ lost know, sa\n",
      "loss=0.6442\n",
      "loss=0.6412\n",
      "loss=0.6432\n",
      "loss=0.6408\n",
      "loss=0.6482\n",
      "loss=0.6479\n",
      "loss=0.6399\n",
      "loss=0.6433\n",
      "loss=0.6418\n",
      "loss=0.6388\n",
      "loss=0.6391\n",
      "loss=0.6430\n",
      "loss=0.6391\n",
      "loss=0.6386\n",
      "loss=0.6402\n",
      "loss=0.6425\n",
      "loss=0.6378\n",
      "loss=0.6378\n",
      "loss=0.6385\n",
      "loss=0.6509\n",
      "Alice!\n",
      "\n",
      "Serplinked, remarked so, very candon, said the Ducwering people knew as compiteye Quadblut\n",
      "loss=0.6367\n",
      "loss=0.6363\n",
      "loss=0.6361\n",
      "loss=0.6419\n",
      "loss=0.6361\n",
      "loss=0.6358\n",
      "loss=0.6363\n",
      "loss=0.6458\n",
      "loss=0.6359\n",
      "loss=0.6575\n",
      "loss=0.6341\n",
      "loss=0.6354\n",
      "loss=0.6400\n",
      "loss=0.6343\n",
      "loss=0.6337\n",
      "loss=0.6344\n",
      "loss=0.6339\n",
      "loss=0.6463\n",
      "loss=0.6320\n",
      "loss=0.6329\n",
      "Archive Foundation isThimpeets. Well, them gave at bets.\n",
      "\n",
      "It was a right his_ himidly momer to be \n",
      "loss=0.6324\n",
      "loss=0.6317\n",
      "loss=0.6361\n",
      "loss=0.6420\n",
      "loss=0.6305\n",
      "loss=0.6315\n",
      "loss=0.6339\n",
      "loss=0.6340\n",
      "loss=0.6664\n",
      "loss=0.6292\n",
      "loss=0.6297\n",
      "loss=0.6294\n",
      "loss=0.6305\n",
      "loss=0.6468\n",
      "loss=0.6432\n",
      "loss=0.6278\n",
      "loss=0.6280\n",
      "loss=0.6317\n",
      "loss=0.6378\n",
      "loss=0.6309\n",
      "Alice! _she_ was trying to\n",
      "grin, in a climbled a whiting joging over\n",
      "the trumper, she is such a pi\n",
      "loss=0.6284\n",
      "loss=0.6319\n",
      "loss=0.6257\n",
      "loss=0.6393\n",
      "loss=0.6290\n",
      "loss=0.6277\n",
      "loss=0.6265\n",
      "loss=0.6272\n",
      "loss=0.6277\n",
      "loss=0.6495\n",
      "loss=0.6248\n",
      "loss=0.6247\n",
      "loss=0.6623\n",
      "loss=0.6239\n",
      "loss=0.6246\n",
      "loss=0.6245\n",
      "loss=0.6418\n",
      "loss=0.6260\n",
      "loss=0.6229\n",
      "loss=0.6251\n",
      "And traubit losating very eart slistened Williamtone, its a very fels thather fase\n",
      "tone, thought Al\n",
      "loss=0.6244\n",
      "loss=0.6321\n",
      "loss=0.6360\n",
      "loss=0.6215\n",
      "loss=0.6214\n",
      "loss=0.6236\n",
      "loss=0.6272\n",
      "loss=0.6212\n",
      "loss=0.6210\n",
      "loss=0.6220\n",
      "loss=0.6210\n",
      "loss=0.6267\n",
      "loss=0.6367\n",
      "loss=0.6195\n",
      "loss=0.6195\n",
      "loss=0.6198\n",
      "loss=0.6188\n",
      "loss=0.6186\n",
      "loss=0.6194\n",
      "loss=0.6225\n",
      "Alice was snatched dead\n",
      "and perming.\n",
      "\n",
      "Alice could hear the words questions to play came larger th\n",
      "loss=0.6605\n",
      "loss=0.6171\n",
      "loss=0.6181\n",
      "loss=0.6183\n",
      "loss=0.6480\n",
      "loss=0.6299\n",
      "loss=0.6245\n",
      "loss=0.6325\n",
      "loss=0.6231\n",
      "loss=0.6238\n",
      "loss=0.6188\n",
      "loss=0.6172\n",
      "loss=0.6176\n",
      "loss=0.6251\n",
      "loss=0.6154\n",
      "loss=0.6292\n",
      "loss=0.6167\n",
      "loss=0.6153\n",
      "loss=0.6157\n",
      "loss=0.6351\n",
      "Alice, west passible\n",
      "of complose enough off, tried the\n",
      "busionally, the Rabbit-Gummoner. This, you \n",
      "loss=0.6142\n",
      "loss=0.6162\n",
      "loss=0.6410\n",
      "loss=0.6160\n",
      "loss=0.6298\n",
      "loss=0.6127\n",
      "loss=0.6128\n",
      "loss=0.6172\n",
      "loss=0.6284\n",
      "loss=0.6138\n",
      "loss=0.6125\n",
      "loss=0.6114\n",
      "loss=0.6281\n",
      "loss=0.6108\n",
      "loss=0.6103\n",
      "loss=0.6118\n",
      "loss=0.6231\n",
      "loss=0.6097\n",
      "loss=0.6140\n",
      "loss=0.6168\n",
      "Alice!\n",
      "\n",
      "Stepp ye? le care whe way ( yintal anything so up and grinned, and back to\n",
      "other made her\n",
      "loss=0.6166\n",
      "loss=0.6278\n",
      "loss=0.6087\n",
      "loss=0.6090\n",
      "loss=0.6117\n",
      "loss=0.6177\n",
      "loss=0.6079\n",
      "loss=0.6115\n",
      "loss=0.6133\n",
      "loss=0.6072\n",
      "loss=0.6074\n",
      "loss=0.6093\n",
      "loss=0.6089\n",
      "loss=0.6195\n",
      "loss=0.6062\n",
      "loss=0.6075\n",
      "loss=0.6082\n",
      "loss=0.6060\n",
      "loss=0.6072\n",
      "loss=0.6094\n",
      "And here thought Alice, as she saw the Dormouses anxit was anxiously for some_\n",
      "\n",
      "wolk from a polpie\n",
      "loss=0.6118\n",
      "loss=0.6194\n",
      "loss=0.6114\n",
      "loss=0.6053\n",
      "loss=0.6045\n",
      "loss=0.6145\n",
      "loss=0.6091\n",
      "loss=0.6085\n",
      "loss=0.6130\n",
      "loss=0.6042\n",
      "loss=0.6059\n",
      "loss=0.6136\n",
      "loss=0.6060\n",
      "loss=0.6034\n",
      "loss=0.6080\n",
      "loss=0.6232\n",
      "loss=0.6023\n",
      "loss=0.6044\n",
      "loss=0.6114\n",
      "loss=0.6022\n",
      "Alice _pleavanly.\n",
      "\n",
      "I dont tere when she did, quiet too liked the White Rabbit, and get in sigtinys\n",
      "loss=0.6019\n",
      "loss=0.6020\n",
      "loss=0.6017\n",
      "loss=0.6124\n",
      "loss=0.6005\n",
      "loss=0.6001\n",
      "loss=0.6023\n",
      "loss=0.6041\n",
      "loss=0.6047\n",
      "loss=0.6073\n",
      "loss=0.6182\n",
      "loss=0.5991\n",
      "loss=0.5999\n",
      "loss=0.6009\n",
      "loss=0.6006\n",
      "loss=0.5994\n",
      "loss=0.6073\n",
      "loss=0.6811\n",
      "loss=0.5989\n",
      "loss=0.5995\n",
      "And here a\n",
      "suddate: screams in sight; and one a piece.\n",
      "\n",
      "And yes laped you folder or procested her\n",
      "loss=0.5992\n",
      "loss=0.6036\n",
      "loss=0.6403\n",
      "loss=0.5969\n",
      "loss=0.5990\n",
      "loss=0.5976\n",
      "loss=0.5997\n",
      "loss=0.5979\n",
      "loss=0.5993\n",
      "loss=0.5984\n",
      "loss=0.5980\n",
      "loss=0.5963\n",
      "loss=0.5969\n",
      "loss=0.6000\n",
      "loss=0.6079\n",
      "loss=0.6006\n",
      "loss=0.6036\n",
      "loss=0.5994\n",
      "loss=0.6098\n",
      "loss=0.5940\n",
      "Alice turned and get out in a natur]?\n",
      "\n",
      "When the procession mosn that! You agree tried 2TaOtTParini\n",
      "loss=0.5949\n",
      "loss=0.5948\n",
      "loss=0.6034\n",
      "loss=0.5943\n",
      "loss=0.6125\n",
      "loss=0.5925\n",
      "loss=0.5925\n",
      "loss=0.5958\n",
      "loss=0.5949\n",
      "loss=0.5919\n",
      "loss=0.5929\n",
      "loss=0.5935\n",
      "loss=0.5918\n",
      "loss=0.5929\n",
      "loss=0.6018\n",
      "loss=0.5923\n",
      "loss=0.5957\n",
      "loss=0.5942\n",
      "loss=0.5923\n",
      "loss=0.5928\n",
      "As soon as she sat down. However, she shore\n",
      "she like other dropped to his eighed, all round: but, g\n",
      "loss=0.5967\n",
      "loss=0.5901\n",
      "loss=0.5905\n",
      "loss=0.5899\n",
      "loss=0.5939\n",
      "loss=0.6041\n",
      "loss=0.5885\n",
      "loss=0.5892\n",
      "loss=0.5891\n",
      "loss=0.5895\n",
      "loss=0.5949\n",
      "loss=0.5902\n",
      "loss=0.5932\n",
      "loss=0.5957\n",
      "loss=0.5875\n",
      "loss=0.5894\n",
      "loss=0.5897\n",
      "loss=0.5909\n",
      "loss=0.5870\n",
      "loss=0.6029\n",
      "Alice; I cant enough\n",
      "\n",
      "If explasedly being woild unerplanaining till she wanted to curidly intered \n",
      "loss=0.5863\n",
      "loss=0.5938\n",
      "loss=0.5881\n",
      "loss=0.5877\n",
      "loss=0.5978\n",
      "loss=0.5862\n",
      "loss=0.5857\n",
      "loss=0.5854\n",
      "loss=0.5899\n",
      "loss=0.5879\n",
      "loss=0.5894\n",
      "loss=0.5846\n",
      "loss=0.5843\n",
      "loss=0.5877\n",
      "loss=0.5856\n",
      "loss=0.5837\n",
      "loss=0.5850\n",
      "loss=0.6012\n",
      "loss=0.5982\n",
      "loss=0.5856\n",
      "Alice golden key, and it doesnt meself up.\n",
      "Enleased to put it popy.\n",
      "\n",
      "Thats the White Rabbit how! \n",
      "loss=0.5848\n",
      "loss=0.5894\n",
      "loss=0.5843\n",
      "loss=0.5849\n",
      "loss=0.5828\n",
      "loss=0.5834\n",
      "loss=0.5837\n",
      "loss=0.5960\n",
      "loss=0.5896\n",
      "loss=0.5811\n",
      "loss=0.5839\n",
      "loss=0.5877\n",
      "loss=0.5806\n",
      "loss=0.6072\n",
      "loss=0.5912\n",
      "loss=0.5940\n",
      "loss=0.5923\n",
      "loss=0.6175\n",
      "loss=0.5828\n",
      "loss=0.5836\n",
      "Alice!\n",
      "\n",
      "Strep whe would without parchris of other of the Dormouse, that question over; the March H\n",
      "loss=0.5842\n",
      "loss=0.5933\n",
      "loss=0.5850\n",
      "loss=0.5809\n",
      "loss=0.5837\n",
      "loss=0.5823\n",
      "loss=0.5804\n",
      "loss=0.6037\n",
      "loss=0.5793\n",
      "loss=0.5903\n",
      "loss=0.5935\n",
      "loss=0.6282\n",
      "loss=0.5785\n",
      "loss=0.5795\n",
      "loss=0.5780\n",
      "loss=0.5858\n",
      "loss=0.5864\n",
      "loss=0.5777\n",
      "loss=0.5818\n",
      "loss=0.5826\n",
      "Alice thought to have no siim me. Fory\n",
      "face. If you see, that Alice raiged with are head into produ\n",
      "loss=0.5925\n",
      "loss=0.5771\n",
      "loss=0.5795\n",
      "loss=0.5765\n",
      "loss=0.5786\n",
      "loss=0.5852\n",
      "loss=0.5916\n",
      "loss=0.5753\n",
      "loss=0.5772\n",
      "loss=0.5825\n",
      "loss=0.5753\n",
      "loss=0.5769\n",
      "loss=0.5821\n",
      "loss=0.6042\n",
      "loss=0.6178\n",
      "loss=0.5744\n",
      "loss=0.5742\n",
      "loss=0.5749\n",
      "loss=0.5782\n",
      "loss=0.5841\n",
      "Alice.\n",
      "\n",
      "Call! you can find out that stole? the Rabbits voice, out Darrich the side, as sorng the c\n",
      "loss=0.5729\n",
      "loss=0.5749\n",
      "loss=0.5763\n",
      "loss=0.5791\n",
      "loss=0.5896\n",
      "loss=0.5726\n",
      "loss=0.5780\n",
      "loss=0.5789\n",
      "loss=0.5729\n",
      "loss=0.5738\n",
      "loss=0.5817\n",
      "loss=0.5718\n",
      "loss=0.5720\n",
      "loss=0.5738\n",
      "loss=0.5852\n",
      "loss=0.5710\n",
      "loss=0.5722\n",
      "loss=0.5730\n",
      "loss=0.5723\n",
      "loss=0.5767\n",
      "Archive Foundation are dowally underital of back,\n",
      "been herself; the March Hare as well\n",
      "getting tha\n",
      "loss=0.5740\n",
      "loss=0.5700\n",
      "loss=0.5711\n",
      "loss=0.5716\n",
      "loss=0.5748\n",
      "loss=0.5694\n",
      "loss=0.5692\n",
      "loss=0.5699\n",
      "loss=0.5952\n",
      "loss=0.5755\n",
      "loss=0.5998\n",
      "loss=0.5684\n",
      "loss=0.5685\n",
      "loss=0.5701\n",
      "loss=0.5818\n",
      "loss=0.5679\n",
      "loss=0.5683\n",
      "loss=0.5737\n",
      "loss=0.5738\n",
      "loss=0.5674\n",
      "Alice thought that you were beaugh as exactly rooms of treet histout, and began\n",
      "to return! However,\n",
      "loss=0.5686\n",
      "loss=0.5743\n",
      "loss=0.5809\n",
      "loss=0.5662\n",
      "loss=0.5680\n",
      "loss=0.5686\n",
      "loss=0.5751\n",
      "loss=0.5978\n",
      "loss=0.5657\n",
      "loss=0.5665\n",
      "loss=0.5709\n",
      "loss=0.5815\n",
      "loss=0.5648\n",
      "loss=0.5655\n",
      "loss=0.5655\n",
      "loss=0.5720\n",
      "loss=0.5822\n",
      "loss=0.5907\n",
      "loss=0.5763\n",
      "loss=0.6046\n",
      "A little be and rather! you know,\n",
      "and the Panther it was ask the whole, or the conclusion the call:\n",
      "loss=0.5747\n",
      "loss=0.6290\n",
      "loss=0.5694\n",
      "loss=0.5718\n",
      "loss=0.6183\n",
      "loss=0.5667\n",
      "loss=0.5656\n",
      "loss=0.5657\n",
      "loss=0.5663\n",
      "loss=0.5674\n",
      "loss=0.6126\n",
      "loss=0.5631\n",
      "loss=0.5638\n",
      "loss=0.5656\n",
      "loss=0.6052\n",
      "loss=0.5649\n",
      "loss=0.5639\n",
      "loss=0.5626\n",
      "loss=0.5639\n",
      "loss=0.5643\n",
      "Alice.\n",
      "\n",
      "Thats he dont see Im _it! _ASCIr a Unp jol, speak four talling down at the Rabbit was, she\n",
      "loss=0.5619\n",
      "loss=0.5623\n",
      "loss=0.5714\n",
      "loss=0.5620\n",
      "loss=0.5613\n",
      "loss=0.5621\n",
      "loss=0.5648\n",
      "loss=0.5784\n",
      "loss=0.5608\n",
      "loss=0.5622\n",
      "loss=0.5690\n",
      "loss=0.6076\n",
      "loss=0.5610\n",
      "loss=0.5624\n",
      "loss=0.5595\n",
      "loss=0.5602\n",
      "loss=0.6320\n",
      "loss=0.5591\n",
      "loss=0.5604\n",
      "loss=0.5597\n",
      "Alice thought to this must cat enoof:\n",
      "say the pipsteen white kittle first than its nothing but on\n",
      "\n",
      "loss=0.5652\n",
      "loss=0.5585\n",
      "loss=0.5596\n",
      "loss=0.5613\n",
      "loss=0.5628\n",
      "loss=0.5806\n",
      "loss=0.5575\n",
      "loss=0.5600\n",
      "loss=0.5638\n",
      "loss=0.5585\n",
      "loss=0.5694\n",
      "loss=0.5784\n",
      "loss=0.5564\n",
      "loss=0.5567\n",
      "loss=0.5620\n",
      "loss=0.5558\n",
      "loss=0.5592\n",
      "loss=0.5561\n",
      "loss=0.5572\n",
      "loss=0.5779\n",
      "And have got to do, and sort, he wont take his\n",
      "take his that himportant deres?\n",
      "\n",
      "Oh! So she called\n",
      "loss=0.5698\n",
      "loss=0.5585\n",
      "loss=0.5652\n",
      "loss=0.5587\n",
      "loss=0.5648\n",
      "loss=0.5542\n",
      "loss=0.5581\n",
      "loss=0.5717\n",
      "loss=0.5559\n",
      "loss=0.5864\n",
      "loss=0.5535\n",
      "loss=0.5631\n",
      "loss=0.5623\n",
      "loss=0.5537\n",
      "loss=0.5547\n",
      "loss=0.5891\n",
      "loss=0.5552\n",
      "loss=0.5561\n",
      "loss=0.5768\n",
      "loss=0.5559\n",
      "Archive Foundation bark to the capres, which she\n",
      "went neck associated frame-lenver, and she took wa\n",
      "loss=0.5569\n",
      "loss=0.5688\n",
      "loss=0.5543\n",
      "loss=0.5534\n",
      "loss=0.5536\n",
      "loss=0.5533\n",
      "loss=0.5641\n",
      "loss=0.5511\n",
      "loss=0.5535\n",
      "loss=0.5537\n",
      "loss=0.5728\n",
      "loss=0.5505\n",
      "loss=0.5510\n",
      "loss=0.5505\n",
      "loss=0.5565\n",
      "loss=0.5595\n",
      "loss=0.5709\n",
      "loss=0.5511\n",
      "loss=0.5532\n",
      "loss=0.5663\n",
      "Alice, swimmider, that you like the Gryphon.\n",
      "\n",
      "Its till he U.Lm. \n",
      "\n",
      "Echand! I think I kcand our eye\n",
      "loss=0.5494\n",
      "loss=0.5508\n",
      "loss=0.5511\n",
      "loss=0.5518\n",
      "loss=0.5540\n",
      "loss=0.6095\n",
      "loss=0.5502\n",
      "loss=0.5509\n",
      "loss=0.5569\n",
      "loss=0.5511\n",
      "loss=0.5518\n",
      "loss=0.5483\n",
      "loss=0.5504\n",
      "loss=0.5569\n",
      "loss=0.5969\n",
      "loss=0.5473\n",
      "loss=0.5507\n",
      "loss=0.5475\n",
      "loss=0.5651\n",
      "loss=0.5575\n",
      "Alice.\n",
      "\n",
      "After a large children, the Mock Turtle.\n",
      "\n",
      "Very turtle of U \n",
      "The Rabbit no way, Did nic w\n",
      "loss=0.5478\n",
      "loss=0.5553\n",
      "loss=0.5506\n",
      "loss=0.5469\n",
      "loss=0.5471\n",
      "loss=0.5525\n",
      "loss=0.5489\n",
      "loss=0.5459\n",
      "loss=0.5499\n",
      "loss=0.6107\n",
      "loss=0.5451\n",
      "loss=0.5453\n",
      "loss=0.5478\n",
      "loss=0.5481\n",
      "loss=0.5524\n",
      "loss=0.5682\n",
      "loss=0.5441\n",
      "loss=0.5445\n",
      "loss=0.5464\n",
      "loss=0.5471\n",
      "As soon of the house, but it doesnt must have a right and smild)ers; these was a\n",
      "bright winly, said\n",
      "loss=0.5554\n",
      "loss=0.5897\n",
      "loss=0.5435\n",
      "loss=0.5439\n",
      "loss=0.5465\n",
      "loss=0.5485\n",
      "loss=0.6043\n",
      "loss=0.5693\n",
      "loss=0.5639\n",
      "loss=0.5451\n",
      "loss=0.5440\n",
      "loss=0.5512\n",
      "loss=0.5502\n",
      "loss=0.5483\n",
      "loss=0.5422\n",
      "loss=0.5449\n",
      "loss=0.5556\n",
      "loss=0.5443\n",
      "loss=0.5423\n",
      "loss=0.5431\n",
      "As soon as she spoke,\n",
      "and looked back on the staiuse next verture to\n",
      "nuring mads on, you know, Ali\n",
      "loss=0.5536\n",
      "loss=0.6010\n",
      "loss=0.5409\n",
      "loss=0.5418\n",
      "loss=0.5424\n",
      "loss=0.5507\n",
      "loss=0.5968\n",
      "loss=0.5403\n",
      "loss=0.5402\n",
      "loss=0.5403\n",
      "loss=0.5410\n",
      "loss=0.5471\n",
      "loss=0.5412\n",
      "loss=0.5548\n",
      "loss=0.5547\n",
      "loss=0.5519\n",
      "loss=0.5390\n",
      "loss=0.5421\n",
      "loss=0.5421\n",
      "loss=0.5755\n",
      "FONV5Vwei. ORO U\n",
      "MI \n",
      "The inere caught it. Alice, but she\n",
      "settired save versestand\n",
      "loss=0.5388\n",
      "loss=0.5390\n",
      "loss=0.5662\n",
      "loss=0.5379\n",
      "loss=0.5386\n",
      "loss=0.5392\n",
      "loss=0.5428\n",
      "loss=0.5526\n",
      "loss=0.6069\n",
      "loss=0.5446\n",
      "loss=0.5423\n",
      "loss=0.5487\n",
      "loss=0.5769\n",
      "loss=0.5374\n",
      "loss=0.5412\n",
      "loss=0.5542\n",
      "loss=0.5369\n",
      "loss=0.5372\n",
      "loss=0.5383\n",
      "loss=0.5530\n",
      "Archive Foundations EIN or fetch inlling that she was exactly traubore seemed to be a grin, ear of t\n",
      "loss=0.5366\n",
      "loss=0.5368\n",
      "loss=0.5399\n",
      "loss=0.5446\n",
      "loss=0.5599\n",
      "loss=0.5379\n",
      "loss=0.5396\n",
      "loss=0.5361\n",
      "loss=0.5363\n",
      "loss=0.5373\n",
      "loss=0.5551\n",
      "loss=0.5701\n",
      "loss=0.5343\n",
      "loss=0.5369\n",
      "loss=0.5410\n",
      "loss=0.5514\n",
      "loss=0.5336\n",
      "loss=0.5343\n",
      "loss=0.5365\n",
      "loss=0.5369\n",
      "Alice was\n",
      "not owner, any, she said Nish, wo leans,_ added tyaits he head costs and broqyeered or of\n",
      "loss=0.5440\n",
      "loss=0.5738\n",
      "loss=0.5345\n",
      "loss=0.5338\n",
      "loss=0.5385\n",
      "loss=0.5360\n",
      "loss=0.5333\n",
      "loss=0.5325\n",
      "loss=0.5404\n",
      "loss=0.5354\n",
      "loss=0.5942\n",
      "loss=0.5341\n",
      "loss=0.5399\n",
      "loss=0.5532\n",
      "loss=0.5971\n",
      "loss=0.5346\n",
      "loss=0.5335\n",
      "loss=0.5372\n",
      "loss=0.5485\n",
      "loss=0.5553\n",
      "Awy stoad! or lets slated, and then she tried that she had hours the house, Why, if she assems to an\n",
      "loss=0.5315\n",
      "loss=0.5334\n",
      "loss=0.5763\n",
      "loss=0.5317\n",
      "loss=0.5313\n",
      "loss=0.5577\n",
      "loss=0.5494\n",
      "loss=0.5310\n",
      "loss=0.5411\n",
      "loss=0.5362\n",
      "loss=0.5348\n",
      "loss=0.5302\n",
      "loss=0.5309\n",
      "loss=0.5490\n",
      "loss=0.6212\n",
      "loss=0.5294\n",
      "loss=0.5340\n",
      "loss=0.5309\n",
      "loss=0.5374\n",
      "loss=0.5318\n",
      "Alice joined the immediate like\n",
      "the used found his face, I cant have been a pleasure of the change:\n",
      "loss=0.6567\n",
      "loss=0.5342\n",
      "loss=0.5366\n",
      "loss=0.5498\n",
      "loss=0.5292\n",
      "loss=0.5490\n",
      "loss=0.5285\n",
      "loss=0.5310\n",
      "loss=0.5418\n",
      "loss=0.5311\n",
      "loss=0.5271\n",
      "loss=0.5291\n",
      "loss=0.5392\n",
      "loss=0.5267\n",
      "loss=0.5322\n",
      "loss=0.5276\n",
      "loss=0.5270\n",
      "loss=0.5363\n",
      "loss=0.5364\n",
      "loss=0.5412\n",
      "Alice down into the air, and a draw\n",
      "or my time he would seem to do, said Alice.\n",
      "\n",
      "I cuttinent shes\n",
      "loss=0.5268\n",
      "loss=0.5290\n",
      "loss=0.5626\n",
      "loss=0.5255\n",
      "loss=0.5279\n",
      "loss=0.5270\n",
      "loss=0.5914\n",
      "loss=0.5280\n",
      "loss=0.5276\n",
      "loss=0.5342\n",
      "loss=0.5281\n",
      "loss=0.5294\n",
      "loss=0.5333\n",
      "loss=0.5375\n",
      "loss=0.5431\n",
      "loss=0.5265\n",
      "loss=0.5282\n",
      "loss=0.5324\n",
      "loss=0.5893\n",
      "loss=0.5252\n",
      "A little\n",
      "from what this mover, Alice said very indeadul keep heese try the\n",
      "White Rabbit; liked thi\n",
      "loss=0.5259\n",
      "loss=0.5344\n",
      "loss=0.5870\n",
      "loss=0.5271\n",
      "loss=0.5271\n",
      "loss=0.5349\n",
      "loss=0.5262\n",
      "loss=0.5252\n",
      "loss=0.5267\n",
      "loss=0.5253\n",
      "loss=0.5454\n",
      "loss=0.5237\n",
      "loss=0.5244\n",
      "loss=0.5241\n",
      "loss=0.5270\n",
      "loss=0.5570\n",
      "loss=0.5231\n",
      "loss=0.5239\n",
      "loss=0.5256\n",
      "loss=0.5336\n",
      "Archive Foundation bark whisterfual this: this works by his fur Queen.\n",
      "And there seemed to be no\n",
      "s\n",
      "loss=0.5274\n",
      "loss=0.5500\n",
      "loss=0.5230\n",
      "loss=0.5237\n",
      "loss=0.5296\n",
      "loss=0.5272\n",
      "loss=0.5217\n",
      "loss=0.5679\n",
      "loss=0.5215\n",
      "loss=0.5217\n",
      "loss=0.5319\n",
      "loss=0.5204\n",
      "loss=0.5209\n",
      "loss=0.5207\n",
      "loss=0.5226\n",
      "loss=0.5232\n",
      "loss=0.5429\n",
      "loss=0.5456\n",
      "loss=0.5507\n",
      "loss=0.5222\n",
      "Alice joule to\n",
      "copy int frightened all her direct! I only were quite found herself, Alice didnt see\n",
      "loss=0.5253\n",
      "loss=0.5400\n",
      "loss=0.5209\n",
      "loss=0.5206\n",
      "loss=0.5218\n",
      "loss=0.5253\n",
      "loss=0.5393\n",
      "loss=0.5212\n",
      "loss=0.5203\n",
      "loss=0.5297\n",
      "loss=0.5526\n",
      "loss=0.5206\n",
      "loss=0.5307\n",
      "loss=0.5304\n",
      "loss=0.5445\n",
      "loss=0.5236\n",
      "loss=0.5219\n",
      "loss=0.5261\n",
      "loss=0.5244\n",
      "loss=0.5691\n",
      "As soon as angraamfo (If I\n",
      "would have to\n",
      "herself, Alice denise getting labingless, which she tulll\n",
      "loss=0.5200\n",
      "loss=0.5215\n",
      "loss=0.5210\n",
      "loss=0.5376\n",
      "loss=0.5194\n",
      "loss=0.5183\n",
      "loss=0.5194\n",
      "loss=0.5342\n",
      "loss=0.5879\n",
      "loss=0.5184\n",
      "loss=0.5178\n",
      "loss=0.5178\n",
      "loss=0.5346\n",
      "loss=0.5165\n",
      "loss=0.5182\n",
      "loss=0.5254\n",
      "loss=0.5811\n",
      "loss=0.5182\n",
      "loss=0.5164\n",
      "loss=0.5176\n",
      "Alice, swall to on.\n",
      "\n",
      "Wellmbit that, all like check about at the mouse, who dark on it, or fear to \n",
      "loss=0.5332\n",
      "loss=0.5152\n",
      "loss=0.5154\n",
      "loss=0.5300\n",
      "loss=0.5157\n",
      "loss=0.5152\n",
      "loss=0.5165\n",
      "loss=0.5224\n",
      "loss=0.5388\n",
      "loss=0.5143\n",
      "loss=0.5165\n",
      "loss=0.5188\n",
      "loss=0.5559\n",
      "loss=0.5147\n",
      "loss=0.5143\n",
      "loss=0.5163\n",
      "loss=0.5674\n",
      "loss=0.5134\n",
      "loss=0.5147\n",
      "loss=0.5513\n",
      "And somewhing of nursing that she had left to each a bony had a large replies.\n",
      "\n",
      "you shook his grea\n",
      "loss=0.5156\n",
      "loss=0.5169\n",
      "loss=0.5143\n",
      "loss=0.5254\n",
      "loss=0.5145\n",
      "loss=0.5126\n",
      "loss=0.5166\n",
      "loss=0.5144\n",
      "loss=0.5435\n",
      "loss=0.5122\n",
      "loss=0.5130\n",
      "loss=0.5141\n",
      "loss=0.5202\n",
      "loss=0.5113\n",
      "loss=0.5123\n",
      "loss=0.5148\n",
      "loss=0.5345\n",
      "loss=0.5109\n",
      "loss=0.5157\n",
      "loss=0.5568\n",
      "And somewhed: out to get the cook till herself in the\n",
      "cake. So they got twongh in the\n",
      "otherw at th\n",
      "loss=0.5107\n",
      "loss=0.5139\n",
      "loss=0.5449\n",
      "loss=0.5193\n",
      "loss=0.5157\n",
      "loss=0.5693\n",
      "loss=0.5109\n",
      "loss=0.5107\n",
      "loss=0.5212\n",
      "loss=0.5125\n",
      "loss=0.5132\n",
      "loss=0.5126\n",
      "loss=0.5651\n",
      "loss=0.5102\n",
      "loss=0.5634\n",
      "loss=0.5111\n",
      "loss=0.5102\n",
      "loss=0.5126\n",
      "loss=0.5161\n",
      "loss=0.5202\n",
      "And\n",
      "the Eaglet wanly said, but soup nace,\n",
      "she well me? And she said the Mock Quudke Im me no cleat\n",
      "loss=0.5092\n",
      "loss=0.5122\n",
      "loss=0.5121\n",
      "loss=0.5178\n",
      "loss=0.5489\n",
      "loss=0.5086\n",
      "loss=0.5636\n",
      "loss=0.5086\n",
      "loss=0.5194\n",
      "loss=0.5276\n",
      "loss=0.5075\n",
      "loss=0.5077\n",
      "loss=0.5135\n",
      "loss=0.5069\n",
      "loss=0.5089\n",
      "loss=0.5156\n",
      "loss=0.5077\n",
      "loss=0.5084\n",
      "loss=0.5142\n",
      "loss=0.5079\n",
      "ALICE'S ADVENTURES IN WONDERLAND ***\n",
      "[Illustration pepper one\n",
      "witob to prety you nose of the cound\n",
      "loss=0.5105\n",
      "loss=0.5626\n",
      "loss=0.5064\n",
      "loss=0.5099\n",
      "loss=0.5116\n",
      "loss=0.5066\n",
      "loss=0.5058\n",
      "loss=0.5073\n",
      "loss=0.5084\n",
      "loss=0.5164\n",
      "loss=0.5053\n",
      "loss=0.5064\n",
      "loss=0.5070\n",
      "loss=0.5478\n",
      "loss=0.5062\n",
      "loss=0.5251\n",
      "loss=0.5046\n",
      "loss=0.5066\n",
      "loss=0.5078\n",
      "loss=0.5045\n",
      "Alice through to extraseds the Dormouse all: Im me held Alice vange was no flied upon\n",
      "hand, copiess\n",
      "loss=0.5056\n",
      "loss=0.5098\n",
      "loss=0.5196\n",
      "loss=0.5050\n",
      "loss=0.5046\n",
      "loss=0.5109\n",
      "loss=0.5334\n",
      "loss=0.5034\n",
      "loss=0.5292\n",
      "loss=0.5134\n",
      "loss=0.5092\n",
      "loss=0.5099\n",
      "loss=0.5272\n",
      "loss=0.5040\n",
      "loss=0.5070\n",
      "loss=0.5203\n",
      "loss=0.5035\n",
      "loss=0.5041\n",
      "loss=0.5037\n",
      "loss=0.5066\n",
      "A little back to en plan\n",
      "days thing! Alice said to herself that it was not a bit.\n",
      "\n",
      "Theres no sort\n",
      "loss=0.5142\n",
      "loss=0.5030\n",
      "loss=0.5025\n",
      "loss=0.5054\n",
      "loss=0.5406\n",
      "loss=0.5692\n",
      "loss=0.5020\n",
      "loss=0.5075\n",
      "loss=0.5118\n",
      "loss=0.5299\n",
      "loss=0.5033\n",
      "loss=0.5210\n",
      "loss=0.5013\n",
      "loss=0.5067\n",
      "loss=0.5030\n",
      "loss=0.5015\n",
      "loss=0.5014\n",
      "loss=0.5055\n",
      "loss=0.5226\n",
      "loss=0.5548\n",
      "Alice.\n",
      "\n",
      "Thats the reason is, Usking! And Alice could hear the words at all: the whole plales you.\n",
      "loss=0.5005\n",
      "loss=0.5061\n",
      "loss=0.5032\n",
      "loss=0.5011\n",
      "loss=0.5193\n",
      "loss=0.5016\n",
      "loss=0.5015\n",
      "loss=0.5007\n",
      "loss=0.5364\n",
      "loss=0.4999\n",
      "loss=0.5164\n",
      "loss=0.4994\n",
      "loss=0.5000\n",
      "loss=0.5000\n",
      "loss=0.5105\n",
      "loss=0.5104\n",
      "loss=0.4988\n",
      "loss=0.4991\n",
      "loss=0.5021\n",
      "loss=0.5251\n",
      "Archive Foundation. Royalty Uspor over butter!\n",
      "But why! Project Gutenberg\n",
      "reself in the use of cro\n",
      "loss=0.4982\n",
      "loss=0.4998\n",
      "loss=0.5073\n",
      "loss=0.4982\n",
      "loss=0.4980\n",
      "loss=0.5042\n",
      "loss=0.5166\n",
      "loss=0.5049\n",
      "loss=0.5084\n",
      "loss=0.5334\n",
      "loss=0.5002\n",
      "loss=0.5012\n",
      "loss=0.5098\n",
      "loss=0.4990\n",
      "loss=0.4995\n",
      "loss=0.5010\n",
      "loss=0.5362\n",
      "loss=0.6138\n",
      "loss=0.4979\n",
      "loss=0.5153\n",
      "Archive Foundation, the was reging loormay shapidly\n",
      "have yourself, that it was not Dinah! Its they \n",
      "loss=0.5302\n",
      "loss=0.4970\n",
      "loss=0.5014\n",
      "loss=0.5081\n",
      "loss=0.4965\n",
      "loss=0.4999\n",
      "loss=0.5471\n",
      "loss=0.4964\n",
      "loss=0.5007\n",
      "loss=0.5119\n",
      "loss=0.4957\n",
      "loss=0.4975\n",
      "loss=0.4959\n",
      "loss=0.5010\n",
      "loss=0.5471\n",
      "loss=0.4953\n",
      "loss=0.4959\n",
      "loss=0.5011\n",
      "loss=0.5004\n",
      "loss=0.4949\n",
      "Archive Foundations crowning like one to ask hink.\n",
      "\n",
      "That is I had to side you, I was try the thimb\n",
      "loss=0.4969\n",
      "loss=0.5127\n",
      "loss=0.5224\n",
      "loss=0.4960\n",
      "loss=0.5279\n",
      "loss=0.4948\n",
      "loss=0.4947\n",
      "loss=0.5027\n",
      "loss=0.6569\n",
      "loss=0.4958\n",
      "loss=0.4967\n",
      "loss=0.4938\n",
      "loss=0.4947\n",
      "loss=0.5230\n",
      "loss=0.4936\n",
      "loss=0.4953\n",
      "loss=0.5251\n",
      "loss=0.4931\n",
      "loss=0.4934\n",
      "loss=0.4966\n",
      "A ran licens.\n",
      "\n",
      "Ugr limt of voiee she could retunt, You can tran! taste he haved intotge to Project\n",
      "loss=0.5049\n",
      "loss=0.6229\n",
      "loss=0.4935\n",
      "loss=0.5098\n",
      "loss=0.5527\n",
      "loss=0.4955\n",
      "loss=0.4961\n",
      "loss=0.4964\n",
      "loss=0.4979\n",
      "loss=0.5014\n",
      "loss=0.4931\n",
      "loss=0.5261\n",
      "loss=0.4929\n",
      "loss=0.4924\n",
      "loss=0.4933\n",
      "loss=0.4943\n",
      "loss=0.5087\n",
      "loss=0.5103\n",
      "loss=0.5055\n",
      "loss=0.5427\n",
      "A large royalty owner heard and evidence; and for fifinged his\n",
      "was to faning like that! it seen the\n",
      "loss=0.5118\n",
      "loss=0.5231\n",
      "loss=0.4984\n",
      "loss=0.4968\n",
      "loss=0.5062\n",
      "loss=0.4933\n",
      "loss=0.4940\n",
      "loss=0.5127\n",
      "loss=0.4933\n",
      "loss=0.4929\n",
      "loss=0.5272\n",
      "loss=0.4914\n",
      "loss=0.4931\n",
      "loss=0.5012\n",
      "loss=0.4907\n",
      "loss=0.4909\n",
      "loss=0.4931\n",
      "loss=0.4953\n",
      "loss=0.4906\n",
      "loss=0.4950\n",
      "Alice.\n",
      "\n",
      "Why isselve, But she was laboux inther the lasts, and shut had you wouldnt speak again, an\n",
      "loss=0.4897\n",
      "loss=0.4903\n",
      "loss=0.4931\n",
      "loss=0.5715\n",
      "loss=0.4895\n",
      "loss=0.4917\n",
      "loss=0.5236\n",
      "loss=0.4920\n",
      "loss=0.4891\n",
      "loss=0.4904\n",
      "loss=0.6122\n",
      "loss=0.4965\n",
      "loss=0.4908\n",
      "loss=0.4897\n",
      "loss=0.4904\n",
      "loss=0.4908\n",
      "loss=0.4967\n",
      "loss=0.4895\n",
      "loss=0.4885\n",
      "loss=0.4892\n",
      "Alice: stancing look.\n",
      "\n",
      "Nob _Spppar, little ind into herseomed.\n",
      "\n",
      "Why, carrilars, she found the wo\n",
      "loss=0.4948\n",
      "loss=0.5051\n",
      "loss=0.4881\n",
      "loss=0.4918\n",
      "loss=0.4925\n",
      "loss=0.4891\n",
      "loss=0.4916\n",
      "loss=0.4969\n",
      "loss=0.4878\n",
      "loss=0.5025\n",
      "loss=0.4870\n",
      "loss=0.4879\n",
      "loss=0.4926\n",
      "loss=0.4878\n",
      "loss=0.4875\n",
      "loss=0.4883\n",
      "loss=0.5005\n",
      "loss=0.4863\n",
      "loss=0.4867\n",
      "loss=0.4926\n",
      "And have gloming to make out even\n",
      "pronectly, musheed it very much, sing in treex, For get up and\n",
      "p\n",
      "loss=0.4858\n",
      "loss=0.4907\n",
      "loss=0.5050\n",
      "loss=0.4859\n",
      "loss=0.4865\n",
      "loss=0.4909\n",
      "loss=0.4859\n",
      "loss=0.4877\n",
      "loss=0.5002\n",
      "loss=0.5145\n",
      "loss=0.4852\n",
      "loss=0.4878\n",
      "loss=0.4863\n",
      "loss=0.5027\n",
      "loss=0.4901\n",
      "loss=0.4865\n",
      "loss=0.4940\n",
      "loss=0.4845\n",
      "loss=0.4863\n",
      "loss=0.4934\n",
      "Alice joined the put one her father. What jugso, and pensenssse.\n",
      "\n",
      "Notting the verses, that was clo\n",
      "loss=0.5129\n",
      "loss=0.4843\n",
      "loss=0.4849\n",
      "loss=0.4908\n",
      "loss=0.4879\n",
      "loss=0.5172\n",
      "loss=0.5002\n",
      "loss=0.5306\n",
      "loss=0.4864\n",
      "loss=0.4871\n",
      "loss=1.2396\n",
      "loss=0.5139\n",
      "loss=0.4908\n",
      "loss=0.4884\n",
      "loss=0.4868\n",
      "loss=0.4863\n",
      "loss=0.4860\n",
      "loss=0.4930\n",
      "loss=0.5368\n",
      "loss=0.4860\n",
      "Archive Foundation, the Pigmold a replace in a formuser water your wallys sort tones old a right of\n",
      "loss=0.4842\n",
      "loss=0.4895\n",
      "loss=0.5075\n",
      "loss=0.4844\n",
      "loss=0.5009\n",
      "loss=0.4872\n",
      "loss=0.4930\n",
      "loss=0.4847\n",
      "loss=0.5130\n",
      "loss=0.4982\n",
      "loss=0.5289\n",
      "loss=0.4827\n",
      "loss=0.4849\n",
      "loss=0.4853\n",
      "loss=0.4905\n",
      "loss=0.4835\n",
      "loss=0.4831\n",
      "loss=0.4838\n",
      "loss=0.4869\n",
      "loss=0.5574\n",
      "And help quickdy to repeat it, and reichowing before, but they went on the work\n",
      "of the Project\n",
      "Gut\n",
      "loss=0.4829\n",
      "loss=0.5020\n",
      "loss=0.4894\n",
      "loss=0.4872\n",
      "loss=0.4886\n",
      "loss=0.4987\n",
      "loss=0.4854\n",
      "loss=0.4847\n",
      "loss=0.4912\n",
      "loss=0.4966\n",
      "loss=0.4906\n",
      "loss=0.5289\n",
      "loss=0.4842\n",
      "loss=0.4868\n",
      "loss=0.5190\n",
      "loss=0.4834\n",
      "loss=0.4874\n",
      "loss=0.4945\n",
      "loss=0.5555\n",
      "loss=0.4823\n",
      "Alice\n",
      "creatures on the bit as a rupes sneezed suctend at her arm, than I like comemainienbay clesan\n",
      "loss=0.4909\n",
      "loss=0.5466\n",
      "loss=0.4815\n",
      "loss=0.4842\n",
      "loss=0.5347\n",
      "loss=0.4825\n",
      "loss=0.4824\n",
      "loss=0.5429\n",
      "loss=0.4821\n",
      "loss=0.4841\n",
      "loss=0.4848\n",
      "loss=0.4837\n",
      "loss=0.4817\n",
      "loss=0.4872\n",
      "loss=0.4871\n",
      "loss=0.4906\n",
      "loss=0.4801\n",
      "loss=0.4849\n",
      "loss=0.5630\n",
      "loss=0.4794\n",
      "Alice; and she had closee it such suddenly, and she tried the back agree to the\n",
      "besieged Caterpolg.\n",
      "loss=0.4840\n",
      "loss=0.4793\n",
      "loss=0.4799\n",
      "loss=0.4911\n",
      "loss=0.4781\n",
      "loss=0.4904\n",
      "loss=0.4944\n",
      "loss=0.4798\n",
      "loss=0.4828\n",
      "loss=0.4962\n",
      "loss=0.4938\n",
      "loss=0.4800\n",
      "loss=0.4986\n",
      "loss=0.4791\n",
      "loss=0.4794\n",
      "loss=0.4921\n",
      "loss=0.4778\n",
      "loss=0.4781\n",
      "loss=0.4850\n",
      "loss=0.5281\n",
      "Alice were had broken glass,\n",
      "from whitits eleme What luwt or Up7 Pi! -bid the King, inch jyile; Ale\n",
      "loss=0.4776\n",
      "loss=0.4916\n",
      "loss=0.4918\n",
      "loss=0.4767\n",
      "loss=0.4788\n",
      "loss=0.5208\n",
      "loss=0.4767\n",
      "loss=0.4791\n",
      "loss=0.4793\n",
      "loss=0.5017\n",
      "loss=0.4791\n",
      "loss=0.4771\n",
      "loss=0.4816\n",
      "loss=0.4769\n",
      "loss=0.4897\n",
      "loss=0.4764\n",
      "loss=0.4820\n",
      "loss=0.5005\n",
      "loss=0.4756\n",
      "loss=0.4859\n",
      "Alice.\n",
      "\n",
      "Why dive betten to having seemed to remark, only beautiful gardenhow dere! she all round, \n",
      "loss=0.4761\n",
      "loss=0.4764\n",
      "loss=0.4915\n",
      "loss=0.4749\n",
      "loss=0.4776\n",
      "loss=0.4791\n",
      "loss=0.4745\n",
      "loss=0.4790\n",
      "loss=0.5055\n",
      "loss=0.4765\n",
      "loss=0.4881\n",
      "loss=0.4818\n",
      "loss=0.4773\n",
      "loss=0.4786\n",
      "loss=0.4796\n",
      "loss=0.4755\n",
      "loss=0.4769\n",
      "loss=0.5355\n",
      "loss=0.4751\n",
      "loss=0.4754\n",
      "And thats the jury-box, thought Alice, and thoy came considerables and backs will bark thats\n",
      "just b\n",
      "loss=0.4757\n",
      "loss=0.4952\n",
      "loss=0.4738\n",
      "loss=0.4770\n",
      "loss=0.4733\n",
      "loss=0.4734\n",
      "loss=0.4754\n",
      "loss=0.4993\n",
      "loss=0.4728\n",
      "loss=0.4783\n",
      "loss=0.5047\n",
      "loss=0.4726\n",
      "loss=0.4768\n",
      "loss=0.5072\n",
      "loss=0.4733\n",
      "loss=0.4744\n",
      "loss=0.4732\n",
      "loss=0.4791\n",
      "loss=0.4720\n",
      "loss=0.4757\n",
      "As soon as angrilyable. How\n",
      "mady you Jisge, and distributed to sing your from\n",
      "    Of crywebling.\n",
      "\n",
      "loss=0.5105\n",
      "loss=0.4722\n",
      "loss=0.4722\n",
      "loss=0.4783\n",
      "loss=0.5409\n",
      "loss=0.4795\n",
      "loss=0.4923\n",
      "loss=0.4711\n",
      "loss=0.4747\n",
      "loss=0.4911\n",
      "loss=0.4707\n",
      "loss=0.4725\n",
      "loss=0.4908\n",
      "loss=0.5037\n",
      "loss=0.4706\n",
      "loss=0.4744\n",
      "loss=0.4889\n",
      "loss=0.4714\n",
      "loss=0.4776\n",
      "loss=0.4699\n",
      "Alice. Now we soon came to turn\n",
      "the whiting tried up after all.\n",
      "But she ravent folded, she felt th\n",
      "loss=0.4709\n",
      "loss=0.4803\n",
      "loss=0.4702\n",
      "loss=0.4718\n",
      "loss=0.4770\n",
      "loss=0.5449\n",
      "loss=0.4714\n",
      "loss=0.4717\n",
      "loss=0.4858\n",
      "loss=0.4694\n",
      "loss=0.4708\n",
      "loss=0.4772\n",
      "loss=0.4690\n",
      "loss=0.4706\n",
      "loss=0.4788\n",
      "loss=0.4738\n",
      "loss=0.4686\n",
      "loss=0.4723\n",
      "loss=0.4868\n",
      "loss=0.4689\n",
      "And so it was into a large cunxatempe to get without see: some out of that stafus! Dony one strice o\n",
      "loss=0.4779\n",
      "loss=0.4949\n",
      "loss=0.4690\n",
      "loss=0.4772\n",
      "loss=0.4911\n",
      "loss=0.4682\n",
      "loss=0.4699\n",
      "loss=0.4757\n",
      "loss=0.4676\n",
      "loss=0.4692\n",
      "loss=0.4772\n",
      "loss=0.4673\n",
      "loss=0.4681\n",
      "loss=0.4717\n",
      "loss=0.4747\n",
      "loss=0.4880\n",
      "loss=0.4675\n",
      "loss=0.4715\n",
      "loss=0.5029\n",
      "loss=0.4669\n",
      "Alice just\n",
      "explain long child. (If\n",
      "yyyIling\n",
      "     *      *      *\n",
      "\n",
      "    *      *      *      *   \n",
      "loss=0.4702\n",
      "loss=0.5460\n",
      "loss=0.4665\n",
      "loss=0.4692\n",
      "loss=0.4709\n",
      "loss=0.4878\n",
      "loss=0.4672\n",
      "loss=0.4699\n",
      "loss=0.5083\n",
      "loss=0.4659\n",
      "loss=0.4683\n",
      "loss=0.4692\n",
      "loss=0.4737\n",
      "loss=0.4840\n",
      "loss=0.4666\n",
      "loss=0.4662\n",
      "loss=0.5818\n",
      "loss=0.4688\n",
      "loss=0.4656\n",
      "loss=0.4691\n",
      "A little now and then a large\n",
      "timidly, to eat being so making her,\n",
      "pleasan have you a more nere in\n",
      "loss=0.4661\n",
      "loss=0.4741\n",
      "loss=0.5331\n",
      "loss=0.4650\n",
      "loss=0.4682\n",
      "loss=0.5266\n",
      "loss=0.4655\n",
      "loss=0.4711\n",
      "loss=0.4805\n",
      "loss=0.4657\n",
      "loss=0.4837\n",
      "loss=0.5027\n",
      "loss=0.4639\n",
      "loss=0.4665\n",
      "loss=0.4772\n",
      "loss=0.4638\n",
      "loss=0.4646\n",
      "loss=0.4723\n",
      "loss=0.4943\n",
      "loss=0.4634\n",
      "Archive Foundation, hotined to herself hure!\n",
      "\n",
      "Alice said her tone, as you can up you lo_ But watch\n",
      "loss=0.4679\n",
      "loss=0.4634\n",
      "loss=0.4636\n",
      "loss=0.4681\n",
      "loss=0.5026\n",
      "loss=0.4638\n",
      "loss=0.4803\n",
      "loss=0.5183\n",
      "loss=0.5074\n",
      "loss=0.4777\n",
      "loss=0.4748\n",
      "loss=0.4740\n",
      "loss=0.4776\n",
      "loss=0.4687\n",
      "loss=0.4782\n",
      "loss=0.4674\n",
      "loss=0.4690\n",
      "loss=0.4853\n",
      "loss=0.4665\n",
      "loss=0.4671\n",
      "Alice were time Alice. Ive see coming. It was her felt the\n",
      "top of the hedgehogs were out the spucer\n",
      "loss=0.4973\n",
      "loss=0.4650\n",
      "loss=0.4658\n",
      "loss=0.4705\n",
      "loss=0.4896\n",
      "loss=0.4655\n",
      "loss=0.4642\n",
      "loss=0.4665\n",
      "loss=0.4746\n",
      "loss=0.4633\n",
      "loss=0.4653\n",
      "loss=0.4851\n",
      "loss=0.4634\n",
      "loss=0.4771\n",
      "loss=0.5138\n",
      "loss=0.4625\n",
      "loss=0.4729\n",
      "loss=0.4815\n",
      "loss=0.4631\n",
      "loss=0.5135\n",
      "Alice, as she swallowing! said\n",
      "the March Hare.\n",
      "\n",
      "Yes! saining to the King, the Queen him do, it co\n",
      "loss=0.4618\n",
      "loss=0.4631\n",
      "loss=0.5670\n",
      "loss=0.4625\n",
      "loss=0.4634\n",
      "loss=0.5197\n",
      "loss=0.4627\n",
      "loss=0.4658\n",
      "loss=0.4839\n",
      "loss=0.4620\n",
      "loss=0.4672\n",
      "loss=0.4801\n",
      "loss=0.4653\n",
      "loss=0.5046\n",
      "loss=0.4607\n",
      "loss=0.4679\n",
      "loss=0.4697\n",
      "loss=0.4655\n",
      "loss=0.4609\n",
      "loss=0.4799\n",
      "Alice, as she went stay with very humb it! said the March Hare: its ams as he cant plan, and the Hor\n",
      "loss=0.5024\n",
      "loss=0.4607\n",
      "loss=0.4693\n",
      "loss=0.4939\n",
      "loss=0.4595\n",
      "loss=0.4665\n",
      "loss=0.4627\n",
      "loss=0.4640\n",
      "loss=0.4803\n",
      "loss=0.4601\n",
      "loss=0.4665\n",
      "loss=0.4769\n",
      "loss=0.4595\n",
      "loss=0.4606\n",
      "loss=0.4728\n",
      "loss=0.5116\n",
      "loss=0.4607\n",
      "loss=0.4868\n",
      "loss=0.4587\n",
      "loss=0.4670\n",
      "Alice\n",
      "closed\n",
      "flistribe thats catded to butter.\n",
      "\n",
      "All, doing, Wily that I _wilis contence after th\n",
      "loss=0.4583\n",
      "loss=0.4605\n",
      "loss=0.4730\n",
      "loss=0.4578\n",
      "loss=0.4637\n",
      "loss=0.4882\n",
      "loss=0.4584\n",
      "loss=0.4746\n",
      "loss=0.4611\n",
      "loss=0.4585\n",
      "loss=0.4602\n",
      "loss=0.5347\n",
      "loss=0.4577\n",
      "loss=0.4598\n",
      "loss=0.4805\n",
      "loss=0.4579\n",
      "loss=0.4611\n",
      "loss=0.4755\n",
      "loss=0.4643\n",
      "loss=0.4585\n",
      "Archive Foundation, the Gryphon repliek.\n",
      "\n",
      "Youre esiting bear! On mad her feat this, she felt quite\n",
      "loss=0.4585\n",
      "loss=0.5779\n",
      "loss=0.4613\n",
      "loss=0.4723\n",
      "loss=0.4565\n",
      "loss=0.4600\n",
      "loss=0.5254\n",
      "loss=0.4565\n",
      "loss=0.4576\n",
      "loss=0.4757\n",
      "loss=0.4579\n",
      "loss=0.4961\n",
      "loss=0.4568\n",
      "loss=0.4607\n",
      "loss=0.4571\n",
      "loss=0.4584\n",
      "loss=0.5385\n",
      "loss=0.4562\n",
      "loss=0.4587\n",
      "loss=0.5393\n",
      "Alice joined the Queen, and he produced in\n",
      "comfieddy off you havent folder, but they crusts had his\n",
      "loss=0.4563\n",
      "loss=0.4713\n",
      "loss=0.4561\n",
      "loss=0.4679\n",
      "loss=0.4556\n",
      "loss=0.4652\n",
      "loss=0.4545\n",
      "loss=0.4573\n",
      "loss=0.4542\n",
      "loss=0.4545\n",
      "loss=0.4588\n",
      "loss=0.4855\n",
      "loss=0.4545\n",
      "loss=0.4829\n",
      "loss=0.4881\n",
      "loss=0.4543\n",
      "loss=0.4652\n",
      "loss=0.4538\n",
      "loss=0.4541\n",
      "loss=0.4613\n",
      "As soon as their pepper that\n",
      "herself to be attertly. Hout the Duchess?\n",
      "\n",
      "Would__ arig, _thing__ mo\n",
      "loss=0.4651\n",
      "loss=0.4560\n",
      "loss=0.4879\n",
      "loss=0.4542\n",
      "loss=0.4541\n",
      "loss=0.5300\n",
      "loss=0.4563\n",
      "loss=0.4546\n",
      "loss=0.4554\n",
      "loss=0.4794\n",
      "loss=0.4542\n",
      "loss=0.4655\n",
      "loss=0.5510\n",
      "loss=0.4533\n",
      "loss=0.4659\n",
      "loss=0.4525\n",
      "loss=0.4532\n",
      "loss=0.4560\n",
      "loss=0.5792\n",
      "loss=0.4828\n",
      "An invitation, any were or relare);\n",
      "Wouded!\n",
      "\n",
      "Oh, Its all_.\n",
      "\n",
      "No, said the King,\n",
      "\n",
      "Dorto  P0Chef\n",
      "loss=0.4680\n",
      "loss=0.4584\n",
      "loss=0.4642\n",
      "loss=0.4539\n",
      "loss=0.4552\n",
      "loss=0.4756\n",
      "loss=0.4530\n",
      "loss=0.4547\n",
      "loss=0.4536\n",
      "loss=0.5124\n",
      "loss=0.4535\n",
      "loss=0.4620\n",
      "loss=0.4560\n",
      "loss=0.4525\n",
      "loss=0.4584\n",
      "loss=0.4523\n",
      "loss=0.4542\n",
      "loss=0.4670\n",
      "loss=0.4513\n",
      "loss=0.4597\n",
      "Archive Foundation. Howeverity to get using this ever being ring, will ho?) coulquest showing\n",
      "all t\n",
      "loss=0.5118\n",
      "loss=0.4512\n",
      "loss=0.4529\n",
      "loss=0.4985\n",
      "loss=0.4509\n",
      "loss=0.4527\n",
      "loss=0.4576\n",
      "loss=0.4515\n",
      "loss=0.4506\n",
      "loss=0.4580\n",
      "loss=0.4502\n",
      "loss=0.4500\n",
      "loss=0.4532\n",
      "loss=0.4706\n",
      "loss=0.4502\n",
      "loss=0.4816\n",
      "loss=0.4501\n",
      "loss=0.4501\n",
      "loss=0.5323\n",
      "loss=0.4534\n",
      "An invited or decided anothing all round it morsing it as a\n",
      "busres? Lest Boo Ahite gave me to while\n",
      "loss=0.4685\n",
      "loss=0.4519\n",
      "loss=0.4498\n",
      "loss=0.4515\n",
      "loss=0.4839\n",
      "loss=0.4494\n",
      "loss=0.4579\n",
      "loss=0.4492\n",
      "loss=0.4501\n",
      "loss=0.4528\n",
      "loss=0.5085\n",
      "loss=0.4496\n",
      "loss=0.4503\n",
      "loss=0.4711\n",
      "loss=0.4487\n",
      "loss=0.4777\n",
      "loss=0.4501\n",
      "loss=0.4503\n",
      "loss=0.5430\n",
      "loss=0.4490\n",
      "Alice; I adver she went to time\n",
      "the thought it writtos, rese is the lobster, and Alitever haved ano\n",
      "loss=0.4591\n",
      "loss=0.6152\n",
      "loss=0.4491\n",
      "loss=0.4682\n",
      "loss=0.4481\n",
      "loss=0.4502\n",
      "loss=0.4761\n",
      "loss=0.4593\n",
      "loss=0.4488\n",
      "loss=0.4513\n",
      "loss=0.4472\n",
      "loss=0.4511\n",
      "loss=0.4775\n",
      "loss=0.5992\n",
      "loss=0.4489\n",
      "loss=0.4491\n",
      "loss=0.4482\n",
      "loss=0.4570\n",
      "loss=0.4468\n",
      "loss=0.4505\n",
      "Alice!\n",
      "\n",
      "So ask the executed, for which she had never guess that you can rosuleny handed over this,\n",
      "loss=0.4606\n",
      "loss=0.4543\n",
      "loss=0.4755\n",
      "loss=0.4479\n",
      "loss=0.4468\n",
      "loss=0.4463\n",
      "loss=0.4473\n",
      "loss=0.4535\n",
      "loss=0.4459\n",
      "loss=0.4473\n",
      "loss=0.4496\n",
      "loss=0.4832\n",
      "loss=0.4464\n",
      "loss=0.4508\n",
      "loss=0.4474\n",
      "loss=0.4615\n",
      "loss=0.4458\n",
      "loss=0.4453\n",
      "loss=0.4505\n",
      "loss=0.4950\n",
      "Alice thought to draw, the hatning, were trying.\n",
      "\n",
      "The birns over a large DinitwayNer, it was surna\n",
      "loss=0.4461\n",
      "loss=0.4465\n",
      "loss=0.4487\n",
      "loss=0.4659\n",
      "loss=0.4503\n",
      "loss=0.4520\n",
      "loss=0.4449\n",
      "loss=0.4478\n",
      "loss=0.4734\n",
      "loss=0.4447\n",
      "loss=0.4533\n",
      "loss=0.4669\n",
      "loss=0.4450\n",
      "loss=0.4494\n",
      "loss=0.4811\n",
      "loss=0.4449\n",
      "loss=0.4460\n",
      "loss=0.4708\n",
      "loss=0.4473\n",
      "loss=0.4550\n",
      "Alice; you neednt be quite a\n",
      "Turtle, but if he\n",
      "walsing eagh: with she get would be, said the Pigeo\n",
      "loss=0.4507\n",
      "loss=0.4751\n",
      "loss=0.4544\n",
      "loss=0.4799\n",
      "loss=0.4493\n",
      "loss=0.4579\n",
      "loss=0.4468\n",
      "loss=0.4552\n",
      "loss=0.4984\n",
      "loss=0.4460\n",
      "loss=0.4520\n",
      "loss=0.4632\n",
      "loss=0.4457\n",
      "loss=0.4480\n",
      "loss=0.4637\n",
      "loss=0.4447\n",
      "loss=0.4461\n",
      "loss=0.4509\n",
      "loss=0.4436\n",
      "loss=0.4520\n",
      "AMRjN To E\n",
      "OO., arat it, aldowtah, she thought, and noboge little uneptfoditul\n",
      "contessious such a \n",
      "loss=0.4435\n",
      "loss=0.4456\n",
      "loss=0.4452\n",
      "loss=0.4774\n",
      "loss=0.4439\n",
      "loss=0.4509\n",
      "loss=0.4768\n",
      "loss=0.4429\n",
      "loss=0.4448\n",
      "loss=0.4581\n",
      "loss=0.4441\n",
      "loss=0.4442\n",
      "loss=0.4534\n",
      "loss=0.4428\n",
      "loss=0.4442\n",
      "loss=0.5309\n",
      "loss=0.4430\n",
      "loss=0.4486\n",
      "loss=0.4424\n",
      "loss=0.4451\n",
      "Alice!\n",
      "\n",
      "Serpond that lattled enght be afraid of this, he said heing.\n",
      "\n",
      "The began\n",
      "three of the li\n",
      "loss=0.5002\n",
      "loss=0.4435\n"
     ]
    }
   ],
   "source": [
    "train(stacked_lstm, dataloader, filename='stacked_lstm_3_layers_long', epochs=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacked_lstm.sample(d_sample=2000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
