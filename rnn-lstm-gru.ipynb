{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN / LSTM / GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download alice in wonderland\n",
    "url = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "book = requests.get(url).content\n",
    "book = book.decode('ascii', 'ignore')\n",
    "vocab = set(book)\n",
    "d_vocab = len(vocab)\n",
    "d_hidden = 100\n",
    "d_batch = 1000000\n",
    "atoi = {a: i for i, a in enumerate(vocab)}\n",
    "itoa = {i: a for a, i in atoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataloader(text, seq_len=25, batch_size=d_batch):\n",
    "    x = [text[i:i+seq_len] for i in range(0, len(text)-seq_len-1, seq_len)]\n",
    "    y = [text[i+1:i+seq_len+1] for i in range(0, len(text)-seq_len-1, seq_len)]\n",
    "    x = t.tensor([[atoi[a] for a in s] for s in x])\n",
    "    y = t.tensor([[atoi[a] for a in s] for s in y])\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader = to_dataloader(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, d_in=10, d_hidden=20, d_out=30):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(d_in, d_hidden)\n",
    "        self.hidden = nn.Linear(d_hidden, d_hidden)\n",
    "        self.unembed = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, xs, memory=None, return_memory=False):\n",
    "        # xs: (batch, d_context, d_vocab)\n",
    "        batch, d_context, _ = xs.shape\n",
    "        outs = []\n",
    "        if memory is None: memory = t.zeros(batch, self.hidden.in_features).to(xs.device)\n",
    "        for i in range(d_context):\n",
    "            x = xs[:, i]\n",
    "            memory = F.tanh(self.embed(x) + self.hidden(memory))\n",
    "            outs.append(self.unembed(memory))\n",
    "        if return_memory:\n",
    "            return t.stack(outs, dim=1), memory\n",
    "        return t.stack(outs, dim=1)\n",
    "\n",
    "model = RNN(d_vocab, d_hidden, d_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def sample(model, text='A', d_sample=100):\n",
    "    model = model.to(device)\n",
    "    memory = t.zeros(1, model.hidden.in_features).to(device)\n",
    "    x = t.tensor([[atoi[c] for c in text]])\n",
    "    x = F.one_hot(x, num_classes=d_vocab).float().to(device)\n",
    "    while len(text) <= d_sample:\n",
    "        outs, memory = model(x, memory=memory, return_memory=True)\n",
    "        probs = outs[0, -1].softmax(dim=0)\n",
    "        next_sample = t.multinomial(probs, num_samples=1)\n",
    "        text += itoa[next_sample.item()]\n",
    "        x = F.one_hot(next_sample, num_classes=d_vocab).float().to(device)[:, None, :]\n",
    "    return text\n",
    "\n",
    "# sample(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882ec2e8a3dd4ed7b2d56ce4d4d53dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.2869\n",
      "Alices Cthink beated associce-extref toINBUOF.\n",
      "TeOlm EINBE THE\n",
      "E Dery how\n",
      "brea what it in the feat\n",
      "loss=1.2870\n",
      "loss=1.2868\n",
      "loss=1.2867\n",
      "loss=1.2867\n",
      "loss=1.2867\n",
      "loss=1.2867\n",
      "loss=1.2866\n",
      "loss=1.2866\n",
      "loss=1.2866\n",
      "loss=1.2865\n",
      "loss=1.2865\n",
      "loss=1.2865\n",
      "loss=1.2864\n",
      "loss=1.2864\n",
      "loss=1.2864\n",
      "loss=1.2863\n",
      "loss=1.2863\n",
      "loss=1.2862\n",
      "loss=1.2862\n",
      "Archive Fgescclt! or shouldrons very repailate or dreadfurer nime, wated so certem! said the Quevery\n",
      "loss=1.2861\n",
      "loss=1.2861\n",
      "loss=1.2862\n",
      "loss=1.2860\n",
      "loss=1.2861\n",
      "loss=1.2862\n",
      "loss=1.2860\n",
      "loss=1.2859\n",
      "loss=1.2860\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, epochs=2000, d_vocab=d_vocab, opt=None, lr=3e-4):\n",
    "    model = model.to(device)\n",
    "    if opt is None:\n",
    "        opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for xs, ys in dataloader:\n",
    "            out = model(F.one_hot(xs, num_classes=d_vocab).float().to(device))\n",
    "            loss = F.cross_entropy(out.permute(0, 2, 1), ys.to(device))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'loss={loss.item():.4f}')\n",
    "        if epoch % 999 == 0:\n",
    "            print(sample(model))\n",
    "\n",
    "train(model, dataloader, epochs=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
